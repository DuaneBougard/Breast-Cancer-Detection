{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run group9_dividedata.ipynb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,\n",
    " #                  solver='lbfgs', verbose=10, tol=1e-4, random_state=1,\n",
    "  #                  learning_rate_init=.00000001)\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(100,), activation='identity', solver='adam', alpha=0.0001, batch_size=100, \n",
    "                 learning_rate='constant', learning_rate_init=0.1, power_t=0.5, max_iter=3900, shuffle=True, \n",
    "                random_state=None, tol=0.00000001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                early_stopping=False, validation_fraction=0.99, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit(X_train).transform(X_train)\n",
    "#X_test_scaled = scaler.fit(X_test).transform(X_test)\n",
    "#from sklearn import preprocessing\n",
    "#scaler=preprocessing.MinMaxScaler()\n",
    "#X_test[X_test.columns]=scaler.fit_transform(X_test[X_test.columns])\n",
    "#X_train[X_train.columns]=scaler.fit_transform(X_train[X_train.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.54342614\n",
      "Iteration 2, loss = 6.04863940\n",
      "Iteration 3, loss = 2.33102934\n",
      "Iteration 4, loss = 1.62094304\n",
      "Iteration 5, loss = 1.09346363\n",
      "Iteration 6, loss = 0.84247435\n",
      "Iteration 7, loss = 0.41879989\n",
      "Iteration 8, loss = 0.42942867\n",
      "Iteration 9, loss = 0.23388934\n",
      "Iteration 10, loss = 0.20315229\n",
      "Iteration 11, loss = 0.19536749\n",
      "Iteration 12, loss = 0.16854897\n",
      "Iteration 13, loss = 0.15979545\n",
      "Iteration 14, loss = 0.15372883\n",
      "Iteration 15, loss = 0.15286404\n",
      "Iteration 16, loss = 0.14005656\n",
      "Iteration 17, loss = 0.13387478\n",
      "Iteration 18, loss = 0.13223511\n",
      "Iteration 19, loss = 0.13243772\n",
      "Iteration 20, loss = 0.12548946\n",
      "Iteration 21, loss = 0.14463681\n",
      "Iteration 22, loss = 0.16289438\n",
      "Iteration 23, loss = 0.27735243\n",
      "Iteration 24, loss = 0.36760664\n",
      "Iteration 25, loss = 0.25481760\n",
      "Iteration 26, loss = 0.16958888\n",
      "Iteration 27, loss = 0.14422603\n",
      "Iteration 28, loss = 0.13382800\n",
      "Iteration 29, loss = 0.18815529\n",
      "Iteration 30, loss = 0.13439931\n",
      "Iteration 31, loss = 0.22102170\n",
      "Iteration 32, loss = 0.20894290\n",
      "Iteration 33, loss = 0.21419068\n",
      "Iteration 34, loss = 0.15690867\n",
      "Iteration 35, loss = 0.12835176\n",
      "Iteration 36, loss = 0.14904841\n",
      "Iteration 37, loss = 0.13317424\n",
      "Iteration 38, loss = 0.12345974\n",
      "Iteration 39, loss = 0.12680248\n",
      "Iteration 40, loss = 0.13008978\n",
      "Iteration 41, loss = 0.12392554\n",
      "Iteration 42, loss = 0.17560953\n",
      "Iteration 43, loss = 0.14764659\n",
      "Iteration 44, loss = 0.13280877\n",
      "Iteration 45, loss = 0.13056889\n",
      "Iteration 46, loss = 0.12293538\n",
      "Iteration 47, loss = 0.11688493\n",
      "Iteration 48, loss = 0.11912457\n",
      "Iteration 49, loss = 0.12735639\n",
      "Iteration 50, loss = 0.11851380\n",
      "Iteration 51, loss = 0.11777364\n",
      "Iteration 52, loss = 0.13938351\n",
      "Iteration 53, loss = 0.11283916\n",
      "Iteration 54, loss = 0.11742746\n",
      "Iteration 55, loss = 0.12175916\n",
      "Iteration 56, loss = 0.11382621\n",
      "Iteration 57, loss = 0.14490231\n",
      "Iteration 58, loss = 0.18287096\n",
      "Iteration 59, loss = 0.19758125\n",
      "Iteration 60, loss = 0.15791915\n",
      "Iteration 61, loss = 0.16260487\n",
      "Iteration 62, loss = 0.13328537\n",
      "Iteration 63, loss = 0.13993900\n",
      "Iteration 64, loss = 0.17777279\n",
      "Iteration 65, loss = 0.14018212\n",
      "Iteration 66, loss = 0.16275344\n",
      "Iteration 67, loss = 0.16083694\n",
      "Iteration 68, loss = 0.15657647\n",
      "Iteration 69, loss = 0.15567252\n",
      "Iteration 70, loss = 0.13995999\n",
      "Iteration 71, loss = 0.14978707\n",
      "Iteration 72, loss = 0.13715548\n",
      "Iteration 73, loss = 0.12628807\n",
      "Iteration 74, loss = 0.11537374\n",
      "Iteration 75, loss = 0.12569080\n",
      "Iteration 76, loss = 0.11518387\n",
      "Iteration 77, loss = 0.13423568\n",
      "Iteration 78, loss = 0.12456083\n",
      "Iteration 79, loss = 0.15885216\n",
      "Iteration 80, loss = 0.11336151\n",
      "Iteration 81, loss = 0.15532993\n",
      "Iteration 82, loss = 0.13515677\n",
      "Iteration 83, loss = 0.13927715\n",
      "Iteration 84, loss = 0.13649969\n",
      "Iteration 85, loss = 0.11764258\n",
      "Iteration 86, loss = 0.17143442\n",
      "Iteration 87, loss = 0.13515137\n",
      "Iteration 88, loss = 0.13917803\n",
      "Iteration 89, loss = 0.14402944\n",
      "Iteration 90, loss = 0.12341807\n",
      "Iteration 91, loss = 0.11803344\n",
      "Iteration 92, loss = 0.11794778\n",
      "Iteration 93, loss = 0.11743746\n",
      "Iteration 94, loss = 0.11189253\n",
      "Iteration 95, loss = 0.11708860\n",
      "Iteration 96, loss = 0.11959180\n",
      "Iteration 97, loss = 0.11993010\n",
      "Iteration 98, loss = 0.12762980\n",
      "Iteration 99, loss = 0.12417919\n",
      "Iteration 100, loss = 0.13175077\n",
      "Iteration 101, loss = 0.13076635\n",
      "Iteration 102, loss = 0.12710875\n",
      "Iteration 103, loss = 0.12092099\n",
      "Iteration 104, loss = 0.13203138\n",
      "Iteration 105, loss = 0.11991155\n",
      "Iteration 106, loss = 0.12750466\n",
      "Iteration 107, loss = 0.13069344\n",
      "Iteration 108, loss = 0.13074076\n",
      "Iteration 109, loss = 0.23229347\n",
      "Iteration 110, loss = 0.15200191\n",
      "Iteration 111, loss = 0.13438718\n",
      "Iteration 112, loss = 0.11625614\n",
      "Iteration 113, loss = 0.12996837\n",
      "Iteration 114, loss = 0.22292195\n",
      "Iteration 115, loss = 0.14547679\n",
      "Iteration 116, loss = 0.14352355\n",
      "Iteration 117, loss = 0.12249192\n",
      "Iteration 118, loss = 0.16084372\n",
      "Iteration 119, loss = 0.21839821\n",
      "Iteration 120, loss = 0.13517387\n",
      "Iteration 121, loss = 0.12667650\n",
      "Iteration 122, loss = 0.12482147\n",
      "Iteration 123, loss = 0.11114693\n",
      "Iteration 124, loss = 0.11557559\n",
      "Iteration 125, loss = 0.12043427\n",
      "Iteration 126, loss = 0.12016692\n",
      "Iteration 127, loss = 0.14419076\n",
      "Iteration 128, loss = 0.11732381\n",
      "Iteration 129, loss = 0.11872909\n",
      "Iteration 130, loss = 0.12451589\n",
      "Iteration 131, loss = 0.11768430\n",
      "Iteration 132, loss = 0.15217001\n",
      "Iteration 133, loss = 0.15837161\n",
      "Iteration 134, loss = 0.12068819\n",
      "Iteration 135, loss = 0.13795815\n",
      "Iteration 136, loss = 0.12108831\n",
      "Iteration 137, loss = 0.13645581\n",
      "Iteration 138, loss = 0.11872425\n",
      "Iteration 139, loss = 0.11481669\n",
      "Iteration 140, loss = 0.11598101\n",
      "Iteration 141, loss = 0.16013590\n",
      "Iteration 142, loss = 0.13658348\n",
      "Iteration 143, loss = 0.12050615\n",
      "Iteration 144, loss = 0.11896887\n",
      "Iteration 145, loss = 0.14432046\n",
      "Iteration 146, loss = 0.14841159\n",
      "Iteration 147, loss = 0.16406804\n",
      "Iteration 148, loss = 0.13899221\n",
      "Iteration 149, loss = 0.13436072\n",
      "Iteration 150, loss = 0.11443367\n",
      "Iteration 151, loss = 0.11690340\n",
      "Iteration 152, loss = 0.13802887\n",
      "Iteration 153, loss = 0.16856734\n",
      "Iteration 154, loss = 0.12255321\n",
      "Iteration 155, loss = 0.11874129\n",
      "Iteration 156, loss = 0.12637293\n",
      "Iteration 157, loss = 0.11846253\n",
      "Iteration 158, loss = 0.11185851\n",
      "Iteration 159, loss = 0.12461617\n",
      "Iteration 160, loss = 0.11752468\n",
      "Iteration 161, loss = 0.16743937\n",
      "Iteration 162, loss = 0.12915222\n",
      "Iteration 163, loss = 0.11352062\n",
      "Iteration 164, loss = 0.13227718\n",
      "Iteration 165, loss = 0.13582349\n",
      "Iteration 166, loss = 0.11410204\n",
      "Iteration 167, loss = 0.12281665\n",
      "Iteration 168, loss = 0.11842109\n",
      "Iteration 169, loss = 0.11392306\n",
      "Iteration 170, loss = 0.11392705\n",
      "Iteration 171, loss = 0.11188875\n",
      "Iteration 172, loss = 0.11593859\n",
      "Iteration 173, loss = 0.11573331\n",
      "Iteration 174, loss = 0.13183700\n",
      "Iteration 175, loss = 0.11998769\n",
      "Iteration 176, loss = 0.12525520\n",
      "Iteration 177, loss = 0.16076352\n",
      "Iteration 178, loss = 0.12650553\n",
      "Iteration 179, loss = 0.11724588\n",
      "Iteration 180, loss = 0.12990166\n",
      "Iteration 181, loss = 0.11826633\n",
      "Iteration 182, loss = 0.11247208\n",
      "Iteration 183, loss = 0.12562097\n",
      "Iteration 184, loss = 0.11978523\n",
      "Iteration 185, loss = 0.13314695\n",
      "Iteration 186, loss = 0.13190193\n",
      "Iteration 187, loss = 0.11416563\n",
      "Iteration 188, loss = 0.11807928\n",
      "Iteration 189, loss = 0.11416499\n",
      "Iteration 190, loss = 0.12649418\n",
      "Iteration 191, loss = 0.11398067\n",
      "Iteration 192, loss = 0.11418346\n",
      "Iteration 193, loss = 0.11424096\n",
      "Iteration 194, loss = 0.10819047\n",
      "Iteration 195, loss = 0.11917148\n",
      "Iteration 196, loss = 0.14409949\n",
      "Iteration 197, loss = 0.17121622\n",
      "Iteration 198, loss = 0.12927822\n",
      "Iteration 199, loss = 0.14504522\n",
      "Iteration 200, loss = 0.16471263\n",
      "Iteration 201, loss = 0.15367244\n",
      "Iteration 202, loss = 0.14063947\n",
      "Iteration 203, loss = 0.12465256\n",
      "Iteration 204, loss = 0.12504005\n",
      "Iteration 205, loss = 0.11710708\n",
      "Iteration 206, loss = 0.12665533\n",
      "Iteration 207, loss = 0.11595686\n",
      "Iteration 208, loss = 0.11248937\n",
      "Iteration 209, loss = 0.11443381\n",
      "Iteration 210, loss = 0.14014913\n",
      "Iteration 211, loss = 0.12787041\n",
      "Iteration 212, loss = 0.12719564\n",
      "Iteration 213, loss = 0.12329023\n",
      "Iteration 214, loss = 0.11858544\n",
      "Iteration 215, loss = 0.12714783\n",
      "Iteration 216, loss = 0.11861983\n",
      "Iteration 217, loss = 0.12159552\n",
      "Iteration 218, loss = 0.11765556\n",
      "Iteration 219, loss = 0.10913842\n",
      "Iteration 220, loss = 0.11931753\n",
      "Iteration 221, loss = 0.12474040\n",
      "Iteration 222, loss = 0.14048008\n",
      "Iteration 223, loss = 0.11885002\n",
      "Iteration 224, loss = 0.11166068\n",
      "Iteration 225, loss = 0.12264951\n",
      "Iteration 226, loss = 0.11568332\n",
      "Iteration 227, loss = 0.11447168\n",
      "Iteration 228, loss = 0.11522433\n",
      "Iteration 229, loss = 0.11673619\n",
      "Iteration 230, loss = 0.11974882\n",
      "Iteration 231, loss = 0.11183440\n",
      "Iteration 232, loss = 0.12255538\n",
      "Iteration 233, loss = 0.18666036\n",
      "Iteration 234, loss = 0.12831963\n",
      "Iteration 235, loss = 0.13204533\n",
      "Iteration 236, loss = 0.16131036\n",
      "Iteration 237, loss = 0.15987371\n",
      "Iteration 238, loss = 0.12470247\n",
      "Iteration 239, loss = 0.12516034\n",
      "Iteration 240, loss = 0.12552724\n",
      "Iteration 241, loss = 0.12600601\n",
      "Iteration 242, loss = 0.14773790\n",
      "Iteration 243, loss = 0.11902692\n",
      "Iteration 244, loss = 0.13537463\n",
      "Iteration 245, loss = 0.12948631\n",
      "Iteration 246, loss = 0.12695638\n",
      "Iteration 247, loss = 0.18891194\n",
      "Iteration 248, loss = 0.16401378\n",
      "Iteration 249, loss = 0.13627957\n",
      "Iteration 250, loss = 0.11960369\n",
      "Iteration 251, loss = 0.16202819\n",
      "Iteration 252, loss = 0.15922248\n",
      "Iteration 253, loss = 0.16917538\n",
      "Iteration 254, loss = 0.13969318\n",
      "Iteration 255, loss = 0.15308685\n",
      "Iteration 256, loss = 0.14465179\n",
      "Iteration 257, loss = 0.13364519\n",
      "Iteration 258, loss = 0.12051262\n",
      "Iteration 259, loss = 0.12381074\n",
      "Iteration 260, loss = 0.12774962\n",
      "Iteration 261, loss = 0.22860168\n",
      "Iteration 262, loss = 0.23391421\n",
      "Iteration 263, loss = 0.27162058\n",
      "Iteration 264, loss = 0.16151996\n",
      "Iteration 265, loss = 0.23946318\n",
      "Iteration 266, loss = 0.29556746\n",
      "Iteration 267, loss = 0.22266130\n",
      "Iteration 268, loss = 0.13157072\n",
      "Iteration 269, loss = 0.15467545\n",
      "Iteration 270, loss = 0.13533259\n",
      "Iteration 271, loss = 0.15275811\n",
      "Iteration 272, loss = 0.13695031\n",
      "Iteration 273, loss = 0.12593310\n",
      "Iteration 274, loss = 0.11362197\n",
      "Iteration 275, loss = 0.13793334\n",
      "Iteration 276, loss = 0.11450955\n",
      "Iteration 277, loss = 0.13316638\n",
      "Iteration 278, loss = 0.13992511\n",
      "Iteration 279, loss = 0.11730349\n",
      "Iteration 280, loss = 0.15169254\n",
      "Iteration 281, loss = 0.13608288\n",
      "Iteration 282, loss = 0.16203647\n",
      "Iteration 283, loss = 0.13126002\n",
      "Iteration 284, loss = 0.13085721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 285, loss = 0.11949055\n",
      "Iteration 286, loss = 0.11816943\n",
      "Iteration 287, loss = 0.11836720\n",
      "Iteration 288, loss = 0.11633315\n",
      "Iteration 289, loss = 0.12489313\n",
      "Iteration 290, loss = 0.11584903\n",
      "Iteration 291, loss = 0.11670792\n",
      "Iteration 292, loss = 0.11934413\n",
      "Iteration 293, loss = 0.12002565\n",
      "Iteration 294, loss = 0.12231593\n",
      "Iteration 295, loss = 0.12224518\n",
      "Iteration 296, loss = 0.11591724\n",
      "Iteration 297, loss = 0.13219340\n",
      "Iteration 298, loss = 0.14378290\n",
      "Iteration 299, loss = 0.11710362\n",
      "Iteration 300, loss = 0.12544244\n",
      "Iteration 301, loss = 0.18653189\n",
      "Iteration 302, loss = 0.11879569\n",
      "Iteration 303, loss = 0.12107665\n",
      "Iteration 304, loss = 0.20666088\n",
      "Iteration 305, loss = 0.14186248\n",
      "Iteration 306, loss = 0.19187438\n",
      "Iteration 307, loss = 0.12222404\n",
      "Iteration 308, loss = 0.12493083\n",
      "Iteration 309, loss = 0.12690853\n",
      "Iteration 310, loss = 0.14522310\n",
      "Iteration 311, loss = 0.13489671\n",
      "Iteration 312, loss = 0.13322398\n",
      "Iteration 313, loss = 0.12760501\n",
      "Iteration 314, loss = 0.12313908\n",
      "Iteration 315, loss = 0.12773447\n",
      "Iteration 316, loss = 0.12087939\n",
      "Iteration 317, loss = 0.12309262\n",
      "Iteration 318, loss = 0.11734059\n",
      "Iteration 319, loss = 0.11828347\n",
      "Iteration 320, loss = 0.11587770\n",
      "Iteration 321, loss = 0.14876428\n",
      "Iteration 322, loss = 0.13368472\n",
      "Iteration 323, loss = 0.14527329\n",
      "Iteration 324, loss = 0.17016368\n",
      "Iteration 325, loss = 0.14377005\n",
      "Iteration 326, loss = 0.16585902\n",
      "Iteration 327, loss = 0.13134350\n",
      "Iteration 328, loss = 0.13833559\n",
      "Iteration 329, loss = 0.11893919\n",
      "Iteration 330, loss = 0.11541275\n",
      "Iteration 331, loss = 0.11935665\n",
      "Iteration 332, loss = 0.13524963\n",
      "Iteration 333, loss = 0.15792853\n",
      "Iteration 334, loss = 0.12891930\n",
      "Iteration 335, loss = 0.22579796\n",
      "Iteration 336, loss = 0.47312583\n",
      "Iteration 337, loss = 0.49956190\n",
      "Iteration 338, loss = 0.26563343\n",
      "Iteration 339, loss = 0.20347056\n",
      "Iteration 340, loss = 0.18684391\n",
      "Iteration 341, loss = 0.20849232\n",
      "Iteration 342, loss = 0.21615999\n",
      "Iteration 343, loss = 0.12569722\n",
      "Iteration 344, loss = 0.20746942\n",
      "Iteration 345, loss = 0.24341041\n",
      "Iteration 346, loss = 0.19518296\n",
      "Iteration 347, loss = 0.14899313\n",
      "Iteration 348, loss = 0.18039871\n",
      "Iteration 349, loss = 0.16134644\n",
      "Iteration 350, loss = 0.16585554\n",
      "Iteration 351, loss = 0.12088005\n",
      "Iteration 352, loss = 0.13234840\n",
      "Iteration 353, loss = 0.14562133\n",
      "Iteration 354, loss = 0.16712488\n",
      "Iteration 355, loss = 0.19386351\n",
      "Iteration 356, loss = 0.23954283\n",
      "Iteration 357, loss = 0.22439734\n",
      "Iteration 358, loss = 0.23957291\n",
      "Iteration 359, loss = 0.19242979\n",
      "Iteration 360, loss = 0.21930505\n",
      "Iteration 361, loss = 0.18114329\n",
      "Iteration 362, loss = 0.18192721\n",
      "Iteration 363, loss = 0.15401122\n",
      "Iteration 364, loss = 0.20808201\n",
      "Iteration 365, loss = 0.16340231\n",
      "Iteration 366, loss = 0.35109427\n",
      "Iteration 367, loss = 4.33245284\n",
      "Iteration 368, loss = 4.70665449\n",
      "Iteration 369, loss = 1.05328986\n",
      "Iteration 370, loss = 1.32237156\n",
      "Iteration 371, loss = 4.07872818\n",
      "Iteration 372, loss = 1.49824571\n",
      "Iteration 373, loss = 1.68611031\n",
      "Iteration 374, loss = 1.18848758\n",
      "Iteration 375, loss = 0.92161724\n",
      "Iteration 376, loss = 1.11242571\n",
      "Iteration 377, loss = 0.75265208\n",
      "Iteration 378, loss = 0.62759026\n",
      "Iteration 379, loss = 0.81014006\n",
      "Iteration 380, loss = 1.52193652\n",
      "Iteration 381, loss = 0.68549914\n",
      "Iteration 382, loss = 1.00516944\n",
      "Iteration 383, loss = 0.74966350\n",
      "Iteration 384, loss = 0.71125524\n",
      "Iteration 385, loss = 0.71412583\n",
      "Iteration 386, loss = 0.85253444\n",
      "Iteration 387, loss = 0.80302428\n",
      "Iteration 388, loss = 0.72712024\n",
      "Iteration 389, loss = 0.39943865\n",
      "Iteration 390, loss = 0.45785069\n",
      "Iteration 391, loss = 0.45612494\n",
      "Iteration 392, loss = 0.35527649\n",
      "Iteration 393, loss = 0.29710637\n",
      "Iteration 394, loss = 0.48189947\n",
      "Iteration 395, loss = 0.37062534\n",
      "Iteration 396, loss = 0.27011139\n",
      "Iteration 397, loss = 0.26499039\n",
      "Iteration 398, loss = 0.18355099\n",
      "Iteration 399, loss = 0.31946036\n",
      "Iteration 400, loss = 0.25585383\n",
      "Iteration 401, loss = 0.28339107\n",
      "Iteration 402, loss = 0.23151151\n",
      "Iteration 403, loss = 0.28212971\n",
      "Iteration 404, loss = 0.17031892\n",
      "Iteration 405, loss = 0.20776307\n",
      "Iteration 406, loss = 0.27572506\n",
      "Iteration 407, loss = 0.38250139\n",
      "Iteration 408, loss = 0.46477556\n",
      "Iteration 409, loss = 0.43170088\n",
      "Iteration 410, loss = 0.62113158\n",
      "Iteration 411, loss = 0.63462276\n",
      "Iteration 412, loss = 0.65313314\n",
      "Iteration 413, loss = 0.47851198\n",
      "Iteration 414, loss = 0.43045161\n",
      "Iteration 415, loss = 0.35050510\n",
      "Iteration 416, loss = 0.41280389\n",
      "Iteration 417, loss = 0.33447523\n",
      "Iteration 418, loss = 0.22426036\n",
      "Iteration 419, loss = 0.20669617\n",
      "Iteration 420, loss = 0.24484617\n",
      "Iteration 421, loss = 0.21243982\n",
      "Iteration 422, loss = 0.16182942\n",
      "Iteration 423, loss = 0.15505605\n",
      "Iteration 424, loss = 0.14775332\n",
      "Iteration 425, loss = 0.12782316\n",
      "Iteration 426, loss = 0.13584026\n",
      "Iteration 427, loss = 0.16763154\n",
      "Iteration 428, loss = 0.17071131\n",
      "Iteration 429, loss = 0.14277608\n",
      "Iteration 430, loss = 0.12316866\n",
      "Iteration 431, loss = 0.12874924\n",
      "Iteration 432, loss = 0.11740209\n",
      "Iteration 433, loss = 0.11474928\n",
      "Iteration 434, loss = 0.11617266\n",
      "Iteration 435, loss = 0.13069876\n",
      "Iteration 436, loss = 0.14909032\n",
      "Iteration 437, loss = 0.13186923\n",
      "Iteration 438, loss = 0.15459238\n",
      "Iteration 439, loss = 0.15758985\n",
      "Iteration 440, loss = 0.12113774\n",
      "Iteration 441, loss = 0.12415854\n",
      "Iteration 442, loss = 0.11737393\n",
      "Iteration 443, loss = 0.11590511\n",
      "Iteration 444, loss = 0.11302385\n",
      "Iteration 445, loss = 0.12380430\n",
      "Iteration 446, loss = 0.11637655\n",
      "Iteration 447, loss = 0.12214965\n",
      "Iteration 448, loss = 0.13018808\n",
      "Iteration 449, loss = 0.12271327\n",
      "Iteration 450, loss = 0.12690163\n",
      "Iteration 451, loss = 0.12323453\n",
      "Iteration 452, loss = 0.11925965\n",
      "Iteration 453, loss = 0.12697317\n",
      "Iteration 454, loss = 0.12962883\n",
      "Iteration 455, loss = 0.12558846\n",
      "Iteration 456, loss = 0.14037657\n",
      "Iteration 457, loss = 0.12644801\n",
      "Iteration 458, loss = 0.16240954\n",
      "Iteration 459, loss = 0.14231000\n",
      "Iteration 460, loss = 0.14741785\n",
      "Iteration 461, loss = 0.14858669\n",
      "Iteration 462, loss = 0.12149022\n",
      "Iteration 463, loss = 0.15007463\n",
      "Iteration 464, loss = 0.12666595\n",
      "Iteration 465, loss = 0.12622731\n",
      "Iteration 466, loss = 0.14402190\n",
      "Iteration 467, loss = 0.14881474\n",
      "Iteration 468, loss = 0.13678595\n",
      "Iteration 469, loss = 0.12813130\n",
      "Iteration 470, loss = 0.13250204\n",
      "Iteration 471, loss = 0.13108869\n",
      "Iteration 472, loss = 0.12325324\n",
      "Iteration 473, loss = 0.13008763\n",
      "Iteration 474, loss = 0.13127260\n",
      "Iteration 475, loss = 0.12452255\n",
      "Iteration 476, loss = 0.13582530\n",
      "Iteration 477, loss = 0.12758235\n",
      "Iteration 478, loss = 0.13076397\n",
      "Iteration 479, loss = 0.12378662\n",
      "Iteration 480, loss = 0.11777905\n",
      "Iteration 481, loss = 0.11926346\n",
      "Iteration 482, loss = 0.12344147\n",
      "Iteration 483, loss = 0.11859405\n",
      "Iteration 484, loss = 0.13424098\n",
      "Iteration 485, loss = 0.12678701\n",
      "Iteration 486, loss = 0.15040248\n",
      "Iteration 487, loss = 0.13956025\n",
      "Iteration 488, loss = 0.14376829\n",
      "Iteration 489, loss = 0.12724708\n",
      "Iteration 490, loss = 0.11899967\n",
      "Iteration 491, loss = 0.13604017\n",
      "Iteration 492, loss = 0.12787581\n",
      "Iteration 493, loss = 0.13548275\n",
      "Iteration 494, loss = 0.12337315\n",
      "Iteration 495, loss = 0.11779144\n",
      "Iteration 496, loss = 0.12691548\n",
      "Iteration 497, loss = 0.15101737\n",
      "Iteration 498, loss = 0.11678481\n",
      "Iteration 499, loss = 0.11544270\n",
      "Iteration 500, loss = 0.12839932\n",
      "Iteration 501, loss = 0.12659637\n",
      "Iteration 502, loss = 0.12423081\n",
      "Iteration 503, loss = 0.12117117\n",
      "Iteration 504, loss = 0.12687008\n",
      "Iteration 505, loss = 0.12663784\n",
      "Iteration 506, loss = 0.11701859\n",
      "Iteration 507, loss = 0.13883954\n",
      "Iteration 508, loss = 0.13111220\n",
      "Iteration 509, loss = 0.12096945\n",
      "Iteration 510, loss = 0.11930643\n",
      "Iteration 511, loss = 0.11832365\n",
      "Iteration 512, loss = 0.11532280\n",
      "Iteration 513, loss = 0.11952196\n",
      "Iteration 514, loss = 0.12034273\n",
      "Iteration 515, loss = 0.12436940\n",
      "Iteration 516, loss = 0.11552501\n",
      "Iteration 517, loss = 0.12303166\n",
      "Iteration 518, loss = 0.11242715\n",
      "Iteration 519, loss = 0.11886822\n",
      "Iteration 520, loss = 0.11775769\n",
      "Iteration 521, loss = 0.11956986\n",
      "Iteration 522, loss = 0.11353275\n",
      "Iteration 523, loss = 0.11478155\n",
      "Iteration 524, loss = 0.12127179\n",
      "Iteration 525, loss = 0.14225393\n",
      "Iteration 526, loss = 0.14156594\n",
      "Iteration 527, loss = 0.14216418\n",
      "Iteration 528, loss = 0.12446204\n",
      "Iteration 529, loss = 0.12014703\n",
      "Iteration 530, loss = 0.11704040\n",
      "Iteration 531, loss = 0.11746202\n",
      "Iteration 532, loss = 0.12197887\n",
      "Iteration 533, loss = 0.11099582\n",
      "Iteration 534, loss = 0.11590041\n",
      "Iteration 535, loss = 0.12106736\n",
      "Iteration 536, loss = 0.12020288\n",
      "Iteration 537, loss = 0.11630823\n",
      "Iteration 538, loss = 0.12165164\n",
      "Iteration 539, loss = 0.12413010\n",
      "Iteration 540, loss = 0.11655711\n",
      "Iteration 541, loss = 0.12959851\n",
      "Iteration 542, loss = 0.11804220\n",
      "Iteration 543, loss = 0.11754335\n",
      "Iteration 544, loss = 0.11182016\n",
      "Iteration 545, loss = 0.14190025\n",
      "Iteration 546, loss = 0.11903497\n",
      "Iteration 547, loss = 0.11393423\n",
      "Iteration 548, loss = 0.12119510\n",
      "Iteration 549, loss = 0.11804551\n",
      "Iteration 550, loss = 0.11447044\n",
      "Iteration 551, loss = 0.11586187\n",
      "Iteration 552, loss = 0.13428560\n",
      "Iteration 553, loss = 0.15697274\n",
      "Iteration 554, loss = 0.12284265\n",
      "Iteration 555, loss = 0.11802493\n",
      "Iteration 556, loss = 0.12612147\n",
      "Iteration 557, loss = 0.12024861\n",
      "Iteration 558, loss = 0.11712979\n",
      "Iteration 559, loss = 0.11689064\n",
      "Iteration 560, loss = 0.11590696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 561, loss = 0.12148776\n",
      "Iteration 562, loss = 0.11458458\n",
      "Iteration 563, loss = 0.11313171\n",
      "Iteration 564, loss = 0.12656226\n",
      "Iteration 565, loss = 0.14185912\n",
      "Iteration 566, loss = 0.13115297\n",
      "Iteration 567, loss = 0.13650341\n",
      "Iteration 568, loss = 0.11170339\n",
      "Iteration 569, loss = 0.14933580\n",
      "Iteration 570, loss = 0.13962952\n",
      "Iteration 571, loss = 0.16972708\n",
      "Iteration 572, loss = 0.12802746\n",
      "Iteration 573, loss = 0.20903336\n",
      "Iteration 574, loss = 0.15193509\n",
      "Iteration 575, loss = 0.13022101\n",
      "Iteration 576, loss = 0.25581468\n",
      "Iteration 577, loss = 0.40326715\n",
      "Iteration 578, loss = 0.42128040\n",
      "Iteration 579, loss = 0.41327967\n",
      "Iteration 580, loss = 0.43200880\n",
      "Iteration 581, loss = 0.33269253\n",
      "Iteration 582, loss = 0.30841266\n",
      "Iteration 583, loss = 0.15109465\n",
      "Iteration 584, loss = 0.14138882\n",
      "Iteration 585, loss = 0.13875419\n",
      "Iteration 586, loss = 0.21955515\n",
      "Iteration 587, loss = 0.14980364\n",
      "Iteration 588, loss = 0.12492137\n",
      "Iteration 589, loss = 0.19002339\n",
      "Iteration 590, loss = 0.13956870\n",
      "Iteration 591, loss = 0.15101090\n",
      "Iteration 592, loss = 0.23639888\n",
      "Iteration 593, loss = 0.15517109\n",
      "Iteration 594, loss = 0.18349445\n",
      "Iteration 595, loss = 0.19393133\n",
      "Iteration 596, loss = 0.23487231\n",
      "Iteration 597, loss = 0.14770369\n",
      "Iteration 598, loss = 0.11732366\n",
      "Iteration 599, loss = 0.12901115\n",
      "Iteration 600, loss = 0.12118096\n",
      "Iteration 601, loss = 0.13238504\n",
      "Iteration 602, loss = 0.12363652\n",
      "Iteration 603, loss = 0.13724870\n",
      "Iteration 604, loss = 0.12635292\n",
      "Iteration 605, loss = 0.12168948\n",
      "Iteration 606, loss = 0.14328190\n",
      "Iteration 607, loss = 0.13858003\n",
      "Iteration 608, loss = 0.10852206\n",
      "Iteration 609, loss = 0.15350422\n",
      "Iteration 610, loss = 0.13393011\n",
      "Iteration 611, loss = 0.12682951\n",
      "Iteration 612, loss = 0.11852785\n",
      "Iteration 613, loss = 0.12499978\n",
      "Iteration 614, loss = 0.13052580\n",
      "Iteration 615, loss = 0.13150791\n",
      "Iteration 616, loss = 0.11302070\n",
      "Iteration 617, loss = 0.11890170\n",
      "Iteration 618, loss = 0.11345858\n",
      "Iteration 619, loss = 0.11499848\n",
      "Iteration 620, loss = 0.11913850\n",
      "Iteration 621, loss = 0.11712283\n",
      "Iteration 622, loss = 0.11871964\n",
      "Iteration 623, loss = 0.11228982\n",
      "Iteration 624, loss = 0.12363102\n",
      "Iteration 625, loss = 0.11306910\n",
      "Iteration 626, loss = 0.11307661\n",
      "Iteration 627, loss = 0.11776534\n",
      "Iteration 628, loss = 0.11875148\n",
      "Iteration 629, loss = 0.12305864\n",
      "Iteration 630, loss = 0.11426669\n",
      "Iteration 631, loss = 0.11590708\n",
      "Iteration 632, loss = 0.11760446\n",
      "Iteration 633, loss = 0.11875359\n",
      "Iteration 634, loss = 0.11627246\n",
      "Iteration 635, loss = 0.11832241\n",
      "Iteration 636, loss = 0.12101806\n",
      "Iteration 637, loss = 0.11208358\n",
      "Iteration 638, loss = 0.13149927\n",
      "Iteration 639, loss = 0.13227335\n",
      "Iteration 640, loss = 0.15434223\n",
      "Iteration 641, loss = 0.16305957\n",
      "Iteration 642, loss = 0.17248466\n",
      "Iteration 643, loss = 0.24286030\n",
      "Iteration 644, loss = 0.39847878\n",
      "Iteration 645, loss = 0.21348325\n",
      "Iteration 646, loss = 0.12999149\n",
      "Iteration 647, loss = 0.13039006\n",
      "Iteration 648, loss = 0.14578188\n",
      "Iteration 649, loss = 0.20076421\n",
      "Iteration 650, loss = 0.24786009\n",
      "Iteration 651, loss = 0.13039546\n",
      "Iteration 652, loss = 0.13464391\n",
      "Iteration 653, loss = 0.12944846\n",
      "Iteration 654, loss = 0.13756274\n",
      "Iteration 655, loss = 0.11626004\n",
      "Iteration 656, loss = 0.11990668\n",
      "Iteration 657, loss = 0.12615202\n",
      "Iteration 658, loss = 0.12327914\n",
      "Iteration 659, loss = 0.11960631\n",
      "Iteration 660, loss = 0.11482834\n",
      "Iteration 661, loss = 0.11642086\n",
      "Iteration 662, loss = 0.11695992\n",
      "Iteration 663, loss = 0.12856319\n",
      "Iteration 664, loss = 0.13603276\n",
      "Iteration 665, loss = 0.12169310\n",
      "Iteration 666, loss = 0.12383856\n",
      "Iteration 667, loss = 0.12412117\n",
      "Iteration 668, loss = 0.12853299\n",
      "Iteration 669, loss = 0.11655459\n",
      "Iteration 670, loss = 0.12109031\n",
      "Iteration 671, loss = 0.12998070\n",
      "Iteration 672, loss = 0.11429799\n",
      "Iteration 673, loss = 0.11583181\n",
      "Iteration 674, loss = 0.11707794\n",
      "Iteration 675, loss = 0.11723485\n",
      "Iteration 676, loss = 0.12065932\n",
      "Iteration 677, loss = 0.13521460\n",
      "Iteration 678, loss = 0.12110391\n",
      "Iteration 679, loss = 0.12817564\n",
      "Iteration 680, loss = 0.14395179\n",
      "Iteration 681, loss = 0.14403946\n",
      "Iteration 682, loss = 0.13269830\n",
      "Iteration 683, loss = 0.12275587\n",
      "Iteration 684, loss = 0.13174827\n",
      "Iteration 685, loss = 0.11740110\n",
      "Iteration 686, loss = 0.11742561\n",
      "Iteration 687, loss = 0.11921086\n",
      "Iteration 688, loss = 0.11838534\n",
      "Iteration 689, loss = 0.13029811\n",
      "Iteration 690, loss = 0.12556434\n",
      "Iteration 691, loss = 0.13651005\n",
      "Iteration 692, loss = 0.12957082\n",
      "Iteration 693, loss = 0.12337359\n",
      "Iteration 694, loss = 0.11989043\n",
      "Iteration 695, loss = 0.12696760\n",
      "Iteration 696, loss = 0.13329707\n",
      "Iteration 697, loss = 0.11929554\n",
      "Iteration 698, loss = 0.12439538\n",
      "Iteration 699, loss = 0.12438375\n",
      "Iteration 700, loss = 0.12599244\n",
      "Iteration 701, loss = 0.13918335\n",
      "Iteration 702, loss = 0.11781973\n",
      "Iteration 703, loss = 0.11634235\n",
      "Iteration 704, loss = 0.11681967\n",
      "Iteration 705, loss = 0.11330150\n",
      "Iteration 706, loss = 0.11697067\n",
      "Iteration 707, loss = 0.11372004\n",
      "Iteration 708, loss = 0.11527989\n",
      "Iteration 709, loss = 0.11311202\n",
      "Iteration 710, loss = 0.11494927\n",
      "Iteration 711, loss = 0.11364649\n",
      "Iteration 712, loss = 0.11446761\n",
      "Iteration 713, loss = 0.11622717\n",
      "Iteration 714, loss = 0.11385669\n",
      "Iteration 715, loss = 0.12490853\n",
      "Iteration 716, loss = 0.13782154\n",
      "Iteration 717, loss = 0.13252284\n",
      "Iteration 718, loss = 0.13440215\n",
      "Iteration 719, loss = 0.11269729\n",
      "Iteration 720, loss = 0.15566837\n",
      "Iteration 721, loss = 0.13856733\n",
      "Iteration 722, loss = 0.14754529\n",
      "Iteration 723, loss = 0.14711657\n",
      "Iteration 724, loss = 0.15643835\n",
      "Iteration 725, loss = 0.11954020\n",
      "Iteration 726, loss = 0.11747463\n",
      "Iteration 727, loss = 0.13044027\n",
      "Iteration 728, loss = 0.12140044\n",
      "Iteration 729, loss = 0.11990031\n",
      "Iteration 730, loss = 0.14953165\n",
      "Iteration 731, loss = 0.11031739\n",
      "Iteration 732, loss = 0.11758086\n",
      "Iteration 733, loss = 0.11869671\n",
      "Iteration 734, loss = 0.12160569\n",
      "Iteration 735, loss = 0.12371737\n",
      "Iteration 736, loss = 0.11455734\n",
      "Iteration 737, loss = 0.11956460\n",
      "Iteration 738, loss = 0.13081241\n",
      "Iteration 739, loss = 0.12520956\n",
      "Iteration 740, loss = 0.12323534\n",
      "Iteration 741, loss = 0.11606929\n",
      "Iteration 742, loss = 0.12055355\n",
      "Iteration 743, loss = 0.12343307\n",
      "Iteration 744, loss = 0.11359135\n",
      "Iteration 745, loss = 0.11702320\n",
      "Iteration 746, loss = 0.11436326\n",
      "Iteration 747, loss = 0.11212860\n",
      "Iteration 748, loss = 0.11368100\n",
      "Iteration 749, loss = 0.12421089\n",
      "Iteration 750, loss = 0.12287386\n",
      "Iteration 751, loss = 0.11266950\n",
      "Iteration 752, loss = 0.11510359\n",
      "Iteration 753, loss = 0.12811510\n",
      "Iteration 754, loss = 0.11905760\n",
      "Iteration 755, loss = 0.14108465\n",
      "Iteration 756, loss = 0.15788904\n",
      "Iteration 757, loss = 0.14521732\n",
      "Iteration 758, loss = 0.11726823\n",
      "Iteration 759, loss = 0.15997793\n",
      "Iteration 760, loss = 0.20001799\n",
      "Iteration 761, loss = 0.15257014\n",
      "Iteration 762, loss = 0.26391757\n",
      "Iteration 763, loss = 0.23001603\n",
      "Iteration 764, loss = 0.48232145\n",
      "Iteration 765, loss = 0.43601195\n",
      "Iteration 766, loss = 0.75158347\n",
      "Iteration 767, loss = 2.32144963\n",
      "Iteration 768, loss = 1.95777749\n",
      "Iteration 769, loss = 1.03816154\n",
      "Iteration 770, loss = 1.31792853\n",
      "Iteration 771, loss = 0.92785516\n",
      "Iteration 772, loss = 0.70761722\n",
      "Iteration 773, loss = 1.22883185\n",
      "Iteration 774, loss = 1.14888319\n",
      "Iteration 775, loss = 1.26735932\n",
      "Iteration 776, loss = 1.25945946\n",
      "Iteration 777, loss = 0.88334241\n",
      "Iteration 778, loss = 0.48704011\n",
      "Iteration 779, loss = 0.59240063\n",
      "Iteration 780, loss = 0.60337668\n",
      "Iteration 781, loss = 0.46014071\n",
      "Iteration 782, loss = 0.34859359\n",
      "Iteration 783, loss = 0.30687426\n",
      "Iteration 784, loss = 0.46988580\n",
      "Iteration 785, loss = 0.63082263\n",
      "Iteration 786, loss = 0.43100885\n",
      "Iteration 787, loss = 0.17710409\n",
      "Iteration 788, loss = 0.21415946\n",
      "Iteration 789, loss = 0.25128638\n",
      "Iteration 790, loss = 0.15816266\n",
      "Iteration 791, loss = 0.17555313\n",
      "Iteration 792, loss = 0.17905070\n",
      "Iteration 793, loss = 0.18161369\n",
      "Iteration 794, loss = 0.12723250\n",
      "Iteration 795, loss = 0.15713620\n",
      "Iteration 796, loss = 0.12030760\n",
      "Iteration 797, loss = 0.23284266\n",
      "Iteration 798, loss = 0.35185776\n",
      "Iteration 799, loss = 0.30815333\n",
      "Iteration 800, loss = 0.16325213\n",
      "Iteration 801, loss = 0.12731882\n",
      "Iteration 802, loss = 0.15595229\n",
      "Iteration 803, loss = 0.13985059\n",
      "Iteration 804, loss = 0.11538377\n",
      "Iteration 805, loss = 0.13139907\n",
      "Iteration 806, loss = 0.12796334\n",
      "Iteration 807, loss = 0.14153734\n",
      "Iteration 808, loss = 0.12751927\n",
      "Iteration 809, loss = 0.15746445\n",
      "Iteration 810, loss = 0.22519979\n",
      "Iteration 811, loss = 0.50536236\n",
      "Iteration 812, loss = 0.46647313\n",
      "Iteration 813, loss = 0.60391096\n",
      "Iteration 814, loss = 0.77311378\n",
      "Iteration 815, loss = 0.59260652\n",
      "Iteration 816, loss = 0.48701104\n",
      "Iteration 817, loss = 0.62575426\n",
      "Iteration 818, loss = 1.85541176\n",
      "Iteration 819, loss = 3.56015639\n",
      "Iteration 820, loss = 3.80391251\n",
      "Iteration 821, loss = 3.90749437\n",
      "Iteration 822, loss = 4.06225890\n",
      "Iteration 823, loss = 1.91721883\n",
      "Iteration 824, loss = 1.70152560\n",
      "Iteration 825, loss = 1.36584835\n",
      "Iteration 826, loss = 1.23289019\n",
      "Iteration 827, loss = 0.88223304\n",
      "Iteration 828, loss = 1.08657195\n",
      "Iteration 829, loss = 0.96302807\n",
      "Iteration 830, loss = 1.14125050\n",
      "Iteration 831, loss = 0.90785577\n",
      "Iteration 832, loss = 0.72890985\n",
      "Iteration 833, loss = 1.84520142\n",
      "Iteration 834, loss = 1.08288193\n",
      "Iteration 835, loss = 1.34530198\n",
      "Iteration 836, loss = 0.52546354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 837, loss = 1.29553529\n",
      "Iteration 838, loss = 0.92886650\n",
      "Iteration 839, loss = 0.77665930\n",
      "Iteration 840, loss = 0.57926100\n",
      "Iteration 841, loss = 0.51083189\n",
      "Iteration 842, loss = 0.87451034\n",
      "Iteration 843, loss = 0.54958108\n",
      "Iteration 844, loss = 0.36329039\n",
      "Iteration 845, loss = 0.80306950\n",
      "Iteration 846, loss = 0.39098262\n",
      "Iteration 847, loss = 0.67365949\n",
      "Iteration 848, loss = 0.62424032\n",
      "Iteration 849, loss = 0.66048100\n",
      "Iteration 850, loss = 0.51386646\n",
      "Iteration 851, loss = 0.41850018\n",
      "Iteration 852, loss = 0.66680992\n",
      "Iteration 853, loss = 0.62150097\n",
      "Iteration 854, loss = 0.50960948\n",
      "Iteration 855, loss = 0.29460885\n",
      "Iteration 856, loss = 0.26295187\n",
      "Iteration 857, loss = 0.37505873\n",
      "Iteration 858, loss = 0.25754402\n",
      "Iteration 859, loss = 0.34399297\n",
      "Iteration 860, loss = 0.37557496\n",
      "Iteration 861, loss = 0.37966310\n",
      "Iteration 862, loss = 0.29689827\n",
      "Iteration 863, loss = 0.22060937\n",
      "Iteration 864, loss = 0.21704873\n",
      "Iteration 865, loss = 0.23621816\n",
      "Iteration 866, loss = 0.24846180\n",
      "Iteration 867, loss = 0.18750419\n",
      "Iteration 868, loss = 0.18372666\n",
      "Iteration 869, loss = 0.17798502\n",
      "Iteration 870, loss = 0.38392994\n",
      "Iteration 871, loss = 0.68960745\n",
      "Iteration 872, loss = 0.76193654\n",
      "Iteration 873, loss = 0.57336656\n",
      "Iteration 874, loss = 0.46107516\n",
      "Iteration 875, loss = 0.52137129\n",
      "Iteration 876, loss = 0.71843939\n",
      "Iteration 877, loss = 0.63085457\n",
      "Iteration 878, loss = 0.55249427\n",
      "Iteration 879, loss = 0.50594954\n",
      "Iteration 880, loss = 0.47426820\n",
      "Iteration 881, loss = 0.75579324\n",
      "Iteration 882, loss = 0.59684455\n",
      "Iteration 883, loss = 0.39259904\n",
      "Iteration 884, loss = 0.38310266\n",
      "Iteration 885, loss = 0.33602302\n",
      "Iteration 886, loss = 0.26115641\n",
      "Iteration 887, loss = 0.14650196\n",
      "Iteration 888, loss = 0.20047300\n",
      "Iteration 889, loss = 0.16564638\n",
      "Iteration 890, loss = 0.19101388\n",
      "Iteration 891, loss = 0.16426590\n",
      "Iteration 892, loss = 0.15684548\n",
      "Iteration 893, loss = 0.13199697\n",
      "Iteration 894, loss = 0.14055092\n",
      "Iteration 895, loss = 0.13489363\n",
      "Iteration 896, loss = 0.13908151\n",
      "Iteration 897, loss = 0.12983396\n",
      "Iteration 898, loss = 0.17451034\n",
      "Iteration 899, loss = 0.12876268\n",
      "Iteration 900, loss = 0.12202979\n",
      "Iteration 901, loss = 0.13061746\n",
      "Iteration 902, loss = 0.15024073\n",
      "Iteration 903, loss = 0.24557528\n",
      "Iteration 904, loss = 0.18352961\n",
      "Iteration 905, loss = 0.18235037\n",
      "Iteration 906, loss = 0.18772104\n",
      "Iteration 907, loss = 0.21834794\n",
      "Iteration 908, loss = 0.21785746\n",
      "Iteration 909, loss = 0.14003075\n",
      "Iteration 910, loss = 0.13077089\n",
      "Iteration 911, loss = 0.16414207\n",
      "Iteration 912, loss = 0.14218543\n",
      "Iteration 913, loss = 0.12398745\n",
      "Iteration 914, loss = 0.12408831\n",
      "Iteration 915, loss = 0.12144218\n",
      "Iteration 916, loss = 0.12229451\n",
      "Iteration 917, loss = 0.14191496\n",
      "Iteration 918, loss = 0.12306712\n",
      "Iteration 919, loss = 0.15137834\n",
      "Iteration 920, loss = 0.13522822\n",
      "Iteration 921, loss = 0.18177536\n",
      "Iteration 922, loss = 0.15373970\n",
      "Iteration 923, loss = 0.11877278\n",
      "Iteration 924, loss = 0.12743104\n",
      "Iteration 925, loss = 0.14517131\n",
      "Iteration 926, loss = 0.11979711\n",
      "Iteration 927, loss = 0.13487332\n",
      "Iteration 928, loss = 0.14661227\n",
      "Iteration 929, loss = 0.17186021\n",
      "Iteration 930, loss = 0.13675816\n",
      "Iteration 931, loss = 0.12927063\n",
      "Iteration 932, loss = 0.14987446\n",
      "Iteration 933, loss = 0.15767369\n",
      "Iteration 934, loss = 0.19679901\n",
      "Iteration 935, loss = 0.54017086\n",
      "Iteration 936, loss = 0.41800583\n",
      "Iteration 937, loss = 0.27060454\n",
      "Iteration 938, loss = 0.23992493\n",
      "Iteration 939, loss = 0.23494772\n",
      "Iteration 940, loss = 0.22131441\n",
      "Iteration 941, loss = 0.14598758\n",
      "Iteration 942, loss = 0.59147297\n",
      "Iteration 943, loss = 0.87036305\n",
      "Iteration 944, loss = 0.82078818\n",
      "Iteration 945, loss = 1.03464561\n",
      "Iteration 946, loss = 0.74297108\n",
      "Iteration 947, loss = 0.48216840\n",
      "Iteration 948, loss = 0.33551542\n",
      "Iteration 949, loss = 0.31643833\n",
      "Iteration 950, loss = 0.40252622\n",
      "Iteration 951, loss = 0.37578454\n",
      "Iteration 952, loss = 0.29513734\n",
      "Iteration 953, loss = 0.22386185\n",
      "Iteration 954, loss = 0.17876654\n",
      "Iteration 955, loss = 0.14218153\n",
      "Iteration 956, loss = 0.18695166\n",
      "Iteration 957, loss = 0.36769514\n",
      "Iteration 958, loss = 0.25960363\n",
      "Iteration 959, loss = 0.19033554\n",
      "Iteration 960, loss = 0.14955824\n",
      "Iteration 961, loss = 0.13422209\n",
      "Iteration 962, loss = 0.12154451\n",
      "Iteration 963, loss = 0.13431087\n",
      "Iteration 964, loss = 0.11297375\n",
      "Iteration 965, loss = 0.13123683\n",
      "Iteration 966, loss = 0.12273728\n",
      "Iteration 967, loss = 0.14897378\n",
      "Iteration 968, loss = 0.15763280\n",
      "Iteration 969, loss = 0.15281975\n",
      "Iteration 970, loss = 0.13018389\n",
      "Iteration 971, loss = 0.12233652\n",
      "Iteration 972, loss = 0.12519723\n",
      "Iteration 973, loss = 0.11706696\n",
      "Iteration 974, loss = 0.15122358\n",
      "Iteration 975, loss = 0.14432927\n",
      "Iteration 976, loss = 0.12900421\n",
      "Iteration 977, loss = 0.11372752\n",
      "Iteration 978, loss = 0.12284720\n",
      "Iteration 979, loss = 0.11842555\n",
      "Iteration 980, loss = 0.11874279\n",
      "Iteration 981, loss = 0.11426919\n",
      "Iteration 982, loss = 0.11579173\n",
      "Iteration 983, loss = 0.12006877\n",
      "Iteration 984, loss = 0.13543509\n",
      "Iteration 985, loss = 0.13797119\n",
      "Iteration 986, loss = 0.15736834\n",
      "Iteration 987, loss = 0.13454982\n",
      "Iteration 988, loss = 0.12814558\n",
      "Iteration 989, loss = 0.15334194\n",
      "Iteration 990, loss = 0.11774217\n",
      "Iteration 991, loss = 0.12276409\n",
      "Iteration 992, loss = 0.12644589\n",
      "Iteration 993, loss = 0.11651975\n",
      "Iteration 994, loss = 0.13201371\n",
      "Iteration 995, loss = 0.11482807\n",
      "Iteration 996, loss = 0.12556329\n",
      "Iteration 997, loss = 0.14560167\n",
      "Iteration 998, loss = 0.12836419\n",
      "Iteration 999, loss = 0.15506442\n",
      "Iteration 1000, loss = 0.14801848\n",
      "Iteration 1001, loss = 0.14796156\n",
      "Iteration 1002, loss = 0.14592963\n",
      "Iteration 1003, loss = 0.13762818\n",
      "Iteration 1004, loss = 0.12810405\n",
      "Iteration 1005, loss = 0.12924488\n",
      "Iteration 1006, loss = 0.13580801\n",
      "Iteration 1007, loss = 0.14724955\n",
      "Iteration 1008, loss = 0.15473444\n",
      "Iteration 1009, loss = 0.20135034\n",
      "Iteration 1010, loss = 0.11826813\n",
      "Iteration 1011, loss = 0.15733929\n",
      "Iteration 1012, loss = 0.23087148\n",
      "Iteration 1013, loss = 0.15321573\n",
      "Iteration 1014, loss = 0.16515721\n",
      "Iteration 1015, loss = 0.26645849\n",
      "Iteration 1016, loss = 0.22245950\n",
      "Iteration 1017, loss = 0.18041038\n",
      "Iteration 1018, loss = 0.25276019\n",
      "Iteration 1019, loss = 0.26070690\n",
      "Iteration 1020, loss = 0.18103696\n",
      "Iteration 1021, loss = 0.14397777\n",
      "Iteration 1022, loss = 0.13874486\n",
      "Iteration 1023, loss = 0.14599786\n",
      "Iteration 1024, loss = 0.17803913\n",
      "Iteration 1025, loss = 0.11817563\n",
      "Iteration 1026, loss = 0.12494223\n",
      "Iteration 1027, loss = 0.14415069\n",
      "Iteration 1028, loss = 0.13060069\n",
      "Iteration 1029, loss = 0.13764976\n",
      "Iteration 1030, loss = 0.14439507\n",
      "Iteration 1031, loss = 0.14460763\n",
      "Iteration 1032, loss = 0.11948644\n",
      "Iteration 1033, loss = 0.13383359\n",
      "Iteration 1034, loss = 0.12082682\n",
      "Iteration 1035, loss = 0.12050299\n",
      "Iteration 1036, loss = 0.11284063\n",
      "Iteration 1037, loss = 0.12020764\n",
      "Iteration 1038, loss = 0.12794018\n",
      "Iteration 1039, loss = 0.11559704\n",
      "Iteration 1040, loss = 0.12235698\n",
      "Iteration 1041, loss = 0.11514639\n",
      "Iteration 1042, loss = 0.12687811\n",
      "Iteration 1043, loss = 0.18079539\n",
      "Iteration 1044, loss = 0.14597132\n",
      "Iteration 1045, loss = 0.12706005\n",
      "Iteration 1046, loss = 0.14040999\n",
      "Iteration 1047, loss = 0.13422416\n",
      "Iteration 1048, loss = 0.13715239\n",
      "Iteration 1049, loss = 0.11887733\n",
      "Iteration 1050, loss = 0.12768054\n",
      "Iteration 1051, loss = 0.12657345\n",
      "Iteration 1052, loss = 0.12975074\n",
      "Iteration 1053, loss = 0.16709911\n",
      "Iteration 1054, loss = 0.13415816\n",
      "Iteration 1055, loss = 0.11929027\n",
      "Iteration 1056, loss = 0.12894272\n",
      "Iteration 1057, loss = 0.12879326\n",
      "Iteration 1058, loss = 0.11670520\n",
      "Iteration 1059, loss = 0.13188097\n",
      "Iteration 1060, loss = 0.12899169\n",
      "Iteration 1061, loss = 0.11807987\n",
      "Iteration 1062, loss = 0.12375269\n",
      "Iteration 1063, loss = 0.11720065\n",
      "Iteration 1064, loss = 0.12670829\n",
      "Iteration 1065, loss = 0.12226635\n",
      "Iteration 1066, loss = 0.16152854\n",
      "Iteration 1067, loss = 0.13596931\n",
      "Iteration 1068, loss = 0.12234172\n",
      "Iteration 1069, loss = 0.13518548\n",
      "Iteration 1070, loss = 0.12684193\n",
      "Iteration 1071, loss = 0.13159843\n",
      "Iteration 1072, loss = 0.11686992\n",
      "Iteration 1073, loss = 0.11671164\n",
      "Iteration 1074, loss = 0.12549766\n",
      "Iteration 1075, loss = 0.12063396\n",
      "Iteration 1076, loss = 0.11539625\n",
      "Iteration 1077, loss = 0.12882267\n",
      "Iteration 1078, loss = 0.11422624\n",
      "Iteration 1079, loss = 0.12711103\n",
      "Iteration 1080, loss = 0.13088936\n",
      "Iteration 1081, loss = 0.15177402\n",
      "Iteration 1082, loss = 0.14681032\n",
      "Iteration 1083, loss = 0.13048753\n",
      "Iteration 1084, loss = 0.12595941\n",
      "Iteration 1085, loss = 0.21896854\n",
      "Iteration 1086, loss = 0.14967579\n",
      "Iteration 1087, loss = 0.14544760\n",
      "Iteration 1088, loss = 0.23346044\n",
      "Iteration 1089, loss = 0.20129121\n",
      "Iteration 1090, loss = 0.14560312\n",
      "Iteration 1091, loss = 0.13880265\n",
      "Iteration 1092, loss = 0.14330492\n",
      "Iteration 1093, loss = 0.13732058\n",
      "Iteration 1094, loss = 0.12669226\n",
      "Iteration 1095, loss = 0.12743241\n",
      "Iteration 1096, loss = 0.13296121\n",
      "Iteration 1097, loss = 0.17510401\n",
      "Iteration 1098, loss = 0.17427996\n",
      "Iteration 1099, loss = 0.13890747\n",
      "Iteration 1100, loss = 0.12739857\n",
      "Iteration 1101, loss = 0.12481467\n",
      "Iteration 1102, loss = 0.12540696\n",
      "Iteration 1103, loss = 0.17172966\n",
      "Iteration 1104, loss = 0.18928443\n",
      "Iteration 1105, loss = 0.13552863\n",
      "Iteration 1106, loss = 0.11960696\n",
      "Iteration 1107, loss = 0.12141022\n",
      "Iteration 1108, loss = 0.12500712\n",
      "Iteration 1109, loss = 0.13098495\n",
      "Iteration 1110, loss = 0.12116285\n",
      "Iteration 1111, loss = 0.12731339\n",
      "Iteration 1112, loss = 0.11845505\n",
      "Iteration 1113, loss = 0.14927657\n",
      "Iteration 1114, loss = 0.12749878\n",
      "Iteration 1115, loss = 0.13179167\n",
      "Iteration 1116, loss = 0.12253163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1117, loss = 0.12075729\n",
      "Iteration 1118, loss = 0.11373127\n",
      "Iteration 1119, loss = 0.14059616\n",
      "Iteration 1120, loss = 0.14386415\n",
      "Iteration 1121, loss = 0.13329924\n",
      "Iteration 1122, loss = 0.14325117\n",
      "Iteration 1123, loss = 0.14150390\n",
      "Iteration 1124, loss = 0.11448866\n",
      "Iteration 1125, loss = 0.13714676\n",
      "Iteration 1126, loss = 0.12979420\n",
      "Iteration 1127, loss = 0.11759220\n",
      "Iteration 1128, loss = 0.11974217\n",
      "Iteration 1129, loss = 0.12910899\n",
      "Iteration 1130, loss = 0.13904832\n",
      "Iteration 1131, loss = 0.17128569\n",
      "Iteration 1132, loss = 0.11845728\n",
      "Iteration 1133, loss = 0.11480937\n",
      "Iteration 1134, loss = 0.11576494\n",
      "Iteration 1135, loss = 0.11905105\n",
      "Iteration 1136, loss = 0.12893275\n",
      "Iteration 1137, loss = 0.12250233\n",
      "Iteration 1138, loss = 0.12809291\n",
      "Iteration 1139, loss = 0.12643730\n",
      "Iteration 1140, loss = 0.12083945\n",
      "Iteration 1141, loss = 0.11902249\n",
      "Iteration 1142, loss = 0.11329766\n",
      "Iteration 1143, loss = 0.11999355\n",
      "Iteration 1144, loss = 0.13298902\n",
      "Iteration 1145, loss = 0.11246103\n",
      "Iteration 1146, loss = 0.11740124\n",
      "Iteration 1147, loss = 0.11512197\n",
      "Iteration 1148, loss = 0.12777473\n",
      "Iteration 1149, loss = 0.11579276\n",
      "Iteration 1150, loss = 0.11926577\n",
      "Iteration 1151, loss = 0.12039985\n",
      "Iteration 1152, loss = 0.13357249\n",
      "Iteration 1153, loss = 0.13637348\n",
      "Iteration 1154, loss = 0.12088854\n",
      "Iteration 1155, loss = 0.12718067\n",
      "Iteration 1156, loss = 0.11962249\n",
      "Iteration 1157, loss = 0.11755322\n",
      "Iteration 1158, loss = 0.13069755\n",
      "Iteration 1159, loss = 0.11929859\n",
      "Iteration 1160, loss = 0.11796305\n",
      "Iteration 1161, loss = 0.12221706\n",
      "Iteration 1162, loss = 0.11769481\n",
      "Iteration 1163, loss = 0.12701888\n",
      "Iteration 1164, loss = 0.12053496\n",
      "Iteration 1165, loss = 0.13139349\n",
      "Iteration 1166, loss = 0.11833995\n",
      "Iteration 1167, loss = 0.11909760\n",
      "Iteration 1168, loss = 0.14406199\n",
      "Iteration 1169, loss = 0.12053079\n",
      "Iteration 1170, loss = 0.16900580\n",
      "Iteration 1171, loss = 0.12934527\n",
      "Iteration 1172, loss = 0.18345338\n",
      "Iteration 1173, loss = 0.18206418\n",
      "Iteration 1174, loss = 0.14385854\n",
      "Iteration 1175, loss = 0.19503922\n",
      "Iteration 1176, loss = 0.16679336\n",
      "Iteration 1177, loss = 0.29655148\n",
      "Iteration 1178, loss = 0.16029860\n",
      "Iteration 1179, loss = 0.14151565\n",
      "Iteration 1180, loss = 0.16739004\n",
      "Iteration 1181, loss = 0.14205727\n",
      "Iteration 1182, loss = 0.14518946\n",
      "Iteration 1183, loss = 0.12213888\n",
      "Iteration 1184, loss = 0.11934453\n",
      "Iteration 1185, loss = 0.12079826\n",
      "Iteration 1186, loss = 0.13658754\n",
      "Iteration 1187, loss = 0.11263502\n",
      "Iteration 1188, loss = 0.12302201\n",
      "Iteration 1189, loss = 0.12032559\n",
      "Iteration 1190, loss = 0.12554434\n",
      "Iteration 1191, loss = 0.11683997\n",
      "Iteration 1192, loss = 0.11852079\n",
      "Iteration 1193, loss = 0.11486497\n",
      "Iteration 1194, loss = 0.11541365\n",
      "Iteration 1195, loss = 0.12379383\n",
      "Iteration 1196, loss = 0.11882479\n",
      "Iteration 1197, loss = 0.11450978\n",
      "Iteration 1198, loss = 0.12872320\n",
      "Iteration 1199, loss = 0.14352397\n",
      "Iteration 1200, loss = 0.14688662\n",
      "Iteration 1201, loss = 0.16978105\n",
      "Iteration 1202, loss = 0.15714962\n",
      "Iteration 1203, loss = 0.13365006\n",
      "Iteration 1204, loss = 0.15694032\n",
      "Iteration 1205, loss = 0.12784881\n",
      "Iteration 1206, loss = 0.13728594\n",
      "Iteration 1207, loss = 0.12592248\n",
      "Iteration 1208, loss = 0.15201752\n",
      "Iteration 1209, loss = 0.11704638\n",
      "Iteration 1210, loss = 0.13823462\n",
      "Iteration 1211, loss = 0.11858231\n",
      "Iteration 1212, loss = 0.12050470\n",
      "Iteration 1213, loss = 0.15469405\n",
      "Iteration 1214, loss = 0.13756802\n",
      "Iteration 1215, loss = 0.15015346\n",
      "Iteration 1216, loss = 0.12736244\n",
      "Iteration 1217, loss = 0.13101062\n",
      "Iteration 1218, loss = 0.11557475\n",
      "Iteration 1219, loss = 0.12388687\n",
      "Iteration 1220, loss = 0.11843530\n",
      "Iteration 1221, loss = 0.13068770\n",
      "Iteration 1222, loss = 0.13271336\n",
      "Iteration 1223, loss = 0.13776871\n",
      "Iteration 1224, loss = 0.14092937\n",
      "Iteration 1225, loss = 0.13577344\n",
      "Iteration 1226, loss = 0.13445021\n",
      "Iteration 1227, loss = 0.13758268\n",
      "Iteration 1228, loss = 0.12308040\n",
      "Iteration 1229, loss = 0.15708133\n",
      "Iteration 1230, loss = 0.13648086\n",
      "Iteration 1231, loss = 0.12693387\n",
      "Iteration 1232, loss = 0.18764115\n",
      "Iteration 1233, loss = 0.30549167\n",
      "Iteration 1234, loss = 0.28367754\n",
      "Iteration 1235, loss = 0.15795249\n",
      "Iteration 1236, loss = 0.15807922\n",
      "Iteration 1237, loss = 0.15833742\n",
      "Iteration 1238, loss = 0.11963804\n",
      "Iteration 1239, loss = 0.17359561\n",
      "Iteration 1240, loss = 0.21602512\n",
      "Iteration 1241, loss = 0.13553518\n",
      "Iteration 1242, loss = 0.35125634\n",
      "Iteration 1243, loss = 0.73260996\n",
      "Iteration 1244, loss = 0.69196799\n",
      "Iteration 1245, loss = 0.78775587\n",
      "Iteration 1246, loss = 0.45377291\n",
      "Iteration 1247, loss = 0.37568445\n",
      "Iteration 1248, loss = 0.21209822\n",
      "Iteration 1249, loss = 0.14025727\n",
      "Iteration 1250, loss = 0.27583619\n",
      "Iteration 1251, loss = 0.41944553\n",
      "Iteration 1252, loss = 0.47202013\n",
      "Iteration 1253, loss = 0.36267264\n",
      "Iteration 1254, loss = 0.13990360\n",
      "Iteration 1255, loss = 0.34200930\n",
      "Iteration 1256, loss = 0.42784822\n",
      "Iteration 1257, loss = 0.28757072\n",
      "Iteration 1258, loss = 0.24147118\n",
      "Iteration 1259, loss = 0.31493517\n",
      "Iteration 1260, loss = 0.41002960\n",
      "Iteration 1261, loss = 0.59131702\n",
      "Iteration 1262, loss = 0.35607573\n",
      "Iteration 1263, loss = 0.21307440\n",
      "Iteration 1264, loss = 0.22665189\n",
      "Iteration 1265, loss = 0.18218353\n",
      "Iteration 1266, loss = 0.17588680\n",
      "Iteration 1267, loss = 0.15143738\n",
      "Iteration 1268, loss = 0.13864470\n",
      "Iteration 1269, loss = 0.15095396\n",
      "Iteration 1270, loss = 0.15016500\n",
      "Iteration 1271, loss = 0.14904727\n",
      "Iteration 1272, loss = 0.16208517\n",
      "Iteration 1273, loss = 0.21832331\n",
      "Iteration 1274, loss = 0.14946236\n",
      "Iteration 1275, loss = 0.12814580\n",
      "Iteration 1276, loss = 0.17449406\n",
      "Iteration 1277, loss = 0.15277218\n",
      "Iteration 1278, loss = 0.14331737\n",
      "Iteration 1279, loss = 0.29377241\n",
      "Iteration 1280, loss = 0.31483453\n",
      "Iteration 1281, loss = 0.49138472\n",
      "Iteration 1282, loss = 0.79873672\n",
      "Iteration 1283, loss = 0.53145624\n",
      "Iteration 1284, loss = 0.47253111\n",
      "Iteration 1285, loss = 0.25751152\n",
      "Iteration 1286, loss = 0.23952101\n",
      "Iteration 1287, loss = 0.31124604\n",
      "Iteration 1288, loss = 0.38770830\n",
      "Iteration 1289, loss = 0.37284933\n",
      "Iteration 1290, loss = 0.24573520\n",
      "Iteration 1291, loss = 0.20842996\n",
      "Iteration 1292, loss = 0.22286079\n",
      "Iteration 1293, loss = 0.27257211\n",
      "Iteration 1294, loss = 0.17353758\n",
      "Iteration 1295, loss = 0.18859732\n",
      "Iteration 1296, loss = 0.32136355\n",
      "Iteration 1297, loss = 0.17889231\n",
      "Iteration 1298, loss = 0.15784785\n",
      "Iteration 1299, loss = 0.14079930\n",
      "Iteration 1300, loss = 0.16218591\n",
      "Iteration 1301, loss = 0.13449394\n",
      "Iteration 1302, loss = 0.13305808\n",
      "Iteration 1303, loss = 0.11811769\n",
      "Iteration 1304, loss = 0.11586543\n",
      "Iteration 1305, loss = 0.12841596\n",
      "Iteration 1306, loss = 0.11955035\n",
      "Iteration 1307, loss = 0.12281061\n",
      "Iteration 1308, loss = 0.11725169\n",
      "Iteration 1309, loss = 0.11903602\n",
      "Iteration 1310, loss = 0.12288500\n",
      "Iteration 1311, loss = 0.11583820\n",
      "Iteration 1312, loss = 0.11376630\n",
      "Iteration 1313, loss = 0.11573499\n",
      "Iteration 1314, loss = 0.11639908\n",
      "Iteration 1315, loss = 0.12670196\n",
      "Iteration 1316, loss = 0.11217723\n",
      "Iteration 1317, loss = 0.12197177\n",
      "Iteration 1318, loss = 0.11798236\n",
      "Iteration 1319, loss = 0.11984620\n",
      "Iteration 1320, loss = 0.11989041\n",
      "Iteration 1321, loss = 0.11678912\n",
      "Iteration 1322, loss = 0.11326805\n",
      "Iteration 1323, loss = 0.11240664\n",
      "Iteration 1324, loss = 0.11589209\n",
      "Iteration 1325, loss = 0.12527361\n",
      "Iteration 1326, loss = 0.13280496\n",
      "Iteration 1327, loss = 0.11235834\n",
      "Iteration 1328, loss = 0.11736440\n",
      "Iteration 1329, loss = 0.12558356\n",
      "Iteration 1330, loss = 0.11531380\n",
      "Iteration 1331, loss = 0.14002279\n",
      "Iteration 1332, loss = 0.13688314\n",
      "Iteration 1333, loss = 0.11753317\n",
      "Iteration 1334, loss = 0.12940858\n",
      "Iteration 1335, loss = 0.12063436\n",
      "Iteration 1336, loss = 0.11846428\n",
      "Iteration 1337, loss = 0.12682241\n",
      "Iteration 1338, loss = 0.12042140\n",
      "Iteration 1339, loss = 0.12537906\n",
      "Iteration 1340, loss = 0.12739507\n",
      "Iteration 1341, loss = 0.15405309\n",
      "Iteration 1342, loss = 0.13632274\n",
      "Iteration 1343, loss = 0.11846313\n",
      "Iteration 1344, loss = 0.14330157\n",
      "Iteration 1345, loss = 0.21600711\n",
      "Iteration 1346, loss = 0.20303812\n",
      "Iteration 1347, loss = 0.14492628\n",
      "Iteration 1348, loss = 0.16809964\n",
      "Iteration 1349, loss = 0.14643087\n",
      "Iteration 1350, loss = 0.13081410\n",
      "Iteration 1351, loss = 0.11758597\n",
      "Iteration 1352, loss = 0.12918880\n",
      "Iteration 1353, loss = 0.11912177\n",
      "Iteration 1354, loss = 0.12171816\n",
      "Iteration 1355, loss = 0.17598137\n",
      "Iteration 1356, loss = 0.22783424\n",
      "Iteration 1357, loss = 0.26565811\n",
      "Iteration 1358, loss = 0.16350043\n",
      "Iteration 1359, loss = 0.15032814\n",
      "Iteration 1360, loss = 0.14618069\n",
      "Iteration 1361, loss = 0.16868950\n",
      "Iteration 1362, loss = 0.11763139\n",
      "Iteration 1363, loss = 0.13673900\n",
      "Iteration 1364, loss = 0.15681027\n",
      "Iteration 1365, loss = 0.16627412\n",
      "Iteration 1366, loss = 0.13503598\n",
      "Iteration 1367, loss = 0.14948531\n",
      "Iteration 1368, loss = 0.17385573\n",
      "Iteration 1369, loss = 0.16644256\n",
      "Iteration 1370, loss = 0.19735175\n",
      "Iteration 1371, loss = 0.13927767\n",
      "Iteration 1372, loss = 0.16919530\n",
      "Iteration 1373, loss = 0.12811525\n",
      "Iteration 1374, loss = 0.12656740\n",
      "Iteration 1375, loss = 0.13972049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1376, loss = 0.15301417\n",
      "Iteration 1377, loss = 0.16843040\n",
      "Iteration 1378, loss = 0.17993581\n",
      "Iteration 1379, loss = 0.11408326\n",
      "Iteration 1380, loss = 0.15155798\n",
      "Iteration 1381, loss = 0.19414899\n",
      "Iteration 1382, loss = 0.17387637\n",
      "Iteration 1383, loss = 0.14106107\n",
      "Iteration 1384, loss = 0.16843005\n",
      "Iteration 1385, loss = 0.17775043\n",
      "Iteration 1386, loss = 0.14402506\n",
      "Iteration 1387, loss = 0.14758650\n",
      "Iteration 1388, loss = 0.15273636\n",
      "Iteration 1389, loss = 0.12118385\n",
      "Iteration 1390, loss = 0.12757758\n",
      "Iteration 1391, loss = 0.13055401\n",
      "Iteration 1392, loss = 0.12072335\n",
      "Iteration 1393, loss = 0.13103398\n",
      "Iteration 1394, loss = 0.12506706\n",
      "Iteration 1395, loss = 0.12451017\n",
      "Iteration 1396, loss = 0.12053140\n",
      "Iteration 1397, loss = 0.12282276\n",
      "Iteration 1398, loss = 0.13759357\n",
      "Iteration 1399, loss = 0.13655094\n",
      "Iteration 1400, loss = 0.12804521\n",
      "Iteration 1401, loss = 0.14476793\n",
      "Iteration 1402, loss = 0.13627716\n",
      "Iteration 1403, loss = 0.11915742\n",
      "Iteration 1404, loss = 0.13588004\n",
      "Iteration 1405, loss = 0.16781449\n",
      "Iteration 1406, loss = 0.19304882\n",
      "Iteration 1407, loss = 0.17542932\n",
      "Iteration 1408, loss = 0.22183149\n",
      "Iteration 1409, loss = 0.12632619\n",
      "Iteration 1410, loss = 0.13669842\n",
      "Iteration 1411, loss = 0.15857219\n",
      "Iteration 1412, loss = 0.24392623\n",
      "Iteration 1413, loss = 0.29183460\n",
      "Iteration 1414, loss = 0.31555717\n",
      "Iteration 1415, loss = 0.22477411\n",
      "Iteration 1416, loss = 0.21930394\n",
      "Iteration 1417, loss = 0.14776306\n",
      "Iteration 1418, loss = 0.14833264\n",
      "Iteration 1419, loss = 0.15488934\n",
      "Iteration 1420, loss = 0.12722594\n",
      "Iteration 1421, loss = 0.17122236\n",
      "Iteration 1422, loss = 0.12836938\n",
      "Iteration 1423, loss = 0.12443529\n",
      "Iteration 1424, loss = 0.17078750\n",
      "Iteration 1425, loss = 0.17058390\n",
      "Iteration 1426, loss = 0.14083245\n",
      "Iteration 1427, loss = 0.19698562\n",
      "Iteration 1428, loss = 0.13599647\n",
      "Iteration 1429, loss = 0.15045893\n",
      "Iteration 1430, loss = 0.13555527\n",
      "Iteration 1431, loss = 0.16069892\n",
      "Iteration 1432, loss = 0.12045980\n",
      "Iteration 1433, loss = 0.14787068\n",
      "Iteration 1434, loss = 0.15223675\n",
      "Iteration 1435, loss = 0.12030540\n",
      "Iteration 1436, loss = 0.13191364\n",
      "Iteration 1437, loss = 0.11909577\n",
      "Iteration 1438, loss = 0.12296246\n",
      "Iteration 1439, loss = 0.15527245\n",
      "Iteration 1440, loss = 0.16785701\n",
      "Iteration 1441, loss = 0.13543126\n",
      "Iteration 1442, loss = 0.12911112\n",
      "Iteration 1443, loss = 0.14905578\n",
      "Iteration 1444, loss = 0.31932049\n",
      "Iteration 1445, loss = 0.51283565\n",
      "Iteration 1446, loss = 1.09037989\n",
      "Iteration 1447, loss = 0.87400332\n",
      "Iteration 1448, loss = 1.00993771\n",
      "Iteration 1449, loss = 0.66390753\n",
      "Iteration 1450, loss = 0.76034452\n",
      "Iteration 1451, loss = 0.31525516\n",
      "Iteration 1452, loss = 0.44728412\n",
      "Iteration 1453, loss = 0.23309764\n",
      "Iteration 1454, loss = 0.15613802\n",
      "Iteration 1455, loss = 0.21733546\n",
      "Iteration 1456, loss = 0.15551876\n",
      "Iteration 1457, loss = 0.13507655\n",
      "Iteration 1458, loss = 0.13789470\n",
      "Iteration 1459, loss = 0.14148755\n",
      "Iteration 1460, loss = 0.14447088\n",
      "Iteration 1461, loss = 0.14289422\n",
      "Iteration 1462, loss = 0.12949718\n",
      "Iteration 1463, loss = 0.13765594\n",
      "Iteration 1464, loss = 0.13214434\n",
      "Iteration 1465, loss = 0.12587383\n",
      "Iteration 1466, loss = 0.13878241\n",
      "Iteration 1467, loss = 0.12235279\n",
      "Iteration 1468, loss = 0.16124123\n",
      "Iteration 1469, loss = 0.14203942\n",
      "Iteration 1470, loss = 0.13257878\n",
      "Iteration 1471, loss = 0.13343650\n",
      "Iteration 1472, loss = 0.19292933\n",
      "Iteration 1473, loss = 0.11716438\n",
      "Iteration 1474, loss = 0.13636281\n",
      "Iteration 1475, loss = 0.15113061\n",
      "Iteration 1476, loss = 0.13742289\n",
      "Iteration 1477, loss = 0.15168215\n",
      "Iteration 1478, loss = 0.16888393\n",
      "Iteration 1479, loss = 0.14127499\n",
      "Iteration 1480, loss = 0.12332456\n",
      "Iteration 1481, loss = 0.12133736\n",
      "Iteration 1482, loss = 0.15487509\n",
      "Iteration 1483, loss = 0.12882033\n",
      "Iteration 1484, loss = 0.11742728\n",
      "Iteration 1485, loss = 0.13109168\n",
      "Iteration 1486, loss = 0.12212772\n",
      "Iteration 1487, loss = 0.11378195\n",
      "Iteration 1488, loss = 0.11574999\n",
      "Iteration 1489, loss = 0.13141518\n",
      "Iteration 1490, loss = 0.12255689\n",
      "Iteration 1491, loss = 0.12767720\n",
      "Iteration 1492, loss = 0.12934320\n",
      "Iteration 1493, loss = 0.13321966\n",
      "Iteration 1494, loss = 0.18243470\n",
      "Iteration 1495, loss = 0.12789448\n",
      "Iteration 1496, loss = 0.12560181\n",
      "Iteration 1497, loss = 0.12784785\n",
      "Iteration 1498, loss = 0.14924530\n",
      "Iteration 1499, loss = 0.12982690\n",
      "Iteration 1500, loss = 0.19326989\n",
      "Iteration 1501, loss = 0.17856128\n",
      "Iteration 1502, loss = 0.24591633\n",
      "Iteration 1503, loss = 0.36940759\n",
      "Iteration 1504, loss = 0.32390967\n",
      "Iteration 1505, loss = 0.16829929\n",
      "Iteration 1506, loss = 0.35320855\n",
      "Iteration 1507, loss = 1.53139262\n",
      "Iteration 1508, loss = 0.74860653\n",
      "Iteration 1509, loss = 0.47167618\n",
      "Iteration 1510, loss = 0.93084074\n",
      "Iteration 1511, loss = 1.30942623\n",
      "Iteration 1512, loss = 3.05059524\n",
      "Iteration 1513, loss = 2.67199589\n",
      "Iteration 1514, loss = 3.91491883\n",
      "Iteration 1515, loss = 3.04549862\n",
      "Iteration 1516, loss = 3.60155188\n",
      "Iteration 1517, loss = 5.36046305\n",
      "Iteration 1518, loss = 1.32722925\n",
      "Iteration 1519, loss = 1.37614214\n",
      "Iteration 1520, loss = 1.94184616\n",
      "Iteration 1521, loss = 1.62721034\n",
      "Iteration 1522, loss = 1.26504586\n",
      "Iteration 1523, loss = 1.36126481\n",
      "Iteration 1524, loss = 1.18643666\n",
      "Iteration 1525, loss = 1.40076052\n",
      "Iteration 1526, loss = 1.48266160\n",
      "Iteration 1527, loss = 1.16086308\n",
      "Iteration 1528, loss = 1.17954464\n",
      "Iteration 1529, loss = 0.84796198\n",
      "Iteration 1530, loss = 0.90006119\n",
      "Iteration 1531, loss = 0.86897716\n",
      "Iteration 1532, loss = 1.00733893\n",
      "Iteration 1533, loss = 0.79056477\n",
      "Iteration 1534, loss = 0.73596921\n",
      "Iteration 1535, loss = 0.64368325\n",
      "Iteration 1536, loss = 0.72533893\n",
      "Iteration 1537, loss = 0.53278120\n",
      "Iteration 1538, loss = 0.43843119\n",
      "Iteration 1539, loss = 0.49192010\n",
      "Iteration 1540, loss = 0.68555683\n",
      "Iteration 1541, loss = 0.82445438\n",
      "Iteration 1542, loss = 0.76122530\n",
      "Iteration 1543, loss = 0.74050263\n",
      "Iteration 1544, loss = 0.75798639\n",
      "Iteration 1545, loss = 1.12765750\n",
      "Iteration 1546, loss = 0.81776133\n",
      "Iteration 1547, loss = 0.70406962\n",
      "Iteration 1548, loss = 0.61843474\n",
      "Iteration 1549, loss = 0.52421161\n",
      "Iteration 1550, loss = 0.57782073\n",
      "Iteration 1551, loss = 0.81326386\n",
      "Iteration 1552, loss = 0.50349271\n",
      "Iteration 1553, loss = 1.01433213\n",
      "Iteration 1554, loss = 0.61231907\n",
      "Iteration 1555, loss = 0.55654792\n",
      "Iteration 1556, loss = 0.39972135\n",
      "Iteration 1557, loss = 0.39530082\n",
      "Iteration 1558, loss = 0.40396942\n",
      "Iteration 1559, loss = 0.40416390\n",
      "Iteration 1560, loss = 0.30586235\n",
      "Iteration 1561, loss = 0.43140096\n",
      "Iteration 1562, loss = 0.38874330\n",
      "Iteration 1563, loss = 0.24584629\n",
      "Iteration 1564, loss = 0.20241770\n",
      "Iteration 1565, loss = 0.26016197\n",
      "Iteration 1566, loss = 0.30378502\n",
      "Iteration 1567, loss = 0.27313144\n",
      "Iteration 1568, loss = 0.31415255\n",
      "Iteration 1569, loss = 0.47027548\n",
      "Iteration 1570, loss = 0.90241457\n",
      "Iteration 1571, loss = 0.43931017\n",
      "Iteration 1572, loss = 0.34787310\n",
      "Iteration 1573, loss = 0.34023727\n",
      "Iteration 1574, loss = 0.30581588\n",
      "Iteration 1575, loss = 0.34563812\n",
      "Iteration 1576, loss = 0.38715327\n",
      "Iteration 1577, loss = 0.26195419\n",
      "Iteration 1578, loss = 0.33816482\n",
      "Iteration 1579, loss = 0.31073079\n",
      "Iteration 1580, loss = 0.26747132\n",
      "Iteration 1581, loss = 0.29706001\n",
      "Iteration 1582, loss = 0.21761348\n",
      "Iteration 1583, loss = 0.21162990\n",
      "Iteration 1584, loss = 0.20322033\n",
      "Iteration 1585, loss = 0.25768977\n",
      "Iteration 1586, loss = 0.18539874\n",
      "Iteration 1587, loss = 0.18512450\n",
      "Iteration 1588, loss = 0.19270880\n",
      "Iteration 1589, loss = 0.17465449\n",
      "Iteration 1590, loss = 0.24012338\n",
      "Iteration 1591, loss = 0.35503823\n",
      "Iteration 1592, loss = 0.23199934\n",
      "Iteration 1593, loss = 0.15529160\n",
      "Iteration 1594, loss = 0.17341216\n",
      "Iteration 1595, loss = 0.14928442\n",
      "Iteration 1596, loss = 0.14865099\n",
      "Iteration 1597, loss = 0.15330221\n",
      "Iteration 1598, loss = 0.19001269\n",
      "Iteration 1599, loss = 0.20872615\n",
      "Iteration 1600, loss = 0.28748824\n",
      "Iteration 1601, loss = 0.19477882\n",
      "Iteration 1602, loss = 0.20621778\n",
      "Iteration 1603, loss = 0.17630392\n",
      "Iteration 1604, loss = 0.18906340\n",
      "Iteration 1605, loss = 0.20178328\n",
      "Iteration 1606, loss = 0.17966217\n",
      "Iteration 1607, loss = 0.13988451\n",
      "Iteration 1608, loss = 0.20212451\n",
      "Iteration 1609, loss = 0.21277357\n",
      "Iteration 1610, loss = 0.17084139\n",
      "Iteration 1611, loss = 0.21881208\n",
      "Iteration 1612, loss = 0.18054886\n",
      "Iteration 1613, loss = 0.14039498\n",
      "Iteration 1614, loss = 0.16132580\n",
      "Iteration 1615, loss = 0.18223891\n",
      "Iteration 1616, loss = 0.28007155\n",
      "Iteration 1617, loss = 0.17581769\n",
      "Iteration 1618, loss = 0.19289817\n",
      "Iteration 1619, loss = 0.18673147\n",
      "Iteration 1620, loss = 0.31699899\n",
      "Iteration 1621, loss = 0.23374582\n",
      "Iteration 1622, loss = 0.16226255\n",
      "Iteration 1623, loss = 0.15834565\n",
      "Iteration 1624, loss = 0.23650133\n",
      "Iteration 1625, loss = 0.22367175\n",
      "Iteration 1626, loss = 0.17041759\n",
      "Iteration 1627, loss = 0.14067452\n",
      "Iteration 1628, loss = 0.15993329\n",
      "Iteration 1629, loss = 0.14154714\n",
      "Iteration 1630, loss = 0.12679403\n",
      "Iteration 1631, loss = 0.11971698\n",
      "Iteration 1632, loss = 0.16592235\n",
      "Iteration 1633, loss = 0.16529783\n",
      "Iteration 1634, loss = 0.15533001\n",
      "Iteration 1635, loss = 0.12938066\n",
      "Iteration 1636, loss = 0.14444551\n",
      "Iteration 1637, loss = 0.13201798\n",
      "Iteration 1638, loss = 0.14458522\n",
      "Iteration 1639, loss = 0.12362907\n",
      "Iteration 1640, loss = 0.12094125\n",
      "Iteration 1641, loss = 0.16503847\n",
      "Iteration 1642, loss = 0.26227696\n",
      "Iteration 1643, loss = 0.34325186\n",
      "Iteration 1644, loss = 0.24132747\n",
      "Iteration 1645, loss = 0.41864583\n",
      "Iteration 1646, loss = 0.64371470\n",
      "Iteration 1647, loss = 0.62323796\n",
      "Iteration 1648, loss = 0.49040526\n",
      "Iteration 1649, loss = 0.36927503\n",
      "Iteration 1650, loss = 0.44178830\n",
      "Iteration 1651, loss = 0.53181209\n",
      "Iteration 1652, loss = 0.46033568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1653, loss = 0.39323105\n",
      "Iteration 1654, loss = 0.22741196\n",
      "Iteration 1655, loss = 0.19065759\n",
      "Iteration 1656, loss = 0.21597913\n",
      "Iteration 1657, loss = 0.21885028\n",
      "Iteration 1658, loss = 0.16692688\n",
      "Iteration 1659, loss = 0.22278503\n",
      "Iteration 1660, loss = 0.20797061\n",
      "Iteration 1661, loss = 0.15844370\n",
      "Iteration 1662, loss = 0.23726154\n",
      "Iteration 1663, loss = 0.16778848\n",
      "Iteration 1664, loss = 0.13071526\n",
      "Iteration 1665, loss = 0.14263247\n",
      "Iteration 1666, loss = 0.16059219\n",
      "Iteration 1667, loss = 0.16127124\n",
      "Iteration 1668, loss = 0.12324547\n",
      "Iteration 1669, loss = 0.15108130\n",
      "Iteration 1670, loss = 0.13459223\n",
      "Iteration 1671, loss = 0.19487147\n",
      "Iteration 1672, loss = 0.13533205\n",
      "Iteration 1673, loss = 0.15314312\n",
      "Iteration 1674, loss = 0.19336491\n",
      "Iteration 1675, loss = 0.25133755\n",
      "Iteration 1676, loss = 0.39442931\n",
      "Iteration 1677, loss = 0.25376097\n",
      "Iteration 1678, loss = 0.17904257\n",
      "Iteration 1679, loss = 0.29054827\n",
      "Iteration 1680, loss = 0.54032214\n",
      "Iteration 1681, loss = 0.55224557\n",
      "Iteration 1682, loss = 0.45642901\n",
      "Iteration 1683, loss = 0.19711130\n",
      "Iteration 1684, loss = 0.17217444\n",
      "Iteration 1685, loss = 0.24454658\n",
      "Iteration 1686, loss = 0.24071003\n",
      "Iteration 1687, loss = 0.20571188\n",
      "Iteration 1688, loss = 0.18154156\n",
      "Iteration 1689, loss = 0.19646007\n",
      "Iteration 1690, loss = 0.17115048\n",
      "Iteration 1691, loss = 0.15883026\n",
      "Iteration 1692, loss = 0.14931529\n",
      "Iteration 1693, loss = 0.18813348\n",
      "Iteration 1694, loss = 0.14233462\n",
      "Iteration 1695, loss = 0.15400775\n",
      "Iteration 1696, loss = 0.14811443\n",
      "Iteration 1697, loss = 0.13322319\n",
      "Iteration 1698, loss = 0.14875959\n",
      "Iteration 1699, loss = 0.16581298\n",
      "Iteration 1700, loss = 0.19467261\n",
      "Iteration 1701, loss = 0.17899918\n",
      "Iteration 1702, loss = 0.15733542\n",
      "Iteration 1703, loss = 0.13089930\n",
      "Iteration 1704, loss = 0.15116480\n",
      "Iteration 1705, loss = 0.13062506\n",
      "Iteration 1706, loss = 0.12033151\n",
      "Iteration 1707, loss = 0.13602651\n",
      "Iteration 1708, loss = 0.13477055\n",
      "Iteration 1709, loss = 0.11821581\n",
      "Iteration 1710, loss = 0.11430715\n",
      "Iteration 1711, loss = 0.11473647\n",
      "Iteration 1712, loss = 0.12161720\n",
      "Iteration 1713, loss = 0.12518061\n",
      "Iteration 1714, loss = 0.11446287\n",
      "Iteration 1715, loss = 0.12123738\n",
      "Iteration 1716, loss = 0.14057607\n",
      "Iteration 1717, loss = 0.12133874\n",
      "Iteration 1718, loss = 0.14513782\n",
      "Iteration 1719, loss = 0.13460582\n",
      "Iteration 1720, loss = 0.11994500\n",
      "Iteration 1721, loss = 0.14960841\n",
      "Iteration 1722, loss = 0.13282473\n",
      "Iteration 1723, loss = 0.11968293\n",
      "Iteration 1724, loss = 0.12806544\n",
      "Iteration 1725, loss = 0.12268280\n",
      "Iteration 1726, loss = 0.12137234\n",
      "Iteration 1727, loss = 0.12956155\n",
      "Iteration 1728, loss = 0.11810765\n",
      "Iteration 1729, loss = 0.11338640\n",
      "Iteration 1730, loss = 0.11911950\n",
      "Iteration 1731, loss = 0.12337003\n",
      "Iteration 1732, loss = 0.11633315\n",
      "Iteration 1733, loss = 0.11814002\n",
      "Iteration 1734, loss = 0.12473988\n",
      "Iteration 1735, loss = 0.15045054\n",
      "Iteration 1736, loss = 0.11757786\n",
      "Iteration 1737, loss = 0.14647969\n",
      "Iteration 1738, loss = 0.12382613\n",
      "Iteration 1739, loss = 0.12888168\n",
      "Iteration 1740, loss = 0.15626287\n",
      "Iteration 1741, loss = 0.12856449\n",
      "Iteration 1742, loss = 0.13593636\n",
      "Iteration 1743, loss = 0.13847957\n",
      "Iteration 1744, loss = 0.20553869\n",
      "Iteration 1745, loss = 0.15443713\n",
      "Iteration 1746, loss = 0.24198876\n",
      "Iteration 1747, loss = 0.24951275\n",
      "Iteration 1748, loss = 0.14771230\n",
      "Iteration 1749, loss = 0.23856004\n",
      "Iteration 1750, loss = 0.16568491\n",
      "Iteration 1751, loss = 0.38923278\n",
      "Iteration 1752, loss = 0.50213212\n",
      "Iteration 1753, loss = 0.52749049\n",
      "Iteration 1754, loss = 0.38819582\n",
      "Iteration 1755, loss = 0.22898697\n",
      "Iteration 1756, loss = 0.13373791\n",
      "Iteration 1757, loss = 0.13047487\n",
      "Iteration 1758, loss = 0.15725478\n",
      "Iteration 1759, loss = 0.14122715\n",
      "Iteration 1760, loss = 0.11917398\n",
      "Iteration 1761, loss = 0.14027139\n",
      "Iteration 1762, loss = 0.12254795\n",
      "Iteration 1763, loss = 0.13622288\n",
      "Iteration 1764, loss = 0.12154947\n",
      "Iteration 1765, loss = 0.12868800\n",
      "Iteration 1766, loss = 0.12307590\n",
      "Iteration 1767, loss = 0.14453675\n",
      "Iteration 1768, loss = 0.14320232\n",
      "Iteration 1769, loss = 0.16667453\n",
      "Iteration 1770, loss = 0.13964732\n",
      "Iteration 1771, loss = 0.14486024\n",
      "Iteration 1772, loss = 0.13083083\n",
      "Iteration 1773, loss = 0.12722711\n",
      "Iteration 1774, loss = 0.12424022\n",
      "Iteration 1775, loss = 0.12256507\n",
      "Iteration 1776, loss = 0.11981953\n",
      "Iteration 1777, loss = 0.15976022\n",
      "Iteration 1778, loss = 0.15382300\n",
      "Iteration 1779, loss = 0.12479064\n",
      "Iteration 1780, loss = 0.14487651\n",
      "Iteration 1781, loss = 0.14118091\n",
      "Iteration 1782, loss = 0.20143122\n",
      "Iteration 1783, loss = 0.16640051\n",
      "Iteration 1784, loss = 0.13144665\n",
      "Iteration 1785, loss = 0.18043791\n",
      "Iteration 1786, loss = 0.15226683\n",
      "Iteration 1787, loss = 0.12077684\n",
      "Iteration 1788, loss = 0.13210808\n",
      "Iteration 1789, loss = 0.12551152\n",
      "Iteration 1790, loss = 0.13284395\n",
      "Iteration 1791, loss = 0.11688794\n",
      "Iteration 1792, loss = 0.12394672\n",
      "Iteration 1793, loss = 0.11867822\n",
      "Iteration 1794, loss = 0.11492567\n",
      "Iteration 1795, loss = 0.11378604\n",
      "Iteration 1796, loss = 0.12015023\n",
      "Iteration 1797, loss = 0.11840571\n",
      "Iteration 1798, loss = 0.12156818\n",
      "Iteration 1799, loss = 0.11555867\n",
      "Iteration 1800, loss = 0.11924208\n",
      "Iteration 1801, loss = 0.12427555\n",
      "Iteration 1802, loss = 0.13886500\n",
      "Iteration 1803, loss = 0.13184003\n",
      "Iteration 1804, loss = 0.13102372\n",
      "Iteration 1805, loss = 0.11402222\n",
      "Iteration 1806, loss = 0.20034582\n",
      "Iteration 1807, loss = 0.22135611\n",
      "Iteration 1808, loss = 0.15161251\n",
      "Iteration 1809, loss = 0.12290297\n",
      "Iteration 1810, loss = 0.12554062\n",
      "Iteration 1811, loss = 0.11360412\n",
      "Iteration 1812, loss = 0.12009289\n",
      "Iteration 1813, loss = 0.16996549\n",
      "Iteration 1814, loss = 0.13472574\n",
      "Iteration 1815, loss = 0.17292977\n",
      "Iteration 1816, loss = 0.17546882\n",
      "Iteration 1817, loss = 0.19936384\n",
      "Iteration 1818, loss = 0.21109905\n",
      "Iteration 1819, loss = 0.17723513\n",
      "Iteration 1820, loss = 0.13614543\n",
      "Iteration 1821, loss = 0.15206116\n",
      "Iteration 1822, loss = 0.13284067\n",
      "Iteration 1823, loss = 0.11646352\n",
      "Iteration 1824, loss = 0.11898072\n",
      "Iteration 1825, loss = 0.11914780\n",
      "Iteration 1826, loss = 0.15390247\n",
      "Iteration 1827, loss = 0.15399731\n",
      "Iteration 1828, loss = 0.19447252\n",
      "Iteration 1829, loss = 0.16810498\n",
      "Iteration 1830, loss = 0.12641920\n",
      "Iteration 1831, loss = 0.14174842\n",
      "Iteration 1832, loss = 0.15901907\n",
      "Iteration 1833, loss = 0.16333737\n",
      "Iteration 1834, loss = 0.13096376\n",
      "Iteration 1835, loss = 0.15148275\n",
      "Iteration 1836, loss = 0.12889242\n",
      "Iteration 1837, loss = 0.12152815\n",
      "Iteration 1838, loss = 0.11973171\n",
      "Iteration 1839, loss = 0.12771345\n",
      "Iteration 1840, loss = 0.13368333\n",
      "Iteration 1841, loss = 0.11684250\n",
      "Iteration 1842, loss = 0.11561041\n",
      "Iteration 1843, loss = 0.11584109\n",
      "Iteration 1844, loss = 0.11669281\n",
      "Iteration 1845, loss = 0.11720250\n",
      "Iteration 1846, loss = 0.11932824\n",
      "Iteration 1847, loss = 0.12259898\n",
      "Iteration 1848, loss = 0.11597548\n",
      "Iteration 1849, loss = 0.11273983\n",
      "Iteration 1850, loss = 0.11464410\n",
      "Iteration 1851, loss = 0.13032545\n",
      "Iteration 1852, loss = 0.11859115\n",
      "Iteration 1853, loss = 0.11307050\n",
      "Iteration 1854, loss = 0.12772331\n",
      "Iteration 1855, loss = 0.11923432\n",
      "Iteration 1856, loss = 0.11986387\n",
      "Iteration 1857, loss = 0.11804775\n",
      "Iteration 1858, loss = 0.11962789\n",
      "Iteration 1859, loss = 0.11484861\n",
      "Iteration 1860, loss = 0.11226840\n",
      "Iteration 1861, loss = 0.12165498\n",
      "Iteration 1862, loss = 0.11527676\n",
      "Iteration 1863, loss = 0.12015780\n",
      "Iteration 1864, loss = 0.12634119\n",
      "Iteration 1865, loss = 0.12727059\n",
      "Iteration 1866, loss = 0.11468156\n",
      "Iteration 1867, loss = 0.11779932\n",
      "Iteration 1868, loss = 0.11681996\n",
      "Iteration 1869, loss = 0.12509397\n",
      "Iteration 1870, loss = 0.12735183\n",
      "Iteration 1871, loss = 0.14607286\n",
      "Iteration 1872, loss = 0.12631046\n",
      "Iteration 1873, loss = 0.13543714\n",
      "Iteration 1874, loss = 0.12830298\n",
      "Iteration 1875, loss = 0.11991126\n",
      "Iteration 1876, loss = 0.13399795\n",
      "Iteration 1877, loss = 0.12599950\n",
      "Iteration 1878, loss = 0.12382704\n",
      "Iteration 1879, loss = 0.12845715\n",
      "Iteration 1880, loss = 0.12056068\n",
      "Iteration 1881, loss = 0.11499108\n",
      "Iteration 1882, loss = 0.14528371\n",
      "Iteration 1883, loss = 0.15132036\n",
      "Iteration 1884, loss = 0.13555709\n",
      "Iteration 1885, loss = 0.17383753\n",
      "Iteration 1886, loss = 0.14866454\n",
      "Iteration 1887, loss = 0.12902619\n",
      "Iteration 1888, loss = 0.12709126\n",
      "Iteration 1889, loss = 0.11547512\n",
      "Iteration 1890, loss = 0.12226567\n",
      "Iteration 1891, loss = 0.13306982\n",
      "Iteration 1892, loss = 0.13413012\n",
      "Iteration 1893, loss = 0.14606682\n",
      "Iteration 1894, loss = 0.14578105\n",
      "Iteration 1895, loss = 0.11801560\n",
      "Iteration 1896, loss = 0.16061900\n",
      "Iteration 1897, loss = 0.12904879\n",
      "Iteration 1898, loss = 0.12342163\n",
      "Iteration 1899, loss = 0.13282019\n",
      "Iteration 1900, loss = 0.27795974\n",
      "Iteration 1901, loss = 0.24428013\n",
      "Iteration 1902, loss = 0.17208617\n",
      "Iteration 1903, loss = 0.28181846\n",
      "Iteration 1904, loss = 0.19390720\n",
      "Iteration 1905, loss = 0.18465535\n",
      "Iteration 1906, loss = 0.23517864\n",
      "Iteration 1907, loss = 0.13820393\n",
      "Iteration 1908, loss = 0.12045381\n",
      "Iteration 1909, loss = 0.13790329\n",
      "Iteration 1910, loss = 0.12438461\n",
      "Iteration 1911, loss = 0.14872764\n",
      "Iteration 1912, loss = 0.18473666\n",
      "Iteration 1913, loss = 0.16388379\n",
      "Iteration 1914, loss = 0.12760238\n",
      "Iteration 1915, loss = 0.13481157\n",
      "Iteration 1916, loss = 0.11282517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1917, loss = 0.12913531\n",
      "Iteration 1918, loss = 0.15196945\n",
      "Iteration 1919, loss = 0.18860780\n",
      "Iteration 1920, loss = 0.15466537\n",
      "Iteration 1921, loss = 0.12078864\n",
      "Iteration 1922, loss = 0.16233133\n",
      "Iteration 1923, loss = 0.13845978\n",
      "Iteration 1924, loss = 0.12249728\n",
      "Iteration 1925, loss = 0.12610677\n",
      "Iteration 1926, loss = 0.11369396\n",
      "Iteration 1927, loss = 0.14091049\n",
      "Iteration 1928, loss = 0.13373103\n",
      "Iteration 1929, loss = 0.17475178\n",
      "Iteration 1930, loss = 0.16848854\n",
      "Iteration 1931, loss = 0.22398657\n",
      "Iteration 1932, loss = 0.24260093\n",
      "Iteration 1933, loss = 0.20735892\n",
      "Iteration 1934, loss = 0.15808301\n",
      "Iteration 1935, loss = 0.17854694\n",
      "Iteration 1936, loss = 0.12803476\n",
      "Iteration 1937, loss = 0.11497633\n",
      "Iteration 1938, loss = 0.12672888\n",
      "Iteration 1939, loss = 0.12124470\n",
      "Iteration 1940, loss = 0.11729537\n",
      "Iteration 1941, loss = 0.13137670\n",
      "Iteration 1942, loss = 0.14486806\n",
      "Iteration 1943, loss = 0.11107068\n",
      "Iteration 1944, loss = 0.12937569\n",
      "Iteration 1945, loss = 0.17603719\n",
      "Iteration 1946, loss = 0.22771853\n",
      "Iteration 1947, loss = 0.16451151\n",
      "Iteration 1948, loss = 0.15665054\n",
      "Iteration 1949, loss = 0.13834150\n",
      "Iteration 1950, loss = 0.12693909\n",
      "Iteration 1951, loss = 0.12919463\n",
      "Iteration 1952, loss = 0.13454030\n",
      "Iteration 1953, loss = 0.12903550\n",
      "Iteration 1954, loss = 0.12050616\n",
      "Iteration 1955, loss = 0.12841307\n",
      "Iteration 1956, loss = 0.15643723\n",
      "Iteration 1957, loss = 0.11966425\n",
      "Iteration 1958, loss = 0.14013789\n",
      "Iteration 1959, loss = 0.14281789\n",
      "Iteration 1960, loss = 0.11611276\n",
      "Iteration 1961, loss = 0.11988775\n",
      "Iteration 1962, loss = 0.12734238\n",
      "Iteration 1963, loss = 0.14358690\n",
      "Iteration 1964, loss = 0.13861711\n",
      "Iteration 1965, loss = 0.11832887\n",
      "Iteration 1966, loss = 0.13901001\n",
      "Iteration 1967, loss = 0.15141712\n",
      "Iteration 1968, loss = 0.12947132\n",
      "Iteration 1969, loss = 0.19612631\n",
      "Iteration 1970, loss = 0.23555517\n",
      "Iteration 1971, loss = 0.41780704\n",
      "Iteration 1972, loss = 0.54849047\n",
      "Iteration 1973, loss = 0.39584868\n",
      "Iteration 1974, loss = 0.27379091\n",
      "Iteration 1975, loss = 0.35530496\n",
      "Iteration 1976, loss = 0.59947991\n",
      "Iteration 1977, loss = 0.56651174\n",
      "Iteration 1978, loss = 0.69086563\n",
      "Iteration 1979, loss = 0.68366682\n",
      "Iteration 1980, loss = 0.48354169\n",
      "Iteration 1981, loss = 0.31629374\n",
      "Iteration 1982, loss = 0.63168181\n",
      "Iteration 1983, loss = 0.52521176\n",
      "Iteration 1984, loss = 0.45691003\n",
      "Iteration 1985, loss = 0.31444263\n",
      "Iteration 1986, loss = 0.18783070\n",
      "Iteration 1987, loss = 0.21949872\n",
      "Iteration 1988, loss = 0.19506708\n",
      "Iteration 1989, loss = 0.14534714\n",
      "Iteration 1990, loss = 0.17628238\n",
      "Iteration 1991, loss = 0.19169757\n",
      "Iteration 1992, loss = 0.15653592\n",
      "Iteration 1993, loss = 0.16330202\n",
      "Iteration 1994, loss = 0.14574531\n",
      "Iteration 1995, loss = 0.11521697\n",
      "Iteration 1996, loss = 0.15116216\n",
      "Iteration 1997, loss = 0.12702959\n",
      "Iteration 1998, loss = 0.13054180\n",
      "Iteration 1999, loss = 0.13192186\n",
      "Iteration 2000, loss = 0.14553106\n",
      "Iteration 2001, loss = 0.12696780\n",
      "Iteration 2002, loss = 0.13862668\n",
      "Iteration 2003, loss = 0.12058434\n",
      "Iteration 2004, loss = 0.12680249\n",
      "Iteration 2005, loss = 0.12724443\n",
      "Iteration 2006, loss = 0.12264852\n",
      "Iteration 2007, loss = 0.14572479\n",
      "Iteration 2008, loss = 0.12738117\n",
      "Iteration 2009, loss = 0.11304457\n",
      "Iteration 2010, loss = 0.12276374\n",
      "Iteration 2011, loss = 0.11685316\n",
      "Iteration 2012, loss = 0.11926115\n",
      "Iteration 2013, loss = 0.12366090\n",
      "Iteration 2014, loss = 0.12065615\n",
      "Iteration 2015, loss = 0.12154815\n",
      "Iteration 2016, loss = 0.11490880\n",
      "Iteration 2017, loss = 0.11466752\n",
      "Iteration 2018, loss = 0.12954874\n",
      "Iteration 2019, loss = 0.13682876\n",
      "Iteration 2020, loss = 0.13242754\n",
      "Iteration 2021, loss = 0.12211378\n",
      "Iteration 2022, loss = 0.15235730\n",
      "Iteration 2023, loss = 0.23034668\n",
      "Iteration 2024, loss = 0.25347316\n",
      "Iteration 2025, loss = 0.18289623\n",
      "Iteration 2026, loss = 0.11728636\n",
      "Iteration 2027, loss = 0.14609135\n",
      "Iteration 2028, loss = 0.14752915\n",
      "Iteration 2029, loss = 0.12517438\n",
      "Iteration 2030, loss = 0.12403449\n",
      "Iteration 2031, loss = 0.14245665\n",
      "Iteration 2032, loss = 0.13115753\n",
      "Iteration 2033, loss = 0.16880100\n",
      "Iteration 2034, loss = 0.15387085\n",
      "Iteration 2035, loss = 0.25232886\n",
      "Iteration 2036, loss = 0.38790315\n",
      "Iteration 2037, loss = 0.34506135\n",
      "Iteration 2038, loss = 0.17563999\n",
      "Iteration 2039, loss = 0.16224110\n",
      "Iteration 2040, loss = 0.16405003\n",
      "Iteration 2041, loss = 0.17254084\n",
      "Iteration 2042, loss = 0.17167572\n",
      "Iteration 2043, loss = 0.12361632\n",
      "Iteration 2044, loss = 0.13623813\n",
      "Iteration 2045, loss = 0.20332969\n",
      "Iteration 2046, loss = 0.17003655\n",
      "Iteration 2047, loss = 0.18509810\n",
      "Iteration 2048, loss = 0.24525371\n",
      "Iteration 2049, loss = 0.24492152\n",
      "Iteration 2050, loss = 0.16646993\n",
      "Iteration 2051, loss = 0.13040795\n",
      "Iteration 2052, loss = 0.17927011\n",
      "Iteration 2053, loss = 0.17049736\n",
      "Iteration 2054, loss = 0.14366263\n",
      "Iteration 2055, loss = 0.13724026\n",
      "Iteration 2056, loss = 0.54359430\n",
      "Iteration 2057, loss = 0.54728414\n",
      "Iteration 2058, loss = 0.36109373\n",
      "Iteration 2059, loss = 0.24959596\n",
      "Iteration 2060, loss = 0.15783232\n",
      "Iteration 2061, loss = 0.24248854\n",
      "Iteration 2062, loss = 0.19351140\n",
      "Iteration 2063, loss = 0.17146754\n",
      "Iteration 2064, loss = 0.22233353\n",
      "Iteration 2065, loss = 0.29892456\n",
      "Iteration 2066, loss = 0.19451302\n",
      "Iteration 2067, loss = 0.21624154\n",
      "Iteration 2068, loss = 0.14739572\n",
      "Iteration 2069, loss = 0.12626607\n",
      "Iteration 2070, loss = 0.15443533\n",
      "Iteration 2071, loss = 0.24603210\n",
      "Iteration 2072, loss = 0.13641890\n",
      "Iteration 2073, loss = 0.13412347\n",
      "Iteration 2074, loss = 0.19056470\n",
      "Iteration 2075, loss = 0.13998888\n",
      "Iteration 2076, loss = 0.12915621\n",
      "Iteration 2077, loss = 0.12867359\n",
      "Iteration 2078, loss = 0.13354466\n",
      "Iteration 2079, loss = 0.14916383\n",
      "Iteration 2080, loss = 0.13451006\n",
      "Iteration 2081, loss = 0.11660685\n",
      "Iteration 2082, loss = 0.13610798\n",
      "Iteration 2083, loss = 0.13198311\n",
      "Iteration 2084, loss = 0.17701373\n",
      "Iteration 2085, loss = 0.14898602\n",
      "Iteration 2086, loss = 0.13070566\n",
      "Iteration 2087, loss = 0.15286106\n",
      "Iteration 2088, loss = 0.19239786\n",
      "Iteration 2089, loss = 0.22506249\n",
      "Iteration 2090, loss = 0.16783568\n",
      "Iteration 2091, loss = 0.14948377\n",
      "Iteration 2092, loss = 0.13172325\n",
      "Iteration 2093, loss = 0.54528871\n",
      "Iteration 2094, loss = 1.26571024\n",
      "Iteration 2095, loss = 0.74685583\n",
      "Iteration 2096, loss = 0.67087384\n",
      "Iteration 2097, loss = 1.27757811\n",
      "Iteration 2098, loss = 0.74831334\n",
      "Iteration 2099, loss = 0.64151540\n",
      "Iteration 2100, loss = 0.67709131\n",
      "Iteration 2101, loss = 0.40962862\n",
      "Iteration 2102, loss = 0.20640181\n",
      "Iteration 2103, loss = 0.26067121\n",
      "Iteration 2104, loss = 0.31019736\n",
      "Iteration 2105, loss = 0.32762818\n",
      "Iteration 2106, loss = 0.24541177\n",
      "Iteration 2107, loss = 0.26483355\n",
      "Iteration 2108, loss = 0.24307302\n",
      "Iteration 2109, loss = 0.15031873\n",
      "Iteration 2110, loss = 0.13743214\n",
      "Iteration 2111, loss = 0.17245532\n",
      "Iteration 2112, loss = 0.16730433\n",
      "Iteration 2113, loss = 0.24402009\n",
      "Iteration 2114, loss = 0.17064924\n",
      "Iteration 2115, loss = 0.22360810\n",
      "Iteration 2116, loss = 0.21251310\n",
      "Iteration 2117, loss = 0.18642889\n",
      "Iteration 2118, loss = 0.17148088\n",
      "Iteration 2119, loss = 0.16335357\n",
      "Iteration 2120, loss = 0.14162787\n",
      "Iteration 2121, loss = 0.14086187\n",
      "Iteration 2122, loss = 0.12674202\n",
      "Iteration 2123, loss = 0.12211308\n",
      "Iteration 2124, loss = 0.13442264\n",
      "Iteration 2125, loss = 0.12763531\n",
      "Iteration 2126, loss = 0.12247433\n",
      "Iteration 2127, loss = 0.12974304\n",
      "Iteration 2128, loss = 0.12187652\n",
      "Iteration 2129, loss = 0.11215991\n",
      "Iteration 2130, loss = 0.12525335\n",
      "Iteration 2131, loss = 0.17067245\n",
      "Iteration 2132, loss = 0.29376336\n",
      "Iteration 2133, loss = 0.29935359\n",
      "Iteration 2134, loss = 0.23964310\n",
      "Iteration 2135, loss = 0.12888214\n",
      "Iteration 2136, loss = 0.12801806\n",
      "Iteration 2137, loss = 0.15359492\n",
      "Iteration 2138, loss = 0.12889453\n",
      "Iteration 2139, loss = 0.13979281\n",
      "Iteration 2140, loss = 0.15234177\n",
      "Iteration 2141, loss = 0.17881905\n",
      "Iteration 2142, loss = 0.20291953\n",
      "Iteration 2143, loss = 0.15836118\n",
      "Iteration 2144, loss = 0.52125258\n",
      "Iteration 2145, loss = 0.58550634\n",
      "Iteration 2146, loss = 0.55635856\n",
      "Iteration 2147, loss = 0.92361682\n",
      "Iteration 2148, loss = 0.56807297\n",
      "Iteration 2149, loss = 0.41794700\n",
      "Iteration 2150, loss = 0.50186114\n",
      "Iteration 2151, loss = 0.41518400\n",
      "Iteration 2152, loss = 0.35463474\n",
      "Iteration 2153, loss = 0.20329721\n",
      "Iteration 2154, loss = 0.17698661\n",
      "Iteration 2155, loss = 0.17827123\n",
      "Iteration 2156, loss = 0.22821475\n",
      "Iteration 2157, loss = 0.24158377\n",
      "Iteration 2158, loss = 0.20433517\n",
      "Iteration 2159, loss = 0.17608661\n",
      "Iteration 2160, loss = 0.17006741\n",
      "Iteration 2161, loss = 0.12489482\n",
      "Iteration 2162, loss = 0.12171309\n",
      "Iteration 2163, loss = 0.11736725\n",
      "Iteration 2164, loss = 0.13257803\n",
      "Iteration 2165, loss = 0.14056039\n",
      "Iteration 2166, loss = 0.12172608\n",
      "Iteration 2167, loss = 0.12325848\n",
      "Iteration 2168, loss = 0.12907524\n",
      "Iteration 2169, loss = 0.12881538\n",
      "Iteration 2170, loss = 0.13078766\n",
      "Iteration 2171, loss = 0.13139629\n",
      "Iteration 2172, loss = 0.15711548\n",
      "Iteration 2173, loss = 0.13631290\n",
      "Iteration 2174, loss = 0.12556801\n",
      "Iteration 2175, loss = 0.12292101\n",
      "Iteration 2176, loss = 0.12603904\n",
      "Iteration 2177, loss = 0.12702267\n",
      "Iteration 2178, loss = 0.12191978\n",
      "Iteration 2179, loss = 0.13250194\n",
      "Iteration 2180, loss = 0.12070714\n",
      "Iteration 2181, loss = 0.11259627\n",
      "Iteration 2182, loss = 0.12296191\n",
      "Iteration 2183, loss = 0.11722538\n",
      "Iteration 2184, loss = 0.11393887\n",
      "Iteration 2185, loss = 0.11570153\n",
      "Iteration 2186, loss = 0.11678975\n",
      "Iteration 2187, loss = 0.13754875\n",
      "Iteration 2188, loss = 0.12918420\n",
      "Iteration 2189, loss = 0.12865787\n",
      "Iteration 2190, loss = 0.11778871\n",
      "Iteration 2191, loss = 0.11547212\n",
      "Iteration 2192, loss = 0.12093499\n",
      "Iteration 2193, loss = 0.11933262\n",
      "Iteration 2194, loss = 0.12666704\n",
      "Iteration 2195, loss = 0.11782934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2196, loss = 0.13311433\n",
      "Iteration 2197, loss = 0.15814378\n",
      "Iteration 2198, loss = 0.12875746\n",
      "Iteration 2199, loss = 0.12109378\n",
      "Iteration 2200, loss = 0.12618852\n",
      "Iteration 2201, loss = 0.11292301\n",
      "Iteration 2202, loss = 0.11588206\n",
      "Iteration 2203, loss = 0.11365958\n",
      "Iteration 2204, loss = 0.12081445\n",
      "Iteration 2205, loss = 0.11538444\n",
      "Iteration 2206, loss = 0.11472512\n",
      "Iteration 2207, loss = 0.12409278\n",
      "Iteration 2208, loss = 0.12280256\n",
      "Iteration 2209, loss = 0.12477035\n",
      "Iteration 2210, loss = 0.12226981\n",
      "Iteration 2211, loss = 0.11562683\n",
      "Iteration 2212, loss = 0.11539883\n",
      "Iteration 2213, loss = 0.11840652\n",
      "Iteration 2214, loss = 0.11828094\n",
      "Iteration 2215, loss = 0.12037141\n",
      "Iteration 2216, loss = 0.11490868\n",
      "Iteration 2217, loss = 0.12060225\n",
      "Iteration 2218, loss = 0.12007909\n",
      "Iteration 2219, loss = 0.15140540\n",
      "Iteration 2220, loss = 0.15710524\n",
      "Iteration 2221, loss = 0.13895386\n",
      "Iteration 2222, loss = 0.13212574\n",
      "Iteration 2223, loss = 0.17170543\n",
      "Iteration 2224, loss = 0.15836076\n",
      "Iteration 2225, loss = 0.18664920\n",
      "Iteration 2226, loss = 0.14766077\n",
      "Iteration 2227, loss = 0.13330134\n",
      "Iteration 2228, loss = 0.16847610\n",
      "Iteration 2229, loss = 0.15088502\n",
      "Iteration 2230, loss = 0.12670552\n",
      "Iteration 2231, loss = 0.11207386\n",
      "Iteration 2232, loss = 0.11696496\n",
      "Iteration 2233, loss = 0.16145858\n",
      "Iteration 2234, loss = 0.18873427\n",
      "Iteration 2235, loss = 0.19412652\n",
      "Iteration 2236, loss = 0.21259401\n",
      "Iteration 2237, loss = 0.15658765\n",
      "Iteration 2238, loss = 0.18815993\n",
      "Iteration 2239, loss = 0.31651932\n",
      "Iteration 2240, loss = 0.28781128\n",
      "Iteration 2241, loss = 0.16427994\n",
      "Iteration 2242, loss = 0.27526590\n",
      "Iteration 2243, loss = 0.68717171\n",
      "Iteration 2244, loss = 0.54012111\n",
      "Iteration 2245, loss = 0.71027843\n",
      "Iteration 2246, loss = 1.71997391\n",
      "Iteration 2247, loss = 1.22641670\n",
      "Iteration 2248, loss = 1.08573930\n",
      "Iteration 2249, loss = 1.16512162\n",
      "Iteration 2250, loss = 0.90136313\n",
      "Iteration 2251, loss = 1.26757121\n",
      "Iteration 2252, loss = 0.87713163\n",
      "Iteration 2253, loss = 0.80174004\n",
      "Iteration 2254, loss = 0.66766462\n",
      "Iteration 2255, loss = 0.59046093\n",
      "Iteration 2256, loss = 0.62098735\n",
      "Iteration 2257, loss = 0.42857184\n",
      "Iteration 2258, loss = 0.42549025\n",
      "Iteration 2259, loss = 0.29876410\n",
      "Iteration 2260, loss = 0.26877965\n",
      "Iteration 2261, loss = 0.22523828\n",
      "Iteration 2262, loss = 0.20201180\n",
      "Iteration 2263, loss = 0.20828626\n",
      "Iteration 2264, loss = 0.22383610\n",
      "Iteration 2265, loss = 0.25968410\n",
      "Iteration 2266, loss = 0.16715613\n",
      "Iteration 2267, loss = 0.16005047\n",
      "Iteration 2268, loss = 0.13678414\n",
      "Iteration 2269, loss = 0.15393395\n",
      "Iteration 2270, loss = 0.13626822\n",
      "Iteration 2271, loss = 0.26120915\n",
      "Iteration 2272, loss = 0.23163984\n",
      "Iteration 2273, loss = 0.20652391\n",
      "Iteration 2274, loss = 0.16880529\n",
      "Iteration 2275, loss = 0.21821861\n",
      "Iteration 2276, loss = 0.19181388\n",
      "Iteration 2277, loss = 0.20866433\n",
      "Iteration 2278, loss = 0.16481943\n",
      "Iteration 2279, loss = 0.19019257\n",
      "Iteration 2280, loss = 0.15899007\n",
      "Iteration 2281, loss = 0.15742433\n",
      "Iteration 2282, loss = 0.17753867\n",
      "Iteration 2283, loss = 0.17222488\n",
      "Iteration 2284, loss = 0.12286059\n",
      "Iteration 2285, loss = 0.14040130\n",
      "Iteration 2286, loss = 0.11995062\n",
      "Iteration 2287, loss = 0.11992762\n",
      "Iteration 2288, loss = 0.13075964\n",
      "Iteration 2289, loss = 0.12981514\n",
      "Iteration 2290, loss = 0.15403691\n",
      "Iteration 2291, loss = 0.12217470\n",
      "Iteration 2292, loss = 0.11923249\n",
      "Iteration 2293, loss = 0.11862418\n",
      "Iteration 2294, loss = 0.13586410\n",
      "Iteration 2295, loss = 0.12079365\n",
      "Iteration 2296, loss = 0.14827353\n",
      "Iteration 2297, loss = 0.12384596\n",
      "Iteration 2298, loss = 0.11761421\n",
      "Iteration 2299, loss = 0.11732001\n",
      "Iteration 2300, loss = 0.12797867\n",
      "Iteration 2301, loss = 0.13914291\n",
      "Iteration 2302, loss = 0.11274025\n",
      "Iteration 2303, loss = 0.12434370\n",
      "Iteration 2304, loss = 0.11928053\n",
      "Iteration 2305, loss = 0.11685650\n",
      "Iteration 2306, loss = 0.11695576\n",
      "Iteration 2307, loss = 0.11761471\n",
      "Iteration 2308, loss = 0.14189257\n",
      "Iteration 2309, loss = 0.11292303\n",
      "Iteration 2310, loss = 0.11616161\n",
      "Iteration 2311, loss = 0.12625340\n",
      "Iteration 2312, loss = 0.13364228\n",
      "Iteration 2313, loss = 0.13456037\n",
      "Iteration 2314, loss = 0.12285469\n",
      "Iteration 2315, loss = 0.12594269\n",
      "Iteration 2316, loss = 0.12918515\n",
      "Iteration 2317, loss = 0.12938653\n",
      "Iteration 2318, loss = 0.12627508\n",
      "Iteration 2319, loss = 0.12447638\n",
      "Iteration 2320, loss = 0.11842174\n",
      "Iteration 2321, loss = 0.11901826\n",
      "Iteration 2322, loss = 0.11476731\n",
      "Iteration 2323, loss = 0.13505027\n",
      "Iteration 2324, loss = 0.12920092\n",
      "Iteration 2325, loss = 0.12907134\n",
      "Iteration 2326, loss = 0.11796698\n",
      "Iteration 2327, loss = 0.11954027\n",
      "Iteration 2328, loss = 0.12251786\n",
      "Iteration 2329, loss = 0.11928263\n",
      "Iteration 2330, loss = 0.11844641\n",
      "Iteration 2331, loss = 0.11318976\n",
      "Iteration 2332, loss = 0.11695686\n",
      "Iteration 2333, loss = 0.12253157\n",
      "Iteration 2334, loss = 0.13946935\n",
      "Iteration 2335, loss = 0.12572615\n",
      "Iteration 2336, loss = 0.13801464\n",
      "Iteration 2337, loss = 0.12636981\n",
      "Iteration 2338, loss = 0.13075005\n",
      "Iteration 2339, loss = 0.12581860\n",
      "Iteration 2340, loss = 0.11473504\n",
      "Iteration 2341, loss = 0.11933525\n",
      "Iteration 2342, loss = 0.11809885\n",
      "Iteration 2343, loss = 0.12095162\n",
      "Iteration 2344, loss = 0.12505389\n",
      "Iteration 2345, loss = 0.12690072\n",
      "Iteration 2346, loss = 0.12473711\n",
      "Iteration 2347, loss = 0.11680829\n",
      "Iteration 2348, loss = 0.11249541\n",
      "Iteration 2349, loss = 0.12271741\n",
      "Iteration 2350, loss = 0.11822180\n",
      "Iteration 2351, loss = 0.12451380\n",
      "Iteration 2352, loss = 0.11679513\n",
      "Iteration 2353, loss = 0.11963639\n",
      "Iteration 2354, loss = 0.11343296\n",
      "Iteration 2355, loss = 0.12274241\n",
      "Iteration 2356, loss = 0.11817200\n",
      "Iteration 2357, loss = 0.11327567\n",
      "Iteration 2358, loss = 0.11512497\n",
      "Iteration 2359, loss = 0.12410107\n",
      "Iteration 2360, loss = 0.11378835\n",
      "Iteration 2361, loss = 0.11966913\n",
      "Iteration 2362, loss = 0.11661037\n",
      "Iteration 2363, loss = 0.11581826\n",
      "Iteration 2364, loss = 0.11627898\n",
      "Iteration 2365, loss = 0.11155366\n",
      "Iteration 2366, loss = 0.11823892\n",
      "Iteration 2367, loss = 0.14164641\n",
      "Iteration 2368, loss = 0.11323685\n",
      "Iteration 2369, loss = 0.12009258\n",
      "Iteration 2370, loss = 0.12188902\n",
      "Iteration 2371, loss = 0.12659606\n",
      "Iteration 2372, loss = 0.12150153\n",
      "Iteration 2373, loss = 0.14981706\n",
      "Iteration 2374, loss = 0.18356612\n",
      "Iteration 2375, loss = 0.16099224\n",
      "Iteration 2376, loss = 0.12579939\n",
      "Iteration 2377, loss = 0.14117115\n",
      "Iteration 2378, loss = 0.12145892\n",
      "Iteration 2379, loss = 0.14330535\n",
      "Iteration 2380, loss = 0.13543864\n",
      "Iteration 2381, loss = 0.13322990\n",
      "Iteration 2382, loss = 0.14489167\n",
      "Iteration 2383, loss = 0.13856935\n",
      "Iteration 2384, loss = 0.13317163\n",
      "Iteration 2385, loss = 0.12136655\n",
      "Iteration 2386, loss = 0.11336644\n",
      "Iteration 2387, loss = 0.11656436\n",
      "Iteration 2388, loss = 0.12098702\n",
      "Iteration 2389, loss = 0.13994977\n",
      "Iteration 2390, loss = 0.12472395\n",
      "Iteration 2391, loss = 0.11445362\n",
      "Iteration 2392, loss = 0.11605632\n",
      "Iteration 2393, loss = 0.13552312\n",
      "Iteration 2394, loss = 0.11596356\n",
      "Iteration 2395, loss = 0.13455152\n",
      "Iteration 2396, loss = 0.13269848\n",
      "Iteration 2397, loss = 0.12320214\n",
      "Iteration 2398, loss = 0.12439525\n",
      "Iteration 2399, loss = 0.11728377\n",
      "Iteration 2400, loss = 0.11731692\n",
      "Iteration 2401, loss = 0.11643126\n",
      "Iteration 2402, loss = 0.11946661\n",
      "Iteration 2403, loss = 0.11670514\n",
      "Iteration 2404, loss = 0.11841539\n",
      "Iteration 2405, loss = 0.11264594\n",
      "Iteration 2406, loss = 0.11761946\n",
      "Iteration 2407, loss = 0.11484568\n",
      "Iteration 2408, loss = 0.11347210\n",
      "Iteration 2409, loss = 0.11851622\n",
      "Iteration 2410, loss = 0.14338058\n",
      "Iteration 2411, loss = 0.12629026\n",
      "Iteration 2412, loss = 0.11080199\n",
      "Iteration 2413, loss = 0.12470851\n",
      "Iteration 2414, loss = 0.11277891\n",
      "Iteration 2415, loss = 0.11670601\n",
      "Iteration 2416, loss = 0.12808126\n",
      "Iteration 2417, loss = 0.16333416\n",
      "Iteration 2418, loss = 0.14721030\n",
      "Iteration 2419, loss = 0.11506074\n",
      "Iteration 2420, loss = 0.11234635\n",
      "Iteration 2421, loss = 0.11507699\n",
      "Iteration 2422, loss = 0.11420289\n",
      "Iteration 2423, loss = 0.11638524\n",
      "Iteration 2424, loss = 0.12356991\n",
      "Iteration 2425, loss = 0.12901618\n",
      "Iteration 2426, loss = 0.11898815\n",
      "Iteration 2427, loss = 0.12235809\n",
      "Iteration 2428, loss = 0.12183112\n",
      "Iteration 2429, loss = 0.11909751\n",
      "Iteration 2430, loss = 0.12581994\n",
      "Iteration 2431, loss = 0.13305309\n",
      "Iteration 2432, loss = 0.13060778\n",
      "Iteration 2433, loss = 0.14099579\n",
      "Iteration 2434, loss = 0.13013577\n",
      "Iteration 2435, loss = 0.12124015\n",
      "Iteration 2436, loss = 0.12457319\n",
      "Iteration 2437, loss = 0.11517462\n",
      "Iteration 2438, loss = 0.11807187\n",
      "Iteration 2439, loss = 0.11443768\n",
      "Iteration 2440, loss = 0.12335233\n",
      "Iteration 2441, loss = 0.11950781\n",
      "Iteration 2442, loss = 0.13286539\n",
      "Iteration 2443, loss = 0.12485570\n",
      "Iteration 2444, loss = 0.13637338\n",
      "Iteration 2445, loss = 0.13792019\n",
      "Iteration 2446, loss = 0.12087046\n",
      "Iteration 2447, loss = 0.12773534\n",
      "Iteration 2448, loss = 0.12321999\n",
      "Iteration 2449, loss = 0.12902365\n",
      "Iteration 2450, loss = 0.12623720\n",
      "Iteration 2451, loss = 0.12024274\n",
      "Iteration 2452, loss = 0.13066488\n",
      "Iteration 2453, loss = 0.12424630\n",
      "Iteration 2454, loss = 0.11562239\n",
      "Iteration 2455, loss = 0.11806665\n",
      "Iteration 2456, loss = 0.11728400\n",
      "Iteration 2457, loss = 0.17599752\n",
      "Iteration 2458, loss = 0.38709791\n",
      "Iteration 2459, loss = 0.49158097\n",
      "Iteration 2460, loss = 0.36987790\n",
      "Iteration 2461, loss = 0.18960434\n",
      "Iteration 2462, loss = 0.15231526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2463, loss = 0.22636509\n",
      "Iteration 2464, loss = 0.17465331\n",
      "Iteration 2465, loss = 0.15836692\n",
      "Iteration 2466, loss = 0.14734092\n",
      "Iteration 2467, loss = 0.13272265\n",
      "Iteration 2468, loss = 0.12635123\n",
      "Iteration 2469, loss = 0.15315858\n",
      "Iteration 2470, loss = 0.14855481\n",
      "Iteration 2471, loss = 0.11605479\n",
      "Iteration 2472, loss = 0.11395905\n",
      "Iteration 2473, loss = 0.13506555\n",
      "Iteration 2474, loss = 0.12540476\n",
      "Iteration 2475, loss = 0.12986819\n",
      "Iteration 2476, loss = 0.13901855\n",
      "Iteration 2477, loss = 0.11835204\n",
      "Iteration 2478, loss = 0.13881623\n",
      "Iteration 2479, loss = 0.12344121\n",
      "Iteration 2480, loss = 0.13137042\n",
      "Iteration 2481, loss = 0.12494104\n",
      "Iteration 2482, loss = 0.11546047\n",
      "Iteration 2483, loss = 0.11882831\n",
      "Iteration 2484, loss = 0.12385188\n",
      "Iteration 2485, loss = 0.14315191\n",
      "Iteration 2486, loss = 0.15497410\n",
      "Iteration 2487, loss = 0.11519796\n",
      "Iteration 2488, loss = 0.12052411\n",
      "Iteration 2489, loss = 0.12867260\n",
      "Iteration 2490, loss = 0.13425361\n",
      "Iteration 2491, loss = 0.14600536\n",
      "Iteration 2492, loss = 0.17319034\n",
      "Iteration 2493, loss = 0.30992229\n",
      "Iteration 2494, loss = 0.20699896\n",
      "Iteration 2495, loss = 1.17789966\n",
      "Iteration 2496, loss = 5.21254815\n",
      "Iteration 2497, loss = 2.70031989\n",
      "Iteration 2498, loss = 1.56718464\n",
      "Iteration 2499, loss = 1.22555101\n",
      "Iteration 2500, loss = 1.04091770\n",
      "Iteration 2501, loss = 1.47898836\n",
      "Iteration 2502, loss = 1.89051585\n",
      "Iteration 2503, loss = 1.22235380\n",
      "Iteration 2504, loss = 1.10726572\n",
      "Iteration 2505, loss = 0.93455143\n",
      "Iteration 2506, loss = 1.07390148\n",
      "Iteration 2507, loss = 0.97759827\n",
      "Iteration 2508, loss = 0.98095178\n",
      "Iteration 2509, loss = 1.38010006\n",
      "Iteration 2510, loss = 1.37510506\n",
      "Iteration 2511, loss = 0.95135971\n",
      "Iteration 2512, loss = 0.63849795\n",
      "Iteration 2513, loss = 0.53093489\n",
      "Iteration 2514, loss = 0.48575793\n",
      "Iteration 2515, loss = 0.78764654\n",
      "Iteration 2516, loss = 0.91297804\n",
      "Iteration 2517, loss = 1.35203992\n",
      "Iteration 2518, loss = 1.51490284\n",
      "Iteration 2519, loss = 1.21144058\n",
      "Iteration 2520, loss = 0.92912302\n",
      "Iteration 2521, loss = 1.32770571\n",
      "Iteration 2522, loss = 1.15569456\n",
      "Iteration 2523, loss = 0.81595607\n",
      "Iteration 2524, loss = 0.70628390\n",
      "Iteration 2525, loss = 0.65748403\n",
      "Iteration 2526, loss = 0.54848249\n",
      "Iteration 2527, loss = 0.48352017\n",
      "Iteration 2528, loss = 0.38859594\n",
      "Iteration 2529, loss = 0.48649918\n",
      "Iteration 2530, loss = 0.44659942\n",
      "Iteration 2531, loss = 0.36306517\n",
      "Iteration 2532, loss = 0.56578331\n",
      "Iteration 2533, loss = 0.39994877\n",
      "Iteration 2534, loss = 0.40874961\n",
      "Iteration 2535, loss = 0.57905397\n",
      "Iteration 2536, loss = 0.42345979\n",
      "Iteration 2537, loss = 0.49347178\n",
      "Iteration 2538, loss = 0.50202194\n",
      "Iteration 2539, loss = 0.37058176\n",
      "Iteration 2540, loss = 0.37602770\n",
      "Iteration 2541, loss = 0.17883996\n",
      "Iteration 2542, loss = 0.18033236\n",
      "Iteration 2543, loss = 0.16201402\n",
      "Iteration 2544, loss = 0.12868021\n",
      "Iteration 2545, loss = 0.16697261\n",
      "Iteration 2546, loss = 0.20026296\n",
      "Iteration 2547, loss = 0.21326949\n",
      "Iteration 2548, loss = 0.18654161\n",
      "Iteration 2549, loss = 0.15323701\n",
      "Iteration 2550, loss = 0.14016926\n",
      "Iteration 2551, loss = 0.13813992\n",
      "Iteration 2552, loss = 0.12919292\n",
      "Iteration 2553, loss = 0.13865788\n",
      "Iteration 2554, loss = 0.12958195\n",
      "Iteration 2555, loss = 0.12481723\n",
      "Iteration 2556, loss = 0.12205049\n",
      "Iteration 2557, loss = 0.11481137\n",
      "Iteration 2558, loss = 0.12381669\n",
      "Iteration 2559, loss = 0.12109676\n",
      "Iteration 2560, loss = 0.11830662\n",
      "Iteration 2561, loss = 0.12397237\n",
      "Iteration 2562, loss = 0.14853967\n",
      "Iteration 2563, loss = 0.16476061\n",
      "Iteration 2564, loss = 0.15388687\n",
      "Iteration 2565, loss = 0.13892060\n",
      "Iteration 2566, loss = 0.13844974\n",
      "Iteration 2567, loss = 0.16595691\n",
      "Iteration 2568, loss = 0.13206805\n",
      "Iteration 2569, loss = 0.12211998\n",
      "Iteration 2570, loss = 0.12712615\n",
      "Iteration 2571, loss = 0.11645046\n",
      "Iteration 2572, loss = 0.13013758\n",
      "Iteration 2573, loss = 0.12275041\n",
      "Iteration 2574, loss = 0.11919102\n",
      "Iteration 2575, loss = 0.11656598\n",
      "Iteration 2576, loss = 0.11193475\n",
      "Iteration 2577, loss = 0.11712092\n",
      "Iteration 2578, loss = 0.12417653\n",
      "Iteration 2579, loss = 0.12252050\n",
      "Iteration 2580, loss = 0.12440932\n",
      "Iteration 2581, loss = 0.12051878\n",
      "Iteration 2582, loss = 0.13917376\n",
      "Iteration 2583, loss = 0.12711427\n",
      "Iteration 2584, loss = 0.13410189\n",
      "Iteration 2585, loss = 0.12327070\n",
      "Iteration 2586, loss = 0.12378481\n",
      "Iteration 2587, loss = 0.13126426\n",
      "Iteration 2588, loss = 0.14040217\n",
      "Iteration 2589, loss = 0.21011941\n",
      "Iteration 2590, loss = 0.14970673\n",
      "Iteration 2591, loss = 0.13675967\n",
      "Iteration 2592, loss = 0.13597247\n",
      "Iteration 2593, loss = 0.13318078\n",
      "Iteration 2594, loss = 0.12934733\n",
      "Iteration 2595, loss = 0.13716707\n",
      "Iteration 2596, loss = 0.13564994\n",
      "Iteration 2597, loss = 0.11896814\n",
      "Iteration 2598, loss = 0.13773083\n",
      "Iteration 2599, loss = 0.12468379\n",
      "Iteration 2600, loss = 0.11978301\n",
      "Iteration 2601, loss = 0.12399188\n",
      "Iteration 2602, loss = 0.13316038\n",
      "Iteration 2603, loss = 0.12072316\n",
      "Iteration 2604, loss = 0.11463205\n",
      "Iteration 2605, loss = 0.11396392\n",
      "Iteration 2606, loss = 0.11187024\n",
      "Iteration 2607, loss = 0.11805910\n",
      "Iteration 2608, loss = 0.11516794\n",
      "Iteration 2609, loss = 0.15621051\n",
      "Iteration 2610, loss = 0.12544848\n",
      "Iteration 2611, loss = 0.11820242\n",
      "Iteration 2612, loss = 0.13817107\n",
      "Iteration 2613, loss = 0.12190757\n",
      "Iteration 2614, loss = 0.13183852\n",
      "Iteration 2615, loss = 0.11860261\n",
      "Iteration 2616, loss = 0.12372507\n",
      "Iteration 2617, loss = 0.15965937\n",
      "Iteration 2618, loss = 0.13574052\n",
      "Iteration 2619, loss = 0.13658123\n",
      "Iteration 2620, loss = 0.11907655\n",
      "Iteration 2621, loss = 0.13184504\n",
      "Iteration 2622, loss = 0.13111871\n",
      "Iteration 2623, loss = 0.11218424\n",
      "Iteration 2624, loss = 0.12373702\n",
      "Iteration 2625, loss = 0.18540645\n",
      "Iteration 2626, loss = 0.27684536\n",
      "Iteration 2627, loss = 0.19412687\n",
      "Iteration 2628, loss = 0.12470510\n",
      "Iteration 2629, loss = 0.12507947\n",
      "Iteration 2630, loss = 0.11346201\n",
      "Iteration 2631, loss = 0.11613945\n",
      "Iteration 2632, loss = 0.11822521\n",
      "Iteration 2633, loss = 0.11677824\n",
      "Iteration 2634, loss = 0.11620701\n",
      "Iteration 2635, loss = 0.11470340\n",
      "Iteration 2636, loss = 0.13046511\n",
      "Iteration 2637, loss = 0.12266812\n",
      "Iteration 2638, loss = 0.12488898\n",
      "Iteration 2639, loss = 0.12252179\n",
      "Iteration 2640, loss = 0.12435704\n",
      "Iteration 2641, loss = 0.11509768\n",
      "Iteration 2642, loss = 0.24832618\n",
      "Iteration 2643, loss = 0.42779421\n",
      "Iteration 2644, loss = 0.43574256\n",
      "Iteration 2645, loss = 0.33042561\n",
      "Iteration 2646, loss = 0.17163274\n",
      "Iteration 2647, loss = 0.16916702\n",
      "Iteration 2648, loss = 0.18163891\n",
      "Iteration 2649, loss = 0.14431142\n",
      "Iteration 2650, loss = 0.20717926\n",
      "Iteration 2651, loss = 0.17952300\n",
      "Iteration 2652, loss = 0.12452673\n",
      "Iteration 2653, loss = 0.13559574\n",
      "Iteration 2654, loss = 0.14236140\n",
      "Iteration 2655, loss = 0.12512605\n",
      "Iteration 2656, loss = 0.13606486\n",
      "Iteration 2657, loss = 0.12877052\n",
      "Iteration 2658, loss = 0.14390085\n",
      "Iteration 2659, loss = 0.13283919\n",
      "Iteration 2660, loss = 0.14453066\n",
      "Iteration 2661, loss = 0.15155317\n",
      "Iteration 2662, loss = 0.13290112\n",
      "Iteration 2663, loss = 0.15568915\n",
      "Iteration 2664, loss = 0.13649917\n",
      "Iteration 2665, loss = 0.11824183\n",
      "Iteration 2666, loss = 0.12171412\n",
      "Iteration 2667, loss = 0.11764273\n",
      "Iteration 2668, loss = 0.12631779\n",
      "Iteration 2669, loss = 0.11992605\n",
      "Iteration 2670, loss = 0.13597159\n",
      "Iteration 2671, loss = 0.12023311\n",
      "Iteration 2672, loss = 0.12002752\n",
      "Iteration 2673, loss = 0.12412496\n",
      "Iteration 2674, loss = 0.11656355\n",
      "Iteration 2675, loss = 0.13475705\n",
      "Iteration 2676, loss = 0.11361117\n",
      "Iteration 2677, loss = 0.13112527\n",
      "Iteration 2678, loss = 0.12531817\n",
      "Iteration 2679, loss = 0.11874102\n",
      "Iteration 2680, loss = 0.14285213\n",
      "Iteration 2681, loss = 0.14025087\n",
      "Iteration 2682, loss = 0.11922536\n",
      "Iteration 2683, loss = 0.12137982\n",
      "Iteration 2684, loss = 0.12060672\n",
      "Iteration 2685, loss = 0.11323875\n",
      "Iteration 2686, loss = 0.13205377\n",
      "Iteration 2687, loss = 0.15245429\n",
      "Iteration 2688, loss = 0.13277189\n",
      "Iteration 2689, loss = 0.12665777\n",
      "Iteration 2690, loss = 0.12276940\n",
      "Iteration 2691, loss = 0.11606084\n",
      "Iteration 2692, loss = 0.11806443\n",
      "Iteration 2693, loss = 0.12270165\n",
      "Iteration 2694, loss = 0.11864636\n",
      "Iteration 2695, loss = 0.11682489\n",
      "Iteration 2696, loss = 0.12850978\n",
      "Iteration 2697, loss = 0.12607424\n",
      "Iteration 2698, loss = 0.11671084\n",
      "Iteration 2699, loss = 0.11349412\n",
      "Iteration 2700, loss = 0.11866656\n",
      "Iteration 2701, loss = 0.11977048\n",
      "Iteration 2702, loss = 0.13470103\n",
      "Iteration 2703, loss = 0.13624856\n",
      "Iteration 2704, loss = 0.16837704\n",
      "Iteration 2705, loss = 0.14628988\n",
      "Iteration 2706, loss = 0.11504180\n",
      "Iteration 2707, loss = 0.11984505\n",
      "Iteration 2708, loss = 0.11996573\n",
      "Iteration 2709, loss = 0.12462309\n",
      "Iteration 2710, loss = 0.12817298\n",
      "Iteration 2711, loss = 0.13337196\n",
      "Iteration 2712, loss = 0.12524205\n",
      "Iteration 2713, loss = 0.15926161\n",
      "Iteration 2714, loss = 0.27730260\n",
      "Iteration 2715, loss = 0.25285002\n",
      "Iteration 2716, loss = 0.16398962\n",
      "Iteration 2717, loss = 0.20934570\n",
      "Iteration 2718, loss = 0.23627265\n",
      "Iteration 2719, loss = 0.15275944\n",
      "Iteration 2720, loss = 0.13782018\n",
      "Iteration 2721, loss = 0.14393510\n",
      "Iteration 2722, loss = 0.11066015\n",
      "Iteration 2723, loss = 0.11991398\n",
      "Iteration 2724, loss = 0.11662323\n",
      "Iteration 2725, loss = 0.11959554\n",
      "Iteration 2726, loss = 0.13342128\n",
      "Iteration 2727, loss = 0.12693585\n",
      "Iteration 2728, loss = 0.11917601\n",
      "Iteration 2729, loss = 0.11773969\n",
      "Iteration 2730, loss = 0.11812754\n",
      "Iteration 2731, loss = 0.11600564\n",
      "Iteration 2732, loss = 0.12028141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2733, loss = 0.13684158\n",
      "Iteration 2734, loss = 0.11658912\n",
      "Iteration 2735, loss = 0.13038963\n",
      "Iteration 2736, loss = 0.12176583\n",
      "Iteration 2737, loss = 0.12638401\n",
      "Iteration 2738, loss = 0.11986382\n",
      "Iteration 2739, loss = 0.11726517\n",
      "Iteration 2740, loss = 0.11568408\n",
      "Iteration 2741, loss = 0.11333873\n",
      "Iteration 2742, loss = 0.11377842\n",
      "Iteration 2743, loss = 0.13155504\n",
      "Iteration 2744, loss = 0.19675867\n",
      "Iteration 2745, loss = 0.15907057\n",
      "Iteration 2746, loss = 0.24851468\n",
      "Iteration 2747, loss = 0.27462006\n",
      "Iteration 2748, loss = 0.26197810\n",
      "Iteration 2749, loss = 0.13595889\n",
      "Iteration 2750, loss = 0.14477560\n",
      "Iteration 2751, loss = 0.19729760\n",
      "Iteration 2752, loss = 0.14596631\n",
      "Iteration 2753, loss = 0.12777505\n",
      "Iteration 2754, loss = 0.11590971\n",
      "Iteration 2755, loss = 0.11966607\n",
      "Iteration 2756, loss = 0.11567042\n",
      "Iteration 2757, loss = 0.13288794\n",
      "Iteration 2758, loss = 0.12103177\n",
      "Iteration 2759, loss = 0.12734326\n",
      "Iteration 2760, loss = 0.12166840\n",
      "Iteration 2761, loss = 0.11227730\n",
      "Iteration 2762, loss = 0.13284982\n",
      "Iteration 2763, loss = 0.12385094\n",
      "Iteration 2764, loss = 0.13228622\n",
      "Iteration 2765, loss = 0.12297862\n",
      "Iteration 2766, loss = 0.13998846\n",
      "Iteration 2767, loss = 0.12388560\n",
      "Iteration 2768, loss = 0.13864140\n",
      "Iteration 2769, loss = 0.13612090\n",
      "Iteration 2770, loss = 0.14310379\n",
      "Iteration 2771, loss = 0.14204355\n",
      "Iteration 2772, loss = 0.12865722\n",
      "Iteration 2773, loss = 0.12108386\n",
      "Iteration 2774, loss = 0.13635843\n",
      "Iteration 2775, loss = 0.12446112\n",
      "Iteration 2776, loss = 0.11706233\n",
      "Iteration 2777, loss = 0.11848167\n",
      "Iteration 2778, loss = 0.12217015\n",
      "Iteration 2779, loss = 0.14837344\n",
      "Iteration 2780, loss = 0.12220352\n",
      "Iteration 2781, loss = 0.13654790\n",
      "Iteration 2782, loss = 0.14262132\n",
      "Iteration 2783, loss = 0.17855537\n",
      "Iteration 2784, loss = 0.16592769\n",
      "Iteration 2785, loss = 0.14289739\n",
      "Iteration 2786, loss = 0.22612690\n",
      "Iteration 2787, loss = 0.23332539\n",
      "Iteration 2788, loss = 0.13167252\n",
      "Iteration 2789, loss = 0.12635257\n",
      "Iteration 2790, loss = 0.15564530\n",
      "Iteration 2791, loss = 0.11773563\n",
      "Iteration 2792, loss = 0.11948993\n",
      "Iteration 2793, loss = 0.12949119\n",
      "Iteration 2794, loss = 0.11512962\n",
      "Iteration 2795, loss = 0.11928142\n",
      "Iteration 2796, loss = 0.13297156\n",
      "Iteration 2797, loss = 0.13469132\n",
      "Iteration 2798, loss = 0.14023166\n",
      "Iteration 2799, loss = 0.12522454\n",
      "Iteration 2800, loss = 0.13128195\n",
      "Iteration 2801, loss = 0.11564583\n",
      "Iteration 2802, loss = 0.17206293\n",
      "Iteration 2803, loss = 0.31962120\n",
      "Iteration 2804, loss = 0.38852765\n",
      "Iteration 2805, loss = 0.27383244\n",
      "Iteration 2806, loss = 0.16830137\n",
      "Iteration 2807, loss = 0.15995430\n",
      "Iteration 2808, loss = 0.17135704\n",
      "Iteration 2809, loss = 0.11949460\n",
      "Iteration 2810, loss = 0.12429638\n",
      "Iteration 2811, loss = 0.12657792\n",
      "Iteration 2812, loss = 0.12368806\n",
      "Iteration 2813, loss = 0.14595699\n",
      "Iteration 2814, loss = 0.12701891\n",
      "Iteration 2815, loss = 0.20363822\n",
      "Iteration 2816, loss = 0.23641520\n",
      "Iteration 2817, loss = 0.22378995\n",
      "Iteration 2818, loss = 0.13134692\n",
      "Iteration 2819, loss = 0.11740875\n",
      "Iteration 2820, loss = 0.11689667\n",
      "Iteration 2821, loss = 0.13372775\n",
      "Iteration 2822, loss = 0.13026469\n",
      "Iteration 2823, loss = 0.11809523\n",
      "Iteration 2824, loss = 0.12136477\n",
      "Iteration 2825, loss = 0.11910022\n",
      "Iteration 2826, loss = 0.13985581\n",
      "Iteration 2827, loss = 0.18954975\n",
      "Iteration 2828, loss = 0.16771708\n",
      "Iteration 2829, loss = 0.12266167\n",
      "Iteration 2830, loss = 0.18617035\n",
      "Iteration 2831, loss = 0.14699885\n",
      "Iteration 2832, loss = 0.12389614\n",
      "Iteration 2833, loss = 0.12152594\n",
      "Iteration 2834, loss = 0.12135166\n",
      "Iteration 2835, loss = 0.12317613\n",
      "Iteration 2836, loss = 0.14334386\n",
      "Iteration 2837, loss = 0.11929687\n",
      "Iteration 2838, loss = 0.12617824\n",
      "Iteration 2839, loss = 0.12148434\n",
      "Iteration 2840, loss = 0.14939017\n",
      "Iteration 2841, loss = 0.13122918\n",
      "Iteration 2842, loss = 0.11922099\n",
      "Iteration 2843, loss = 0.12674040\n",
      "Iteration 2844, loss = 0.11592031\n",
      "Iteration 2845, loss = 0.12168962\n",
      "Iteration 2846, loss = 0.11625066\n",
      "Iteration 2847, loss = 0.12547719\n",
      "Iteration 2848, loss = 0.11273921\n",
      "Iteration 2849, loss = 0.11612371\n",
      "Iteration 2850, loss = 0.12428082\n",
      "Iteration 2851, loss = 0.11970649\n",
      "Iteration 2852, loss = 0.11543763\n",
      "Iteration 2853, loss = 0.12527110\n",
      "Iteration 2854, loss = 0.11521842\n",
      "Iteration 2855, loss = 0.11328151\n",
      "Iteration 2856, loss = 0.13078275\n",
      "Iteration 2857, loss = 0.11678989\n",
      "Iteration 2858, loss = 0.11973652\n",
      "Iteration 2859, loss = 0.11040417\n",
      "Iteration 2860, loss = 0.12504409\n",
      "Iteration 2861, loss = 0.11563388\n",
      "Iteration 2862, loss = 0.11221729\n",
      "Iteration 2863, loss = 0.11824220\n",
      "Iteration 2864, loss = 0.12530337\n",
      "Iteration 2865, loss = 0.12928765\n",
      "Iteration 2866, loss = 0.16639414\n",
      "Iteration 2867, loss = 0.13921877\n",
      "Iteration 2868, loss = 0.13128384\n",
      "Iteration 2869, loss = 0.12471960\n",
      "Iteration 2870, loss = 0.11479630\n",
      "Iteration 2871, loss = 0.13367298\n",
      "Iteration 2872, loss = 0.11384328\n",
      "Iteration 2873, loss = 0.11234132\n",
      "Iteration 2874, loss = 0.12002202\n",
      "Iteration 2875, loss = 0.11574198\n",
      "Iteration 2876, loss = 0.13137449\n",
      "Iteration 2877, loss = 0.12018806\n",
      "Iteration 2878, loss = 0.11627132\n",
      "Iteration 2879, loss = 0.13077831\n",
      "Iteration 2880, loss = 0.13125993\n",
      "Iteration 2881, loss = 0.11260635\n",
      "Iteration 2882, loss = 0.11697543\n",
      "Iteration 2883, loss = 0.11498069\n",
      "Iteration 2884, loss = 0.11811829\n",
      "Iteration 2885, loss = 0.11424641\n",
      "Iteration 2886, loss = 0.11744228\n",
      "Iteration 2887, loss = 0.11388624\n",
      "Iteration 2888, loss = 0.11528482\n",
      "Iteration 2889, loss = 0.11470412\n",
      "Iteration 2890, loss = 0.11443548\n",
      "Iteration 2891, loss = 0.12326009\n",
      "Iteration 2892, loss = 0.11974756\n",
      "Iteration 2893, loss = 0.14062657\n",
      "Iteration 2894, loss = 0.13046141\n",
      "Iteration 2895, loss = 0.12425153\n",
      "Iteration 2896, loss = 0.11679868\n",
      "Iteration 2897, loss = 0.11991534\n",
      "Iteration 2898, loss = 0.12308077\n",
      "Iteration 2899, loss = 0.11744471\n",
      "Iteration 2900, loss = 0.12877267\n",
      "Iteration 2901, loss = 0.11591389\n",
      "Iteration 2902, loss = 0.12669751\n",
      "Iteration 2903, loss = 0.11932698\n",
      "Iteration 2904, loss = 0.12753680\n",
      "Iteration 2905, loss = 0.11440944\n",
      "Iteration 2906, loss = 0.12570285\n",
      "Iteration 2907, loss = 0.12068947\n",
      "Iteration 2908, loss = 0.15822257\n",
      "Iteration 2909, loss = 0.13008898\n",
      "Iteration 2910, loss = 0.14710289\n",
      "Iteration 2911, loss = 0.11801514\n",
      "Iteration 2912, loss = 0.11714332\n",
      "Iteration 2913, loss = 0.13603915\n",
      "Iteration 2914, loss = 0.13338292\n",
      "Iteration 2915, loss = 0.13721394\n",
      "Iteration 2916, loss = 0.16088193\n",
      "Iteration 2917, loss = 0.14310909\n",
      "Iteration 2918, loss = 0.13496556\n",
      "Iteration 2919, loss = 0.12064032\n",
      "Iteration 2920, loss = 0.14002256\n",
      "Iteration 2921, loss = 0.14420902\n",
      "Iteration 2922, loss = 0.25931369\n",
      "Iteration 2923, loss = 0.28136712\n",
      "Iteration 2924, loss = 0.19806595\n",
      "Iteration 2925, loss = 0.14332372\n",
      "Iteration 2926, loss = 0.12315704\n",
      "Iteration 2927, loss = 0.17795466\n",
      "Iteration 2928, loss = 0.17128717\n",
      "Iteration 2929, loss = 0.18158558\n",
      "Iteration 2930, loss = 0.19813883\n",
      "Iteration 2931, loss = 0.14386700\n",
      "Iteration 2932, loss = 0.14144879\n",
      "Iteration 2933, loss = 0.12408014\n",
      "Iteration 2934, loss = 0.12176596\n",
      "Iteration 2935, loss = 0.11173042\n",
      "Iteration 2936, loss = 0.11974952\n",
      "Iteration 2937, loss = 0.12657163\n",
      "Iteration 2938, loss = 0.13704327\n",
      "Iteration 2939, loss = 0.13432146\n",
      "Iteration 2940, loss = 0.11635291\n",
      "Iteration 2941, loss = 0.11537933\n",
      "Iteration 2942, loss = 0.13344574\n",
      "Iteration 2943, loss = 0.12407934\n",
      "Iteration 2944, loss = 0.13411364\n",
      "Iteration 2945, loss = 0.14194630\n",
      "Iteration 2946, loss = 0.20335426\n",
      "Iteration 2947, loss = 0.21591222\n",
      "Iteration 2948, loss = 0.12646964\n",
      "Iteration 2949, loss = 0.14674670\n",
      "Iteration 2950, loss = 0.16473194\n",
      "Iteration 2951, loss = 0.20989838\n",
      "Iteration 2952, loss = 0.26606383\n",
      "Iteration 2953, loss = 0.17047015\n",
      "Iteration 2954, loss = 0.27448058\n",
      "Iteration 2955, loss = 0.30787895\n",
      "Iteration 2956, loss = 1.00247676\n",
      "Iteration 2957, loss = 0.80046929\n",
      "Iteration 2958, loss = 1.25116903\n",
      "Iteration 2959, loss = 1.16118968\n",
      "Iteration 2960, loss = 1.94737913\n",
      "Iteration 2961, loss = 3.13613858\n",
      "Iteration 2962, loss = 2.91947447\n",
      "Iteration 2963, loss = 3.10593953\n",
      "Iteration 2964, loss = 2.37476804\n",
      "Iteration 2965, loss = 1.95826046\n",
      "Iteration 2966, loss = 1.54983461\n",
      "Iteration 2967, loss = 1.23883351\n",
      "Iteration 2968, loss = 1.11136729\n",
      "Iteration 2969, loss = 1.34681715\n",
      "Iteration 2970, loss = 0.87869355\n",
      "Iteration 2971, loss = 1.08209353\n",
      "Iteration 2972, loss = 0.70901842\n",
      "Iteration 2973, loss = 0.86858787\n",
      "Iteration 2974, loss = 1.39622763\n",
      "Iteration 2975, loss = 0.78756443\n",
      "Iteration 2976, loss = 0.73235411\n",
      "Iteration 2977, loss = 0.89707878\n",
      "Iteration 2978, loss = 0.69682865\n",
      "Iteration 2979, loss = 0.63330772\n",
      "Iteration 2980, loss = 0.73270688\n",
      "Iteration 2981, loss = 0.79651392\n",
      "Iteration 2982, loss = 1.38748762\n",
      "Iteration 2983, loss = 0.86675746\n",
      "Iteration 2984, loss = 1.06341118\n",
      "Iteration 2985, loss = 0.77440828\n",
      "Iteration 2986, loss = 0.70422161\n",
      "Iteration 2987, loss = 0.73980791\n",
      "Iteration 2988, loss = 0.69404895\n",
      "Iteration 2989, loss = 0.60463293\n",
      "Iteration 2990, loss = 0.53637573\n",
      "Iteration 2991, loss = 0.47127069\n",
      "Iteration 2992, loss = 0.26146205\n",
      "Iteration 2993, loss = 0.26818947\n",
      "Iteration 2994, loss = 0.69256794\n",
      "Iteration 2995, loss = 1.19746415\n",
      "Iteration 2996, loss = 0.97323592\n",
      "Iteration 2997, loss = 0.59982855\n",
      "Iteration 2998, loss = 0.69106335\n",
      "Iteration 2999, loss = 0.67018252\n",
      "Iteration 3000, loss = 0.55103234\n",
      "Iteration 3001, loss = 0.52790488\n",
      "Iteration 3002, loss = 0.49978680\n",
      "Iteration 3003, loss = 0.31898495\n",
      "Iteration 3004, loss = 0.34603889\n",
      "Iteration 3005, loss = 0.33235770\n",
      "Iteration 3006, loss = 0.26631202\n",
      "Iteration 3007, loss = 0.23530658\n",
      "Iteration 3008, loss = 0.34235199\n",
      "Iteration 3009, loss = 0.30303183\n",
      "Iteration 3010, loss = 0.20401560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3011, loss = 0.22336101\n",
      "Iteration 3012, loss = 0.25390981\n",
      "Iteration 3013, loss = 0.17027296\n",
      "Iteration 3014, loss = 0.20688956\n",
      "Iteration 3015, loss = 0.21515784\n",
      "Iteration 3016, loss = 0.13643723\n",
      "Iteration 3017, loss = 0.17349354\n",
      "Iteration 3018, loss = 0.14859024\n",
      "Iteration 3019, loss = 0.13555770\n",
      "Iteration 3020, loss = 0.13344637\n",
      "Iteration 3021, loss = 0.16680653\n",
      "Iteration 3022, loss = 0.19270189\n",
      "Iteration 3023, loss = 0.13579191\n",
      "Iteration 3024, loss = 0.16200285\n",
      "Iteration 3025, loss = 0.13973934\n",
      "Iteration 3026, loss = 0.12931906\n",
      "Iteration 3027, loss = 0.19119393\n",
      "Iteration 3028, loss = 0.28167964\n",
      "Iteration 3029, loss = 0.59140685\n",
      "Iteration 3030, loss = 0.55022178\n",
      "Iteration 3031, loss = 0.49974373\n",
      "Iteration 3032, loss = 0.32952142\n",
      "Iteration 3033, loss = 0.20472493\n",
      "Iteration 3034, loss = 0.20750643\n",
      "Iteration 3035, loss = 0.19543754\n",
      "Iteration 3036, loss = 0.15451032\n",
      "Iteration 3037, loss = 0.12342971\n",
      "Iteration 3038, loss = 0.13802496\n",
      "Iteration 3039, loss = 0.14079553\n",
      "Iteration 3040, loss = 0.15623054\n",
      "Iteration 3041, loss = 0.14429070\n",
      "Iteration 3042, loss = 0.13588559\n",
      "Iteration 3043, loss = 0.12549033\n",
      "Iteration 3044, loss = 0.14274941\n",
      "Iteration 3045, loss = 0.14354245\n",
      "Iteration 3046, loss = 0.13477802\n",
      "Iteration 3047, loss = 0.14111699\n",
      "Iteration 3048, loss = 0.13143448\n",
      "Iteration 3049, loss = 0.12311693\n",
      "Iteration 3050, loss = 0.14977220\n",
      "Iteration 3051, loss = 0.14031707\n",
      "Iteration 3052, loss = 0.11997527\n",
      "Iteration 3053, loss = 0.14131981\n",
      "Iteration 3054, loss = 0.18398450\n",
      "Iteration 3055, loss = 0.15378944\n",
      "Iteration 3056, loss = 0.12698914\n",
      "Iteration 3057, loss = 0.12760532\n",
      "Iteration 3058, loss = 0.12046734\n",
      "Iteration 3059, loss = 0.14222665\n",
      "Iteration 3060, loss = 0.12660609\n",
      "Iteration 3061, loss = 0.13179738\n",
      "Iteration 3062, loss = 0.11828436\n",
      "Iteration 3063, loss = 0.11809241\n",
      "Iteration 3064, loss = 0.11854023\n",
      "Iteration 3065, loss = 0.11712047\n",
      "Iteration 3066, loss = 0.12082520\n",
      "Iteration 3067, loss = 0.12001040\n",
      "Iteration 3068, loss = 0.12108097\n",
      "Iteration 3069, loss = 0.13237895\n",
      "Iteration 3070, loss = 0.11402820\n",
      "Iteration 3071, loss = 0.11422992\n",
      "Iteration 3072, loss = 0.11399157\n",
      "Iteration 3073, loss = 0.11362130\n",
      "Iteration 3074, loss = 0.11838171\n",
      "Iteration 3075, loss = 0.11942791\n",
      "Iteration 3076, loss = 0.11507800\n",
      "Iteration 3077, loss = 0.12038047\n",
      "Iteration 3078, loss = 0.12886210\n",
      "Iteration 3079, loss = 0.12128301\n",
      "Iteration 3080, loss = 0.12050119\n",
      "Iteration 3081, loss = 0.11552261\n",
      "Iteration 3082, loss = 0.12889030\n",
      "Iteration 3083, loss = 0.16770335\n",
      "Iteration 3084, loss = 0.11937847\n",
      "Iteration 3085, loss = 0.12530955\n",
      "Iteration 3086, loss = 0.12569231\n",
      "Iteration 3087, loss = 0.11491823\n",
      "Iteration 3088, loss = 0.12583926\n",
      "Iteration 3089, loss = 0.11842485\n",
      "Iteration 3090, loss = 0.12577074\n",
      "Iteration 3091, loss = 0.12278488\n",
      "Iteration 3092, loss = 0.12208040\n",
      "Iteration 3093, loss = 0.12800099\n",
      "Iteration 3094, loss = 0.14624823\n",
      "Iteration 3095, loss = 0.18339595\n",
      "Iteration 3096, loss = 0.15834338\n",
      "Iteration 3097, loss = 0.13791656\n",
      "Iteration 3098, loss = 0.18797578\n",
      "Iteration 3099, loss = 0.16586597\n",
      "Iteration 3100, loss = 0.12482158\n",
      "Iteration 3101, loss = 0.13477076\n",
      "Iteration 3102, loss = 0.11752896\n",
      "Iteration 3103, loss = 0.12234643\n",
      "Iteration 3104, loss = 0.12273685\n",
      "Iteration 3105, loss = 0.12923901\n",
      "Iteration 3106, loss = 0.14467942\n",
      "Iteration 3107, loss = 0.13107275\n",
      "Iteration 3108, loss = 0.12593411\n",
      "Iteration 3109, loss = 0.13066525\n",
      "Iteration 3110, loss = 0.15739311\n",
      "Iteration 3111, loss = 0.14084034\n",
      "Iteration 3112, loss = 0.15329123\n",
      "Iteration 3113, loss = 0.18537433\n",
      "Iteration 3114, loss = 0.17210989\n",
      "Iteration 3115, loss = 0.13901483\n",
      "Iteration 3116, loss = 0.16022244\n",
      "Iteration 3117, loss = 0.14271360\n",
      "Iteration 3118, loss = 0.15817085\n",
      "Iteration 3119, loss = 0.15793519\n",
      "Iteration 3120, loss = 0.11885194\n",
      "Iteration 3121, loss = 0.15277229\n",
      "Iteration 3122, loss = 0.14343109\n",
      "Iteration 3123, loss = 0.14590508\n",
      "Iteration 3124, loss = 0.14334981\n",
      "Iteration 3125, loss = 0.11863659\n",
      "Iteration 3126, loss = 0.12498105\n",
      "Iteration 3127, loss = 0.13035476\n",
      "Iteration 3128, loss = 0.13028771\n",
      "Iteration 3129, loss = 0.13336970\n",
      "Iteration 3130, loss = 0.12337258\n",
      "Iteration 3131, loss = 0.12686553\n",
      "Iteration 3132, loss = 0.11367995\n",
      "Iteration 3133, loss = 0.12641587\n",
      "Iteration 3134, loss = 0.11207489\n",
      "Iteration 3135, loss = 0.13111417\n",
      "Iteration 3136, loss = 0.11879935\n",
      "Iteration 3137, loss = 0.12064476\n",
      "Iteration 3138, loss = 0.12263207\n",
      "Iteration 3139, loss = 0.11640722\n",
      "Iteration 3140, loss = 0.11523300\n",
      "Iteration 3141, loss = 0.12848815\n",
      "Iteration 3142, loss = 0.13899510\n",
      "Iteration 3143, loss = 0.11835595\n",
      "Iteration 3144, loss = 0.12217634\n",
      "Iteration 3145, loss = 0.12764758\n",
      "Iteration 3146, loss = 0.12706760\n",
      "Iteration 3147, loss = 0.13911677\n",
      "Iteration 3148, loss = 0.12155094\n",
      "Iteration 3149, loss = 0.12781211\n",
      "Iteration 3150, loss = 0.11374288\n",
      "Iteration 3151, loss = 0.12342543\n",
      "Iteration 3152, loss = 0.11118137\n",
      "Iteration 3153, loss = 0.12357878\n",
      "Iteration 3154, loss = 0.12361608\n",
      "Iteration 3155, loss = 0.12872576\n",
      "Iteration 3156, loss = 0.14609955\n",
      "Iteration 3157, loss = 0.11706891\n",
      "Iteration 3158, loss = 0.11767550\n",
      "Iteration 3159, loss = 0.12421957\n",
      "Iteration 3160, loss = 0.12258703\n",
      "Iteration 3161, loss = 0.11910972\n",
      "Iteration 3162, loss = 0.12304634\n",
      "Iteration 3163, loss = 0.11825812\n",
      "Iteration 3164, loss = 0.11624775\n",
      "Iteration 3165, loss = 0.12743075\n",
      "Iteration 3166, loss = 0.11321965\n",
      "Iteration 3167, loss = 0.12168155\n",
      "Iteration 3168, loss = 0.13666063\n",
      "Iteration 3169, loss = 0.20881756\n",
      "Iteration 3170, loss = 0.17853139\n",
      "Iteration 3171, loss = 0.14960379\n",
      "Iteration 3172, loss = 0.17256744\n",
      "Iteration 3173, loss = 0.15203023\n",
      "Iteration 3174, loss = 0.11921137\n",
      "Iteration 3175, loss = 0.13838447\n",
      "Iteration 3176, loss = 0.12941477\n",
      "Iteration 3177, loss = 0.18138672\n",
      "Iteration 3178, loss = 0.13669336\n",
      "Iteration 3179, loss = 0.13446883\n",
      "Iteration 3180, loss = 0.14985061\n",
      "Iteration 3181, loss = 0.12886563\n",
      "Iteration 3182, loss = 0.12548218\n",
      "Iteration 3183, loss = 0.13496716\n",
      "Iteration 3184, loss = 0.12638754\n",
      "Iteration 3185, loss = 0.12058239\n",
      "Iteration 3186, loss = 0.12512874\n",
      "Iteration 3187, loss = 0.11619793\n",
      "Iteration 3188, loss = 0.12519172\n",
      "Iteration 3189, loss = 0.11596056\n",
      "Iteration 3190, loss = 0.11242826\n",
      "Iteration 3191, loss = 0.12098313\n",
      "Iteration 3192, loss = 0.11187282\n",
      "Iteration 3193, loss = 0.11531654\n",
      "Iteration 3194, loss = 0.14083857\n",
      "Iteration 3195, loss = 0.12366012\n",
      "Iteration 3196, loss = 0.12790940\n",
      "Iteration 3197, loss = 0.13161541\n",
      "Iteration 3198, loss = 0.12262259\n",
      "Iteration 3199, loss = 0.11604956\n",
      "Iteration 3200, loss = 0.12951971\n",
      "Iteration 3201, loss = 0.11399633\n",
      "Iteration 3202, loss = 0.12901360\n",
      "Iteration 3203, loss = 0.13089846\n",
      "Iteration 3204, loss = 0.12233672\n",
      "Iteration 3205, loss = 0.13279402\n",
      "Iteration 3206, loss = 0.11438205\n",
      "Iteration 3207, loss = 0.12443865\n",
      "Iteration 3208, loss = 0.11804604\n",
      "Iteration 3209, loss = 0.11351174\n",
      "Iteration 3210, loss = 0.14292814\n",
      "Iteration 3211, loss = 0.12309056\n",
      "Iteration 3212, loss = 0.12226111\n",
      "Iteration 3213, loss = 0.11855306\n",
      "Iteration 3214, loss = 0.11826485\n",
      "Iteration 3215, loss = 0.11925596\n",
      "Iteration 3216, loss = 0.11842676\n",
      "Iteration 3217, loss = 0.11624192\n",
      "Iteration 3218, loss = 0.12623162\n",
      "Iteration 3219, loss = 0.11526316\n",
      "Iteration 3220, loss = 0.11948144\n",
      "Iteration 3221, loss = 0.12128828\n",
      "Iteration 3222, loss = 0.12668943\n",
      "Iteration 3223, loss = 0.12899898\n",
      "Iteration 3224, loss = 0.12398588\n",
      "Iteration 3225, loss = 0.14013454\n",
      "Iteration 3226, loss = 0.11754455\n",
      "Iteration 3227, loss = 0.11816592\n",
      "Iteration 3228, loss = 0.12760709\n",
      "Iteration 3229, loss = 0.11806379\n",
      "Iteration 3230, loss = 0.12699058\n",
      "Iteration 3231, loss = 0.11312460\n",
      "Iteration 3232, loss = 0.11642720\n",
      "Iteration 3233, loss = 0.11475425\n",
      "Iteration 3234, loss = 0.11421469\n",
      "Iteration 3235, loss = 0.11345955\n",
      "Iteration 3236, loss = 0.11203900\n",
      "Iteration 3237, loss = 0.11346419\n",
      "Iteration 3238, loss = 0.11842776\n",
      "Iteration 3239, loss = 0.14033530\n",
      "Iteration 3240, loss = 0.12106320\n",
      "Iteration 3241, loss = 0.13159652\n",
      "Iteration 3242, loss = 0.11490702\n",
      "Iteration 3243, loss = 0.11870528\n",
      "Iteration 3244, loss = 0.14791789\n",
      "Iteration 3245, loss = 0.12466696\n",
      "Iteration 3246, loss = 0.12125292\n",
      "Iteration 3247, loss = 0.11572951\n",
      "Iteration 3248, loss = 0.12203317\n",
      "Iteration 3249, loss = 0.12023521\n",
      "Iteration 3250, loss = 0.14556751\n",
      "Iteration 3251, loss = 0.11785004\n",
      "Iteration 3252, loss = 0.12123305\n",
      "Iteration 3253, loss = 0.12569164\n",
      "Iteration 3254, loss = 0.11456525\n",
      "Iteration 3255, loss = 0.12278822\n",
      "Iteration 3256, loss = 0.11948391\n",
      "Iteration 3257, loss = 0.12194585\n",
      "Iteration 3258, loss = 0.13356578\n",
      "Iteration 3259, loss = 0.15068674\n",
      "Iteration 3260, loss = 0.13176480\n",
      "Iteration 3261, loss = 0.15490578\n",
      "Iteration 3262, loss = 0.19945186\n",
      "Iteration 3263, loss = 0.13763538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3264, loss = 0.13113686\n",
      "Iteration 3265, loss = 0.13837040\n",
      "Iteration 3266, loss = 0.12785537\n",
      "Iteration 3267, loss = 0.12585300\n",
      "Iteration 3268, loss = 0.12474984\n",
      "Iteration 3269, loss = 0.11655489\n",
      "Iteration 3270, loss = 0.11848297\n",
      "Iteration 3271, loss = 0.11600898\n",
      "Iteration 3272, loss = 0.13768373\n",
      "Iteration 3273, loss = 0.11652595\n",
      "Iteration 3274, loss = 0.15160364\n",
      "Iteration 3275, loss = 0.17057025\n",
      "Iteration 3276, loss = 0.27838750\n",
      "Iteration 3277, loss = 0.31465028\n",
      "Iteration 3278, loss = 0.17560795\n",
      "Iteration 3279, loss = 0.14519683\n",
      "Iteration 3280, loss = 0.16907304\n",
      "Iteration 3281, loss = 0.13285912\n",
      "Iteration 3282, loss = 0.12391427\n",
      "Iteration 3283, loss = 0.13568389\n",
      "Iteration 3284, loss = 0.12232129\n",
      "Iteration 3285, loss = 0.11940735\n",
      "Iteration 3286, loss = 0.11862844\n",
      "Iteration 3287, loss = 0.12189971\n",
      "Iteration 3288, loss = 0.11827745\n",
      "Iteration 3289, loss = 0.12675320\n",
      "Iteration 3290, loss = 0.11304827\n",
      "Iteration 3291, loss = 0.11912369\n",
      "Iteration 3292, loss = 0.12219307\n",
      "Iteration 3293, loss = 0.12581042\n",
      "Iteration 3294, loss = 0.13396267\n",
      "Iteration 3295, loss = 0.13055432\n",
      "Iteration 3296, loss = 0.14005091\n",
      "Iteration 3297, loss = 0.24780198\n",
      "Iteration 3298, loss = 0.18684179\n",
      "Iteration 3299, loss = 0.60392654\n",
      "Iteration 3300, loss = 0.61887040\n",
      "Iteration 3301, loss = 0.68696713\n",
      "Iteration 3302, loss = 0.56894431\n",
      "Iteration 3303, loss = 0.67448483\n",
      "Iteration 3304, loss = 0.54552210\n",
      "Iteration 3305, loss = 0.19844782\n",
      "Iteration 3306, loss = 0.24596710\n",
      "Iteration 3307, loss = 0.34985653\n",
      "Iteration 3308, loss = 0.38439766\n",
      "Iteration 3309, loss = 0.26779575\n",
      "Iteration 3310, loss = 0.18696118\n",
      "Iteration 3311, loss = 0.15292674\n",
      "Iteration 3312, loss = 0.16583815\n",
      "Iteration 3313, loss = 0.15184783\n",
      "Iteration 3314, loss = 0.14227114\n",
      "Iteration 3315, loss = 0.13578553\n",
      "Iteration 3316, loss = 0.14663548\n",
      "Iteration 3317, loss = 0.15851955\n",
      "Iteration 3318, loss = 0.13240981\n",
      "Iteration 3319, loss = 0.12542654\n",
      "Iteration 3320, loss = 0.12528277\n",
      "Iteration 3321, loss = 0.11897766\n",
      "Iteration 3322, loss = 0.11558776\n",
      "Iteration 3323, loss = 0.12303983\n",
      "Iteration 3324, loss = 0.12979782\n",
      "Iteration 3325, loss = 0.15077636\n",
      "Iteration 3326, loss = 0.17402349\n",
      "Iteration 3327, loss = 0.32130072\n",
      "Iteration 3328, loss = 0.37496334\n",
      "Iteration 3329, loss = 0.28276473\n",
      "Iteration 3330, loss = 0.18605047\n",
      "Iteration 3331, loss = 0.21718413\n",
      "Iteration 3332, loss = 0.26395573\n",
      "Iteration 3333, loss = 0.24959515\n",
      "Iteration 3334, loss = 0.28701040\n",
      "Iteration 3335, loss = 0.26916894\n",
      "Iteration 3336, loss = 0.23578384\n",
      "Iteration 3337, loss = 0.15086760\n",
      "Iteration 3338, loss = 0.17996495\n",
      "Iteration 3339, loss = 0.16208357\n",
      "Iteration 3340, loss = 0.13806670\n",
      "Iteration 3341, loss = 0.14849196\n",
      "Iteration 3342, loss = 0.12402715\n",
      "Iteration 3343, loss = 0.12138987\n",
      "Iteration 3344, loss = 0.12042693\n",
      "Iteration 3345, loss = 0.13121280\n",
      "Iteration 3346, loss = 0.16074606\n",
      "Iteration 3347, loss = 0.11303550\n",
      "Iteration 3348, loss = 0.13552031\n",
      "Iteration 3349, loss = 0.13327287\n",
      "Iteration 3350, loss = 0.12708803\n",
      "Iteration 3351, loss = 0.13639239\n",
      "Iteration 3352, loss = 0.12318136\n",
      "Iteration 3353, loss = 0.14046680\n",
      "Iteration 3354, loss = 0.13285863\n",
      "Iteration 3355, loss = 0.13853139\n",
      "Iteration 3356, loss = 0.12679077\n",
      "Iteration 3357, loss = 0.11917312\n",
      "Iteration 3358, loss = 0.16554706\n",
      "Iteration 3359, loss = 0.14789769\n",
      "Iteration 3360, loss = 0.15269662\n",
      "Iteration 3361, loss = 0.13791898\n",
      "Iteration 3362, loss = 0.13380008\n",
      "Iteration 3363, loss = 0.13795974\n",
      "Iteration 3364, loss = 0.16857592\n",
      "Iteration 3365, loss = 0.14141803\n",
      "Iteration 3366, loss = 0.11303191\n",
      "Iteration 3367, loss = 0.12765609\n",
      "Iteration 3368, loss = 0.14067204\n",
      "Iteration 3369, loss = 0.14045898\n",
      "Iteration 3370, loss = 0.17666192\n",
      "Iteration 3371, loss = 0.20821258\n",
      "Iteration 3372, loss = 0.13169609\n",
      "Iteration 3373, loss = 0.12549707\n",
      "Iteration 3374, loss = 0.14159210\n",
      "Iteration 3375, loss = 0.12589603\n",
      "Iteration 3376, loss = 0.12655602\n",
      "Iteration 3377, loss = 0.14787895\n",
      "Iteration 3378, loss = 0.13297610\n",
      "Iteration 3379, loss = 0.12411829\n",
      "Iteration 3380, loss = 0.11946509\n",
      "Iteration 3381, loss = 0.12258131\n",
      "Iteration 3382, loss = 0.11626668\n",
      "Iteration 3383, loss = 0.11998218\n",
      "Iteration 3384, loss = 0.11685995\n",
      "Iteration 3385, loss = 0.11416572\n",
      "Iteration 3386, loss = 0.12045014\n",
      "Iteration 3387, loss = 0.15525007\n",
      "Iteration 3388, loss = 0.11936368\n",
      "Iteration 3389, loss = 0.12812447\n",
      "Iteration 3390, loss = 0.12932099\n",
      "Iteration 3391, loss = 0.11328760\n",
      "Iteration 3392, loss = 0.11567403\n",
      "Iteration 3393, loss = 0.11623594\n",
      "Iteration 3394, loss = 0.12981909\n",
      "Iteration 3395, loss = 0.13891148\n",
      "Iteration 3396, loss = 0.12288100\n",
      "Iteration 3397, loss = 0.12384982\n",
      "Iteration 3398, loss = 0.12168278\n",
      "Iteration 3399, loss = 0.13304665\n",
      "Iteration 3400, loss = 0.13946962\n",
      "Iteration 3401, loss = 0.16083908\n",
      "Iteration 3402, loss = 0.12314067\n",
      "Iteration 3403, loss = 0.11675705\n",
      "Iteration 3404, loss = 0.11929875\n",
      "Iteration 3405, loss = 0.11669846\n",
      "Iteration 3406, loss = 0.12270459\n",
      "Iteration 3407, loss = 0.14819932\n",
      "Iteration 3408, loss = 0.13157731\n",
      "Iteration 3409, loss = 0.11966204\n",
      "Iteration 3410, loss = 0.12649261\n",
      "Iteration 3411, loss = 0.11617419\n",
      "Iteration 3412, loss = 0.15986600\n",
      "Iteration 3413, loss = 0.12952823\n",
      "Iteration 3414, loss = 0.14350877\n",
      "Iteration 3415, loss = 0.12592331\n",
      "Iteration 3416, loss = 0.12931630\n",
      "Iteration 3417, loss = 0.14982727\n",
      "Iteration 3418, loss = 0.12632055\n",
      "Iteration 3419, loss = 0.11890251\n",
      "Iteration 3420, loss = 0.15542330\n",
      "Iteration 3421, loss = 0.14798183\n",
      "Iteration 3422, loss = 0.14525178\n",
      "Iteration 3423, loss = 0.13314688\n",
      "Iteration 3424, loss = 0.13458378\n",
      "Iteration 3425, loss = 0.12147596\n",
      "Iteration 3426, loss = 0.11908387\n",
      "Iteration 3427, loss = 0.11961411\n",
      "Iteration 3428, loss = 0.13837095\n",
      "Iteration 3429, loss = 0.12878399\n",
      "Iteration 3430, loss = 0.12513693\n",
      "Iteration 3431, loss = 0.12634984\n",
      "Iteration 3432, loss = 0.12672179\n",
      "Iteration 3433, loss = 0.21049768\n",
      "Iteration 3434, loss = 0.41213843\n",
      "Iteration 3435, loss = 0.42262468\n",
      "Iteration 3436, loss = 0.50468855\n",
      "Iteration 3437, loss = 0.28314890\n",
      "Iteration 3438, loss = 0.23531498\n",
      "Iteration 3439, loss = 0.19308570\n",
      "Iteration 3440, loss = 0.13701288\n",
      "Iteration 3441, loss = 0.18940276\n",
      "Iteration 3442, loss = 0.13916263\n",
      "Iteration 3443, loss = 0.12384660\n",
      "Iteration 3444, loss = 0.12687795\n",
      "Iteration 3445, loss = 0.12820139\n",
      "Iteration 3446, loss = 0.13253619\n",
      "Iteration 3447, loss = 0.13213219\n",
      "Iteration 3448, loss = 0.12436410\n",
      "Iteration 3449, loss = 0.11461799\n",
      "Iteration 3450, loss = 0.14874878\n",
      "Iteration 3451, loss = 0.13214495\n",
      "Iteration 3452, loss = 0.12143450\n",
      "Iteration 3453, loss = 0.12021742\n",
      "Iteration 3454, loss = 0.11517731\n",
      "Iteration 3455, loss = 0.12722297\n",
      "Iteration 3456, loss = 0.11992044\n",
      "Iteration 3457, loss = 0.13402743\n",
      "Iteration 3458, loss = 0.11873511\n",
      "Iteration 3459, loss = 0.12571367\n",
      "Iteration 3460, loss = 0.17039323\n",
      "Iteration 3461, loss = 0.15974866\n",
      "Iteration 3462, loss = 0.14512679\n",
      "Iteration 3463, loss = 0.15655576\n",
      "Iteration 3464, loss = 0.12538163\n",
      "Iteration 3465, loss = 0.14689771\n",
      "Iteration 3466, loss = 0.14626748\n",
      "Iteration 3467, loss = 0.18862229\n",
      "Iteration 3468, loss = 0.16437404\n",
      "Iteration 3469, loss = 0.13177869\n",
      "Iteration 3470, loss = 0.11827253\n",
      "Iteration 3471, loss = 0.15479770\n",
      "Iteration 3472, loss = 0.20429025\n",
      "Iteration 3473, loss = 0.13081829\n",
      "Iteration 3474, loss = 0.13564316\n",
      "Iteration 3475, loss = 0.13434333\n",
      "Iteration 3476, loss = 0.14499933\n",
      "Iteration 3477, loss = 0.18972526\n",
      "Iteration 3478, loss = 0.14879207\n",
      "Iteration 3479, loss = 0.12206694\n",
      "Iteration 3480, loss = 0.13378047\n",
      "Iteration 3481, loss = 0.12072493\n",
      "Iteration 3482, loss = 0.11563208\n",
      "Iteration 3483, loss = 0.11389964\n",
      "Iteration 3484, loss = 0.11403120\n",
      "Iteration 3485, loss = 0.12184180\n",
      "Iteration 3486, loss = 0.11965634\n",
      "Iteration 3487, loss = 0.13056290\n",
      "Iteration 3488, loss = 0.11772413\n",
      "Iteration 3489, loss = 0.11360431\n",
      "Iteration 3490, loss = 0.11184075\n",
      "Iteration 3491, loss = 0.11120458\n",
      "Iteration 3492, loss = 0.13437234\n",
      "Iteration 3493, loss = 0.13338400\n",
      "Iteration 3494, loss = 0.13865482\n",
      "Iteration 3495, loss = 0.14322403\n",
      "Iteration 3496, loss = 0.18429163\n",
      "Iteration 3497, loss = 0.14586203\n",
      "Iteration 3498, loss = 0.13904562\n",
      "Iteration 3499, loss = 0.17236378\n",
      "Iteration 3500, loss = 0.12889121\n",
      "Iteration 3501, loss = 0.15820281\n",
      "Iteration 3502, loss = 0.20008546\n",
      "Iteration 3503, loss = 0.27773628\n",
      "Iteration 3504, loss = 0.19921054\n",
      "Iteration 3505, loss = 0.38833746\n",
      "Iteration 3506, loss = 0.21720468\n",
      "Iteration 3507, loss = 0.42310168\n",
      "Iteration 3508, loss = 0.49517442\n",
      "Iteration 3509, loss = 0.81987216\n",
      "Iteration 3510, loss = 0.56816273\n",
      "Iteration 3511, loss = 0.82570635\n",
      "Iteration 3512, loss = 0.67341796\n",
      "Iteration 3513, loss = 0.41217940\n",
      "Iteration 3514, loss = 0.40930326\n",
      "Iteration 3515, loss = 0.64681684\n",
      "Iteration 3516, loss = 0.45557924\n",
      "Iteration 3517, loss = 0.23625613\n",
      "Iteration 3518, loss = 0.36630826\n",
      "Iteration 3519, loss = 0.48588844\n",
      "Iteration 3520, loss = 0.56186426\n",
      "Iteration 3521, loss = 0.59243595\n",
      "Iteration 3522, loss = 0.43176575\n",
      "Iteration 3523, loss = 0.33619862\n",
      "Iteration 3524, loss = 0.36644461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3525, loss = 0.44862887\n",
      "Iteration 3526, loss = 0.33295435\n",
      "Iteration 3527, loss = 0.37874574\n",
      "Iteration 3528, loss = 0.49666046\n",
      "Iteration 3529, loss = 0.82269955\n",
      "Iteration 3530, loss = 0.45092209\n",
      "Iteration 3531, loss = 0.41565227\n",
      "Iteration 3532, loss = 0.27720493\n",
      "Iteration 3533, loss = 0.20466652\n",
      "Iteration 3534, loss = 0.22780699\n",
      "Iteration 3535, loss = 0.16589616\n",
      "Iteration 3536, loss = 0.22535416\n",
      "Iteration 3537, loss = 0.18208087\n",
      "Iteration 3538, loss = 0.20673632\n",
      "Iteration 3539, loss = 0.19272320\n",
      "Iteration 3540, loss = 0.30030996\n",
      "Iteration 3541, loss = 0.25079741\n",
      "Iteration 3542, loss = 0.15851227\n",
      "Iteration 3543, loss = 0.20100159\n",
      "Iteration 3544, loss = 0.19635157\n",
      "Iteration 3545, loss = 0.15502486\n",
      "Iteration 3546, loss = 0.12791470\n",
      "Iteration 3547, loss = 0.13488985\n",
      "Iteration 3548, loss = 0.12711960\n",
      "Iteration 3549, loss = 0.14053224\n",
      "Iteration 3550, loss = 0.12536285\n",
      "Iteration 3551, loss = 0.13888333\n",
      "Iteration 3552, loss = 0.12635814\n",
      "Iteration 3553, loss = 0.12064496\n",
      "Iteration 3554, loss = 0.11708833\n",
      "Iteration 3555, loss = 0.11329029\n",
      "Iteration 3556, loss = 0.12157142\n",
      "Iteration 3557, loss = 0.11733608\n",
      "Iteration 3558, loss = 0.14461835\n",
      "Iteration 3559, loss = 0.15771725\n",
      "Iteration 3560, loss = 0.12537014\n",
      "Iteration 3561, loss = 0.12757763\n",
      "Iteration 3562, loss = 0.12884128\n",
      "Iteration 3563, loss = 0.12916937\n",
      "Iteration 3564, loss = 0.12963291\n",
      "Iteration 3565, loss = 0.12917590\n",
      "Iteration 3566, loss = 0.15142460\n",
      "Iteration 3567, loss = 0.13656172\n",
      "Iteration 3568, loss = 0.11499910\n",
      "Iteration 3569, loss = 0.17574556\n",
      "Iteration 3570, loss = 0.15795399\n",
      "Iteration 3571, loss = 0.34259776\n",
      "Iteration 3572, loss = 0.30359877\n",
      "Iteration 3573, loss = 0.22307460\n",
      "Iteration 3574, loss = 0.32799302\n",
      "Iteration 3575, loss = 0.30384654\n",
      "Iteration 3576, loss = 0.36744941\n",
      "Iteration 3577, loss = 0.32130093\n",
      "Iteration 3578, loss = 2.25557402\n",
      "Iteration 3579, loss = 0.91857124\n",
      "Iteration 3580, loss = 1.52331720\n",
      "Iteration 3581, loss = 1.14304542\n",
      "Iteration 3582, loss = 1.01703214\n",
      "Iteration 3583, loss = 0.87124394\n",
      "Iteration 3584, loss = 0.74292176\n",
      "Iteration 3585, loss = 0.77697657\n",
      "Iteration 3586, loss = 0.81894628\n",
      "Iteration 3587, loss = 1.06815827\n",
      "Iteration 3588, loss = 0.93388595\n",
      "Iteration 3589, loss = 0.65295633\n",
      "Iteration 3590, loss = 0.58972218\n",
      "Iteration 3591, loss = 0.74207553\n",
      "Iteration 3592, loss = 0.68643494\n",
      "Iteration 3593, loss = 0.38741362\n",
      "Iteration 3594, loss = 0.99538449\n",
      "Iteration 3595, loss = 0.86998665\n",
      "Iteration 3596, loss = 0.71875011\n",
      "Iteration 3597, loss = 0.50536068\n",
      "Iteration 3598, loss = 0.44787300\n",
      "Iteration 3599, loss = 0.46855672\n",
      "Iteration 3600, loss = 0.22833724\n",
      "Iteration 3601, loss = 0.68734280\n",
      "Iteration 3602, loss = 0.46685038\n",
      "Iteration 3603, loss = 0.35514416\n",
      "Iteration 3604, loss = 0.27237634\n",
      "Iteration 3605, loss = 0.28717016\n",
      "Iteration 3606, loss = 0.23283701\n",
      "Iteration 3607, loss = 0.40449784\n",
      "Iteration 3608, loss = 0.27529474\n",
      "Iteration 3609, loss = 0.33714352\n",
      "Iteration 3610, loss = 0.25206811\n",
      "Iteration 3611, loss = 0.17434120\n",
      "Iteration 3612, loss = 0.15700042\n",
      "Iteration 3613, loss = 0.14656757\n",
      "Iteration 3614, loss = 0.18080217\n",
      "Iteration 3615, loss = 0.17630761\n",
      "Iteration 3616, loss = 0.14838455\n",
      "Iteration 3617, loss = 0.19117643\n",
      "Iteration 3618, loss = 0.18151974\n",
      "Iteration 3619, loss = 0.15447972\n",
      "Iteration 3620, loss = 0.15062911\n",
      "Iteration 3621, loss = 0.14316988\n",
      "Iteration 3622, loss = 0.12972788\n",
      "Iteration 3623, loss = 0.13448144\n",
      "Iteration 3624, loss = 0.12120230\n",
      "Iteration 3625, loss = 0.11837576\n",
      "Iteration 3626, loss = 0.11517157\n",
      "Iteration 3627, loss = 0.15196225\n",
      "Iteration 3628, loss = 0.13144572\n",
      "Iteration 3629, loss = 0.13925007\n",
      "Iteration 3630, loss = 0.11904805\n",
      "Iteration 3631, loss = 0.14288297\n",
      "Iteration 3632, loss = 0.11912183\n",
      "Iteration 3633, loss = 0.12185994\n",
      "Iteration 3634, loss = 0.12708801\n",
      "Iteration 3635, loss = 0.12378686\n",
      "Iteration 3636, loss = 0.12840761\n",
      "Iteration 3637, loss = 0.13285386\n",
      "Iteration 3638, loss = 0.12300579\n",
      "Iteration 3639, loss = 0.13502718\n",
      "Iteration 3640, loss = 0.13358104\n",
      "Iteration 3641, loss = 0.12022527\n",
      "Iteration 3642, loss = 0.12122668\n",
      "Iteration 3643, loss = 0.12147807\n",
      "Iteration 3644, loss = 0.12024066\n",
      "Iteration 3645, loss = 0.11446097\n",
      "Iteration 3646, loss = 0.12104036\n",
      "Iteration 3647, loss = 0.13457523\n",
      "Iteration 3648, loss = 0.12311310\n",
      "Iteration 3649, loss = 0.12741775\n",
      "Iteration 3650, loss = 0.12451296\n",
      "Iteration 3651, loss = 0.12579836\n",
      "Iteration 3652, loss = 0.12251974\n",
      "Iteration 3653, loss = 0.12304439\n",
      "Iteration 3654, loss = 0.12355930\n",
      "Iteration 3655, loss = 0.12456293\n",
      "Iteration 3656, loss = 0.12150967\n",
      "Iteration 3657, loss = 0.11947307\n",
      "Iteration 3658, loss = 0.11910633\n",
      "Iteration 3659, loss = 0.12295042\n",
      "Iteration 3660, loss = 0.11403024\n",
      "Iteration 3661, loss = 0.11257192\n",
      "Iteration 3662, loss = 0.11917824\n",
      "Iteration 3663, loss = 0.12502523\n",
      "Iteration 3664, loss = 0.12482112\n",
      "Iteration 3665, loss = 0.11213024\n",
      "Iteration 3666, loss = 0.14539303\n",
      "Iteration 3667, loss = 0.12797377\n",
      "Iteration 3668, loss = 0.12091114\n",
      "Iteration 3669, loss = 0.16799902\n",
      "Iteration 3670, loss = 0.14891007\n",
      "Iteration 3671, loss = 0.15366229\n",
      "Iteration 3672, loss = 0.16511345\n",
      "Iteration 3673, loss = 0.14012403\n",
      "Iteration 3674, loss = 0.21354351\n",
      "Iteration 3675, loss = 0.29527902\n",
      "Iteration 3676, loss = 0.33109485\n",
      "Iteration 3677, loss = 0.19320035\n",
      "Iteration 3678, loss = 0.16805973\n",
      "Iteration 3679, loss = 0.19525313\n",
      "Iteration 3680, loss = 0.12759640\n",
      "Iteration 3681, loss = 0.12964606\n",
      "Iteration 3682, loss = 0.14013584\n",
      "Iteration 3683, loss = 0.12476706\n",
      "Iteration 3684, loss = 0.11699591\n",
      "Iteration 3685, loss = 0.11996342\n",
      "Iteration 3686, loss = 0.12240579\n",
      "Iteration 3687, loss = 0.11746294\n",
      "Iteration 3688, loss = 0.12193311\n",
      "Iteration 3689, loss = 0.13056397\n",
      "Iteration 3690, loss = 0.12509931\n",
      "Iteration 3691, loss = 0.11890381\n",
      "Iteration 3692, loss = 0.12039467\n",
      "Iteration 3693, loss = 0.12177923\n",
      "Iteration 3694, loss = 0.11414019\n",
      "Iteration 3695, loss = 0.12473542\n",
      "Iteration 3696, loss = 0.12841738\n",
      "Iteration 3697, loss = 0.11407620\n",
      "Iteration 3698, loss = 0.12285911\n",
      "Iteration 3699, loss = 0.11259035\n",
      "Iteration 3700, loss = 0.11787577\n",
      "Iteration 3701, loss = 0.11334774\n",
      "Iteration 3702, loss = 0.12001243\n",
      "Iteration 3703, loss = 0.12358623\n",
      "Iteration 3704, loss = 0.12129384\n",
      "Iteration 3705, loss = 0.11695862\n",
      "Iteration 3706, loss = 0.11614734\n",
      "Iteration 3707, loss = 0.11681155\n",
      "Iteration 3708, loss = 0.12010428\n",
      "Iteration 3709, loss = 0.11685994\n",
      "Iteration 3710, loss = 0.12138587\n",
      "Iteration 3711, loss = 0.11706122\n",
      "Iteration 3712, loss = 0.13211036\n",
      "Iteration 3713, loss = 0.11479966\n",
      "Iteration 3714, loss = 0.12012849\n",
      "Iteration 3715, loss = 0.11112676\n",
      "Iteration 3716, loss = 0.11669635\n",
      "Iteration 3717, loss = 0.11518263\n",
      "Iteration 3718, loss = 0.11315570\n",
      "Iteration 3719, loss = 0.11337694\n",
      "Iteration 3720, loss = 0.11800888\n",
      "Iteration 3721, loss = 0.11493436\n",
      "Iteration 3722, loss = 0.11694073\n",
      "Iteration 3723, loss = 0.16074607\n",
      "Iteration 3724, loss = 0.27899960\n",
      "Iteration 3725, loss = 0.29538003\n",
      "Iteration 3726, loss = 0.18746007\n",
      "Iteration 3727, loss = 0.13398801\n",
      "Iteration 3728, loss = 0.31715164\n",
      "Iteration 3729, loss = 0.21090012\n",
      "Iteration 3730, loss = 0.14106271\n",
      "Iteration 3731, loss = 0.17269740\n",
      "Iteration 3732, loss = 0.17133213\n",
      "Iteration 3733, loss = 0.16032894\n",
      "Iteration 3734, loss = 0.44041711\n",
      "Iteration 3735, loss = 1.52627084\n",
      "Iteration 3736, loss = 3.00040380\n",
      "Iteration 3737, loss = 3.48862744\n",
      "Iteration 3738, loss = 2.33706005\n",
      "Iteration 3739, loss = 1.04440878\n",
      "Iteration 3740, loss = 1.04520159\n",
      "Iteration 3741, loss = 0.99326808\n",
      "Iteration 3742, loss = 1.23063582\n",
      "Iteration 3743, loss = 0.77007768\n",
      "Iteration 3744, loss = 1.28006151\n",
      "Iteration 3745, loss = 1.32519149\n",
      "Iteration 3746, loss = 1.28984385\n",
      "Iteration 3747, loss = 0.82133398\n",
      "Iteration 3748, loss = 1.25407916\n",
      "Iteration 3749, loss = 0.81717530\n",
      "Iteration 3750, loss = 0.83044919\n",
      "Iteration 3751, loss = 1.11422250\n",
      "Iteration 3752, loss = 0.74644088\n",
      "Iteration 3753, loss = 0.86702823\n",
      "Iteration 3754, loss = 0.87952356\n",
      "Iteration 3755, loss = 0.71337733\n",
      "Iteration 3756, loss = 1.04401192\n",
      "Iteration 3757, loss = 1.09991359\n",
      "Iteration 3758, loss = 0.64877515\n",
      "Iteration 3759, loss = 0.62287106\n",
      "Iteration 3760, loss = 0.58362270\n",
      "Iteration 3761, loss = 0.45901194\n",
      "Iteration 3762, loss = 0.33833726\n",
      "Iteration 3763, loss = 0.56614838\n",
      "Iteration 3764, loss = 0.29635114\n",
      "Iteration 3765, loss = 0.68655031\n",
      "Iteration 3766, loss = 0.76056839\n",
      "Iteration 3767, loss = 0.59499339\n",
      "Iteration 3768, loss = 0.85677212\n",
      "Iteration 3769, loss = 0.57921729\n",
      "Iteration 3770, loss = 0.78318424\n",
      "Iteration 3771, loss = 0.58972947\n",
      "Iteration 3772, loss = 0.61797341\n",
      "Iteration 3773, loss = 0.28700570\n",
      "Iteration 3774, loss = 0.30620883\n",
      "Iteration 3775, loss = 0.31202410\n",
      "Iteration 3776, loss = 0.26163064\n",
      "Iteration 3777, loss = 0.20409724\n",
      "Iteration 3778, loss = 0.25868226\n",
      "Iteration 3779, loss = 0.19543964\n",
      "Iteration 3780, loss = 0.21414219\n",
      "Iteration 3781, loss = 0.17672179\n",
      "Iteration 3782, loss = 0.16908535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3783, loss = 0.21928033\n",
      "Iteration 3784, loss = 0.22734241\n",
      "Iteration 3785, loss = 0.18839513\n",
      "Iteration 3786, loss = 0.15312448\n",
      "Iteration 3787, loss = 0.15379091\n",
      "Iteration 3788, loss = 0.13247281\n",
      "Iteration 3789, loss = 0.14230003\n",
      "Iteration 3790, loss = 0.13576458\n",
      "Iteration 3791, loss = 0.12777326\n",
      "Iteration 3792, loss = 0.13772059\n",
      "Iteration 3793, loss = 0.14251986\n",
      "Iteration 3794, loss = 0.22014263\n",
      "Iteration 3795, loss = 0.29157232\n",
      "Iteration 3796, loss = 0.21619681\n",
      "Iteration 3797, loss = 0.17470707\n",
      "Iteration 3798, loss = 0.16139626\n",
      "Iteration 3799, loss = 0.21535383\n",
      "Iteration 3800, loss = 0.20997692\n",
      "Iteration 3801, loss = 0.16478784\n",
      "Iteration 3802, loss = 0.16443244\n",
      "Iteration 3803, loss = 0.14172482\n",
      "Iteration 3804, loss = 0.15863405\n",
      "Iteration 3805, loss = 0.13028395\n",
      "Iteration 3806, loss = 0.12001303\n",
      "Iteration 3807, loss = 0.13222449\n",
      "Iteration 3808, loss = 0.12828452\n",
      "Iteration 3809, loss = 0.12896932\n",
      "Iteration 3810, loss = 0.13228366\n",
      "Iteration 3811, loss = 0.11559351\n",
      "Iteration 3812, loss = 0.12032018\n",
      "Iteration 3813, loss = 0.15152056\n",
      "Iteration 3814, loss = 0.12706457\n",
      "Iteration 3815, loss = 0.14954656\n",
      "Iteration 3816, loss = 0.13366290\n",
      "Iteration 3817, loss = 0.16987130\n",
      "Iteration 3818, loss = 0.12989917\n",
      "Iteration 3819, loss = 0.19443473\n",
      "Iteration 3820, loss = 0.14430794\n",
      "Iteration 3821, loss = 0.12504613\n",
      "Iteration 3822, loss = 0.12707932\n",
      "Iteration 3823, loss = 0.12007332\n",
      "Iteration 3824, loss = 0.11976703\n",
      "Iteration 3825, loss = 0.12665774\n",
      "Iteration 3826, loss = 0.11392257\n",
      "Iteration 3827, loss = 0.14908490\n",
      "Iteration 3828, loss = 0.15248780\n",
      "Iteration 3829, loss = 0.16865737\n",
      "Iteration 3830, loss = 0.14722780\n",
      "Iteration 3831, loss = 0.14754848\n",
      "Iteration 3832, loss = 0.17844477\n",
      "Iteration 3833, loss = 0.14707201\n",
      "Iteration 3834, loss = 0.17584917\n",
      "Iteration 3835, loss = 0.16550591\n",
      "Iteration 3836, loss = 0.12970185\n",
      "Iteration 3837, loss = 0.16139588\n",
      "Iteration 3838, loss = 0.11763075\n",
      "Iteration 3839, loss = 0.13087872\n",
      "Iteration 3840, loss = 0.12742140\n",
      "Iteration 3841, loss = 0.11618813\n",
      "Iteration 3842, loss = 0.12135042\n",
      "Iteration 3843, loss = 0.11412996\n",
      "Iteration 3844, loss = 0.11151655\n",
      "Iteration 3845, loss = 0.11705569\n",
      "Iteration 3846, loss = 0.11806317\n",
      "Iteration 3847, loss = 0.12523406\n",
      "Iteration 3848, loss = 0.13521944\n",
      "Iteration 3849, loss = 0.14379102\n",
      "Iteration 3850, loss = 0.12201338\n",
      "Iteration 3851, loss = 0.12794039\n",
      "Iteration 3852, loss = 0.13027679\n",
      "Iteration 3853, loss = 0.12635623\n",
      "Iteration 3854, loss = 0.12567616\n",
      "Iteration 3855, loss = 0.12348239\n",
      "Iteration 3856, loss = 0.15143505\n",
      "Iteration 3857, loss = 0.13312323\n",
      "Iteration 3858, loss = 0.12049469\n",
      "Iteration 3859, loss = 0.11802389\n",
      "Iteration 3860, loss = 0.12099973\n",
      "Iteration 3861, loss = 0.13173891\n",
      "Iteration 3862, loss = 0.13456175\n",
      "Iteration 3863, loss = 0.14572945\n",
      "Iteration 3864, loss = 0.15010790\n",
      "Iteration 3865, loss = 0.18349604\n",
      "Iteration 3866, loss = 0.15974188\n",
      "Iteration 3867, loss = 0.13066399\n",
      "Iteration 3868, loss = 0.14145498\n",
      "Iteration 3869, loss = 0.11865023\n",
      "Iteration 3870, loss = 0.11543146\n",
      "Iteration 3871, loss = 0.12674481\n",
      "Iteration 3872, loss = 0.12380417\n",
      "Iteration 3873, loss = 0.11685282\n",
      "Iteration 3874, loss = 0.11810369\n",
      "Iteration 3875, loss = 0.11395838\n",
      "Iteration 3876, loss = 0.11215641\n",
      "Iteration 3877, loss = 0.12011502\n",
      "Iteration 3878, loss = 0.12172463\n",
      "Iteration 3879, loss = 0.12421198\n",
      "Iteration 3880, loss = 0.24681348\n",
      "Iteration 3881, loss = 0.43537552\n",
      "Iteration 3882, loss = 0.37790437\n",
      "Iteration 3883, loss = 0.30449667\n",
      "Iteration 3884, loss = 0.26298656\n",
      "Iteration 3885, loss = 0.20422205\n",
      "Iteration 3886, loss = 0.14695408\n",
      "Iteration 3887, loss = 0.14458111\n",
      "Iteration 3888, loss = 0.16965233\n",
      "Iteration 3889, loss = 0.19875838\n",
      "Iteration 3890, loss = 0.13556819\n",
      "Iteration 3891, loss = 0.12392129\n",
      "Iteration 3892, loss = 0.12548167\n",
      "Iteration 3893, loss = 0.11906207\n",
      "Iteration 3894, loss = 0.13442916\n",
      "Iteration 3895, loss = 0.12889962\n",
      "Iteration 3896, loss = 0.17863694\n",
      "Iteration 3897, loss = 0.28058214\n",
      "Iteration 3898, loss = 0.31833265\n",
      "Iteration 3899, loss = 0.17601851\n",
      "Iteration 3900, loss = 0.14286945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.1, max_iter=3900, momentum=0.9,\n",
       "       n_iter_no_change=10000, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=1e-08,\n",
       "       validation_fraction=0.99, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.971429\n"
     ]
    }
   ],
   "source": [
    "print(\"CV Accuracy: %f\" %  mlp.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
