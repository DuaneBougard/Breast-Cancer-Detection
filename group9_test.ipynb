{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.24573280\n",
      "Iteration 2, loss = 1.17628814\n",
      "Iteration 3, loss = 0.68447037\n",
      "Iteration 4, loss = 0.44687444\n",
      "Iteration 5, loss = 0.32938646\n",
      "Iteration 6, loss = 0.34642078\n",
      "Iteration 7, loss = 0.31202511\n",
      "Iteration 8, loss = 0.41495194\n",
      "Iteration 9, loss = 0.31255905\n",
      "Iteration 10, loss = 0.23194531\n",
      "Iteration 11, loss = 0.21524622\n",
      "Iteration 12, loss = 0.20164670\n",
      "Iteration 13, loss = 0.15945045\n",
      "Iteration 14, loss = 0.15166244\n",
      "Iteration 15, loss = 0.13115122\n",
      "Iteration 16, loss = 0.12676636\n",
      "Iteration 17, loss = 0.12189675\n",
      "Iteration 18, loss = 0.11865841\n",
      "Iteration 19, loss = 0.15155359\n",
      "Iteration 20, loss = 0.17952011\n",
      "Iteration 21, loss = 0.12773925\n",
      "Iteration 22, loss = 0.13508613\n",
      "Iteration 23, loss = 0.16417387\n",
      "Iteration 24, loss = 0.14007850\n",
      "Iteration 25, loss = 0.12619031\n",
      "Iteration 26, loss = 0.12161150\n",
      "Iteration 27, loss = 0.11533408\n",
      "Iteration 28, loss = 0.11754138\n",
      "Iteration 29, loss = 0.11609097\n",
      "Iteration 30, loss = 0.14620210\n",
      "Iteration 31, loss = 0.18651978\n",
      "Iteration 32, loss = 0.18364480\n",
      "Iteration 33, loss = 0.20948808\n",
      "Iteration 34, loss = 0.18721380\n",
      "Iteration 35, loss = 0.13869818\n",
      "Iteration 36, loss = 0.11855858\n",
      "Iteration 37, loss = 0.14254806\n",
      "Iteration 38, loss = 0.17333081\n",
      "Iteration 39, loss = 0.13795347\n",
      "Iteration 40, loss = 0.12951481\n",
      "Iteration 41, loss = 0.13836602\n",
      "Iteration 42, loss = 0.11896473\n",
      "Iteration 43, loss = 0.13818013\n",
      "Iteration 44, loss = 0.11936252\n",
      "Iteration 45, loss = 0.12031754\n",
      "Iteration 46, loss = 0.12105565\n",
      "Iteration 47, loss = 0.12897787\n",
      "Iteration 48, loss = 0.14862685\n",
      "Iteration 49, loss = 0.12727345\n",
      "Iteration 50, loss = 0.13375022\n",
      "Iteration 51, loss = 0.12577554\n",
      "Iteration 52, loss = 0.11441455\n",
      "Iteration 53, loss = 0.12106255\n",
      "Iteration 54, loss = 0.13641856\n",
      "Iteration 55, loss = 0.19058366\n",
      "Iteration 56, loss = 0.13613005\n",
      "Iteration 57, loss = 0.11480077\n",
      "Iteration 58, loss = 0.12427324\n",
      "Iteration 59, loss = 0.11539432\n",
      "Iteration 60, loss = 0.11537537\n",
      "Iteration 61, loss = 0.11790817\n",
      "Iteration 62, loss = 0.15417825\n",
      "Iteration 63, loss = 0.17893238\n",
      "Iteration 64, loss = 0.22085254\n",
      "Iteration 65, loss = 0.32408670\n",
      "Iteration 66, loss = 0.13957904\n",
      "Iteration 67, loss = 0.34741795\n",
      "Iteration 68, loss = 0.20784504\n",
      "Iteration 69, loss = 0.23667382\n",
      "Iteration 70, loss = 0.15305637\n",
      "Iteration 71, loss = 0.18392857\n",
      "Iteration 72, loss = 0.16516556\n",
      "Iteration 73, loss = 0.15501883\n",
      "Iteration 74, loss = 0.16199071\n",
      "Iteration 75, loss = 0.14371862\n",
      "Iteration 76, loss = 0.15730419\n",
      "Iteration 77, loss = 0.17238057\n",
      "Iteration 78, loss = 0.13224982\n",
      "Iteration 79, loss = 0.12386704\n",
      "Iteration 80, loss = 0.11077931\n",
      "Iteration 81, loss = 0.13374033\n",
      "Iteration 82, loss = 0.11878400\n",
      "Iteration 83, loss = 0.13737188\n",
      "Iteration 84, loss = 0.11958567\n",
      "Iteration 85, loss = 0.12143186\n",
      "Iteration 86, loss = 0.11368527\n",
      "Iteration 87, loss = 0.11898657\n",
      "Iteration 88, loss = 0.11854943\n",
      "Iteration 89, loss = 0.11484328\n",
      "Iteration 90, loss = 0.12394937\n",
      "Iteration 91, loss = 0.20359583\n",
      "Iteration 92, loss = 0.18440448\n",
      "Iteration 93, loss = 0.27403499\n",
      "Iteration 94, loss = 0.29301768\n",
      "Iteration 95, loss = 0.56338789\n",
      "Iteration 96, loss = 0.43157975\n",
      "Iteration 97, loss = 0.37924384\n",
      "Iteration 98, loss = 0.19612099\n",
      "Iteration 99, loss = 0.18180240\n",
      "Iteration 100, loss = 0.13003698\n",
      "Iteration 101, loss = 0.13549806\n",
      "Iteration 102, loss = 0.13653737\n",
      "Iteration 103, loss = 0.12505562\n",
      "Iteration 104, loss = 0.15016152\n",
      "Iteration 105, loss = 0.11849935\n",
      "Iteration 106, loss = 0.12352755\n",
      "Iteration 107, loss = 0.11997447\n",
      "Iteration 108, loss = 0.12142723\n",
      "Iteration 109, loss = 0.11634371\n",
      "Iteration 110, loss = 0.11966112\n",
      "Iteration 111, loss = 0.11923221\n",
      "Iteration 112, loss = 0.11752644\n",
      "Iteration 113, loss = 0.11835032\n",
      "Iteration 114, loss = 0.15548097\n",
      "Iteration 115, loss = 0.12968506\n",
      "Iteration 116, loss = 0.13333218\n",
      "Iteration 117, loss = 0.13585123\n",
      "Iteration 118, loss = 0.13250287\n",
      "Iteration 119, loss = 0.15177896\n",
      "Iteration 120, loss = 0.12643436\n",
      "Iteration 121, loss = 0.12830045\n",
      "Iteration 122, loss = 0.12118681\n",
      "Iteration 123, loss = 0.13546783\n",
      "Iteration 124, loss = 0.14152558\n",
      "Iteration 125, loss = 0.37641649\n",
      "Iteration 126, loss = 0.21569872\n",
      "Iteration 127, loss = 4.38081007\n",
      "Iteration 128, loss = 1.32358765\n",
      "Iteration 129, loss = 3.87889864\n",
      "Iteration 130, loss = 1.60333499\n",
      "Iteration 131, loss = 1.14286127\n",
      "Iteration 132, loss = 1.08512740\n",
      "Iteration 133, loss = 0.99844667\n",
      "Iteration 134, loss = 0.82792850\n",
      "Iteration 135, loss = 0.43603272\n",
      "Iteration 136, loss = 0.43870424\n",
      "Iteration 137, loss = 0.56894620\n",
      "Iteration 138, loss = 0.63506003\n",
      "Iteration 139, loss = 0.53366554\n",
      "Iteration 140, loss = 0.35381878\n",
      "Iteration 141, loss = 0.25901275\n",
      "Iteration 142, loss = 0.26173270\n",
      "Iteration 143, loss = 0.26833973\n",
      "Iteration 144, loss = 0.37777377\n",
      "Iteration 145, loss = 0.29633482\n",
      "Iteration 146, loss = 0.22225743\n",
      "Iteration 147, loss = 0.20874127\n",
      "Iteration 148, loss = 0.24481658\n",
      "Iteration 149, loss = 0.18307310\n",
      "Iteration 150, loss = 0.18808167\n",
      "Iteration 151, loss = 0.20812137\n",
      "Iteration 152, loss = 0.18694801\n",
      "Iteration 153, loss = 0.15702043\n",
      "Iteration 154, loss = 0.15965984\n",
      "Iteration 155, loss = 0.14915620\n",
      "Iteration 156, loss = 0.11758246\n",
      "Iteration 157, loss = 0.13531678\n",
      "Iteration 158, loss = 0.14426481\n",
      "Iteration 159, loss = 0.12179948\n",
      "Iteration 160, loss = 0.16518996\n",
      "Iteration 161, loss = 0.15766357\n",
      "Iteration 162, loss = 0.15069752\n",
      "Iteration 163, loss = 0.14080056\n",
      "Iteration 164, loss = 0.16007399\n",
      "Iteration 165, loss = 0.17526525\n",
      "Iteration 166, loss = 0.14976711\n",
      "Iteration 167, loss = 0.12319380\n",
      "Iteration 168, loss = 0.17877758\n",
      "Iteration 169, loss = 0.13718303\n",
      "Iteration 170, loss = 0.14803138\n",
      "Iteration 171, loss = 0.20539924\n",
      "Iteration 172, loss = 0.16457653\n",
      "Iteration 173, loss = 0.19775739\n",
      "Iteration 174, loss = 0.20369929\n",
      "Iteration 175, loss = 0.20993630\n",
      "Iteration 176, loss = 0.29776494\n",
      "Iteration 177, loss = 0.27160151\n",
      "Iteration 178, loss = 0.31247783\n",
      "Iteration 179, loss = 0.46092832\n",
      "Iteration 180, loss = 0.43300672\n",
      "Iteration 181, loss = 0.56779844\n",
      "Iteration 182, loss = 0.56730328\n",
      "Iteration 183, loss = 0.51640606\n",
      "Iteration 184, loss = 0.45711549\n",
      "Iteration 185, loss = 0.30274408\n",
      "Iteration 186, loss = 0.23948942\n",
      "Iteration 187, loss = 0.20889166\n",
      "Iteration 188, loss = 0.35135761\n",
      "Iteration 189, loss = 0.15932626\n",
      "Iteration 190, loss = 0.16700002\n",
      "Iteration 191, loss = 0.19294824\n",
      "Iteration 192, loss = 0.14796720\n",
      "Iteration 193, loss = 0.17089008\n",
      "Iteration 194, loss = 0.14727826\n",
      "Iteration 195, loss = 0.13340126\n",
      "Iteration 196, loss = 0.11762318\n",
      "Iteration 197, loss = 0.13959464\n",
      "Iteration 198, loss = 0.11917846\n",
      "Iteration 199, loss = 0.12032511\n",
      "Iteration 200, loss = 0.12416141\n",
      "Iteration 201, loss = 0.12391215\n",
      "Iteration 202, loss = 0.11844469\n",
      "Iteration 203, loss = 0.11642830\n",
      "Iteration 204, loss = 0.11836181\n",
      "Iteration 205, loss = 0.12169415\n",
      "Iteration 206, loss = 0.12039303\n",
      "Iteration 207, loss = 0.11387970\n",
      "Iteration 208, loss = 0.11453130\n",
      "Iteration 209, loss = 0.11332823\n",
      "Iteration 210, loss = 0.11674513\n",
      "Iteration 211, loss = 0.11525596\n",
      "Iteration 212, loss = 0.11949543\n",
      "Iteration 213, loss = 0.11496662\n",
      "Iteration 214, loss = 0.11647832\n",
      "Iteration 215, loss = 0.12105564\n",
      "Iteration 216, loss = 0.14329285\n",
      "Iteration 217, loss = 0.11983076\n",
      "Iteration 218, loss = 0.12267351\n",
      "Iteration 219, loss = 0.11793497\n",
      "Iteration 220, loss = 0.11400263\n",
      "Iteration 221, loss = 0.11364685\n",
      "Iteration 222, loss = 0.12186013\n",
      "Iteration 223, loss = 0.11555527\n",
      "Iteration 224, loss = 0.13462160\n",
      "Iteration 225, loss = 0.13561621\n",
      "Iteration 226, loss = 0.13043671\n",
      "Iteration 227, loss = 0.14594342\n",
      "Iteration 228, loss = 0.15425033\n",
      "Iteration 229, loss = 0.13932909\n",
      "Iteration 230, loss = 0.13572539\n",
      "Iteration 231, loss = 0.12542927\n",
      "Iteration 232, loss = 0.11979051\n",
      "Iteration 233, loss = 0.11689936\n",
      "Iteration 234, loss = 0.12640637\n",
      "Iteration 235, loss = 0.11604049\n",
      "Iteration 236, loss = 0.11914709\n",
      "Iteration 237, loss = 0.11319739\n",
      "Iteration 238, loss = 0.11025101\n",
      "Iteration 239, loss = 0.11518863\n",
      "Iteration 240, loss = 0.12600970\n",
      "Iteration 241, loss = 0.11750723\n",
      "Iteration 242, loss = 0.12407671\n",
      "Iteration 243, loss = 0.11898561\n",
      "Iteration 244, loss = 0.11544857\n",
      "Iteration 245, loss = 0.10999127\n",
      "Iteration 246, loss = 0.11634827\n",
      "Iteration 247, loss = 0.12032930\n",
      "Iteration 248, loss = 0.11293253\n",
      "Iteration 249, loss = 0.11622469\n",
      "Iteration 250, loss = 0.13024851\n",
      "Iteration 251, loss = 0.11022609\n",
      "Iteration 252, loss = 0.12017321\n",
      "Iteration 253, loss = 0.11614383\n",
      "Iteration 254, loss = 0.11012781\n",
      "Iteration 255, loss = 0.11646584\n",
      "Iteration 256, loss = 0.11383120\n",
      "Iteration 257, loss = 0.12449541\n",
      "Iteration 258, loss = 0.12343582\n",
      "Iteration 259, loss = 0.12378050\n",
      "Iteration 260, loss = 0.12558098\n",
      "Iteration 261, loss = 0.13223270\n",
      "Iteration 262, loss = 0.11896630\n",
      "Iteration 263, loss = 0.11825103\n",
      "Iteration 264, loss = 0.11906184\n",
      "Iteration 265, loss = 0.11381873\n",
      "Iteration 266, loss = 0.11523684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 267, loss = 0.11682037\n",
      "Iteration 268, loss = 0.11594265\n",
      "Iteration 269, loss = 0.11959155\n",
      "Iteration 270, loss = 0.11112517\n",
      "Iteration 271, loss = 0.11189158\n",
      "Iteration 272, loss = 0.11374312\n",
      "Iteration 273, loss = 0.11197041\n",
      "Iteration 274, loss = 0.11081025\n",
      "Iteration 275, loss = 0.11330387\n",
      "Iteration 276, loss = 0.11360627\n",
      "Iteration 277, loss = 0.11018319\n",
      "Iteration 278, loss = 0.11577958\n",
      "Iteration 279, loss = 0.11379294\n",
      "Iteration 280, loss = 0.11523426\n",
      "Iteration 281, loss = 0.11374512\n",
      "Iteration 282, loss = 0.11630182\n",
      "Iteration 283, loss = 0.11568518\n",
      "Iteration 284, loss = 0.11379116\n",
      "Iteration 285, loss = 0.11660067\n",
      "Iteration 286, loss = 0.11939059\n",
      "Iteration 287, loss = 0.12117604\n",
      "Iteration 288, loss = 0.11426696\n",
      "Iteration 289, loss = 0.13733079\n",
      "Iteration 290, loss = 0.11939652\n",
      "Iteration 291, loss = 0.11762185\n",
      "Iteration 292, loss = 0.11423851\n",
      "Iteration 293, loss = 0.11710619\n",
      "Iteration 294, loss = 0.11322025\n",
      "Iteration 295, loss = 0.11949796\n",
      "Iteration 296, loss = 0.11282146\n",
      "Iteration 297, loss = 0.11691548\n",
      "Iteration 298, loss = 0.12066540\n",
      "Iteration 299, loss = 0.11211416\n",
      "Iteration 300, loss = 0.11737689\n",
      "Iteration 301, loss = 0.12588332\n",
      "Iteration 302, loss = 0.11043528\n",
      "Iteration 303, loss = 0.11304900\n",
      "Iteration 304, loss = 0.12896985\n",
      "Iteration 305, loss = 0.11477809\n",
      "Iteration 306, loss = 0.11498402\n",
      "Iteration 307, loss = 0.11929546\n",
      "Iteration 308, loss = 0.11796758\n",
      "Iteration 309, loss = 0.11436078\n",
      "Iteration 310, loss = 0.11199108\n",
      "Iteration 311, loss = 0.11705395\n",
      "Iteration 312, loss = 0.11996925\n",
      "Iteration 313, loss = 0.11660194\n",
      "Iteration 314, loss = 0.11331209\n",
      "Iteration 315, loss = 0.11603132\n",
      "Iteration 316, loss = 0.11768663\n",
      "Iteration 317, loss = 0.11232252\n",
      "Iteration 318, loss = 0.11451394\n",
      "Iteration 319, loss = 0.11566850\n",
      "Iteration 320, loss = 0.11608323\n",
      "Iteration 321, loss = 0.11455445\n",
      "Iteration 322, loss = 0.10915107\n",
      "Iteration 323, loss = 0.13619679\n",
      "Iteration 324, loss = 0.12324483\n",
      "Iteration 325, loss = 0.12405771\n",
      "Iteration 326, loss = 0.11996636\n",
      "Iteration 327, loss = 0.12539666\n",
      "Iteration 328, loss = 0.12064049\n",
      "Iteration 329, loss = 0.13026897\n",
      "Iteration 330, loss = 0.11237501\n",
      "Iteration 331, loss = 0.11304452\n",
      "Iteration 332, loss = 0.11206856\n",
      "Iteration 333, loss = 0.11971164\n",
      "Iteration 334, loss = 0.11523503\n",
      "Iteration 335, loss = 0.11664235\n",
      "Iteration 336, loss = 0.11594167\n",
      "Iteration 337, loss = 0.12694522\n",
      "Iteration 338, loss = 0.12807810\n",
      "Iteration 339, loss = 0.13721734\n",
      "Iteration 340, loss = 0.17512316\n",
      "Iteration 341, loss = 0.14342120\n",
      "Iteration 342, loss = 0.14563318\n",
      "Iteration 343, loss = 0.11320147\n",
      "Iteration 344, loss = 0.11886345\n",
      "Iteration 345, loss = 0.13056573\n",
      "Iteration 346, loss = 0.12273600\n",
      "Iteration 347, loss = 0.10970424\n",
      "Iteration 348, loss = 0.11517779\n",
      "Iteration 349, loss = 0.11404781\n",
      "Iteration 350, loss = 0.12686950\n",
      "Iteration 351, loss = 0.13213587\n",
      "Iteration 352, loss = 0.14207035\n",
      "Iteration 353, loss = 0.17209746\n",
      "Iteration 354, loss = 0.21247155\n",
      "Iteration 355, loss = 0.16085316\n",
      "Iteration 356, loss = 0.15729402\n",
      "Iteration 357, loss = 0.15612928\n",
      "Iteration 358, loss = 0.14642487\n",
      "Iteration 359, loss = 0.11407790\n",
      "Iteration 360, loss = 0.12315658\n",
      "Iteration 361, loss = 0.12563075\n",
      "Iteration 362, loss = 0.15010359\n",
      "Iteration 363, loss = 0.12064885\n",
      "Iteration 364, loss = 0.12192967\n",
      "Iteration 365, loss = 0.12649744\n",
      "Iteration 366, loss = 0.12726139\n",
      "Iteration 367, loss = 0.18814761\n",
      "Iteration 368, loss = 0.18207376\n",
      "Iteration 369, loss = 0.14131118\n",
      "Iteration 370, loss = 0.17380431\n",
      "Iteration 371, loss = 0.14243911\n",
      "Iteration 372, loss = 0.14897470\n",
      "Iteration 373, loss = 0.21075199\n",
      "Iteration 374, loss = 0.19045285\n",
      "Iteration 375, loss = 0.18510291\n",
      "Iteration 376, loss = 0.18226879\n",
      "Iteration 377, loss = 0.15867802\n",
      "Iteration 378, loss = 0.21246248\n",
      "Iteration 379, loss = 0.22051088\n",
      "Iteration 380, loss = 0.13818057\n",
      "Iteration 381, loss = 0.20308473\n",
      "Iteration 382, loss = 0.12812742\n",
      "Iteration 383, loss = 0.18127295\n",
      "Iteration 384, loss = 0.28322655\n",
      "Iteration 385, loss = 0.23837918\n",
      "Iteration 386, loss = 0.18619223\n",
      "Iteration 387, loss = 0.16753412\n",
      "Iteration 388, loss = 0.14802062\n",
      "Iteration 389, loss = 0.12593429\n",
      "Iteration 390, loss = 0.16170858\n",
      "Iteration 391, loss = 0.15688044\n",
      "Iteration 392, loss = 0.13168056\n",
      "Iteration 393, loss = 0.17286888\n",
      "Iteration 394, loss = 0.15893025\n",
      "Iteration 395, loss = 0.21297282\n",
      "Iteration 396, loss = 0.12125572\n",
      "Iteration 397, loss = 0.14299470\n",
      "Iteration 398, loss = 0.19032519\n",
      "Iteration 399, loss = 0.17281468\n",
      "Iteration 400, loss = 0.16659335\n",
      "Iteration 401, loss = 0.22480011\n",
      "Iteration 402, loss = 0.18119216\n",
      "Iteration 403, loss = 0.31794521\n",
      "Iteration 404, loss = 0.18170904\n",
      "Iteration 405, loss = 1.59705717\n",
      "Iteration 406, loss = 6.03919879\n",
      "Iteration 407, loss = 2.86192244\n",
      "Iteration 408, loss = 1.48044189\n",
      "Iteration 409, loss = 2.59962876\n",
      "Iteration 410, loss = 1.48673687\n",
      "Iteration 411, loss = 0.87047447\n",
      "Iteration 412, loss = 1.38066668\n",
      "Iteration 413, loss = 0.67601728\n",
      "Iteration 414, loss = 0.89877446\n",
      "Iteration 415, loss = 0.54868642\n",
      "Iteration 416, loss = 0.76645882\n",
      "Iteration 417, loss = 0.75250133\n",
      "Iteration 418, loss = 0.48286136\n",
      "Iteration 419, loss = 0.48436835\n",
      "Iteration 420, loss = 0.41630929\n",
      "Iteration 421, loss = 0.25066493\n",
      "Iteration 422, loss = 0.24041120\n",
      "Iteration 423, loss = 0.23684319\n",
      "Iteration 424, loss = 0.24605185\n",
      "Iteration 425, loss = 0.22744188\n",
      "Iteration 426, loss = 0.40692016\n",
      "Iteration 427, loss = 0.47762031\n",
      "Iteration 428, loss = 0.42369911\n",
      "Iteration 429, loss = 0.39708648\n",
      "Iteration 430, loss = 0.36699870\n",
      "Iteration 431, loss = 0.27456395\n",
      "Iteration 432, loss = 0.22151657\n",
      "Iteration 433, loss = 0.15698026\n",
      "Iteration 434, loss = 0.21709500\n",
      "Iteration 435, loss = 0.25628463\n",
      "Iteration 436, loss = 0.22173431\n",
      "Iteration 437, loss = 0.33244235\n",
      "Iteration 438, loss = 0.21404696\n",
      "Iteration 439, loss = 0.18602860\n",
      "Iteration 440, loss = 0.18096294\n",
      "Iteration 441, loss = 0.15231637\n",
      "Iteration 442, loss = 0.14608469\n",
      "Iteration 443, loss = 0.14148007\n",
      "Iteration 444, loss = 0.15447598\n",
      "Iteration 445, loss = 0.40783056\n",
      "Iteration 446, loss = 0.38093059\n",
      "Iteration 447, loss = 0.24522693\n",
      "Iteration 448, loss = 0.47000214\n",
      "Iteration 449, loss = 0.32437977\n",
      "Iteration 450, loss = 0.41374167\n",
      "Iteration 451, loss = 0.64421422\n",
      "Iteration 452, loss = 0.35249770\n",
      "Iteration 453, loss = 0.50396715\n",
      "Iteration 454, loss = 0.60773907\n",
      "Iteration 455, loss = 0.83679440\n",
      "Iteration 456, loss = 0.66388996\n",
      "Iteration 457, loss = 0.92756270\n",
      "Iteration 458, loss = 0.43718885\n",
      "Iteration 459, loss = 0.47362870\n",
      "Iteration 460, loss = 0.36653734\n",
      "Iteration 461, loss = 0.26156524\n",
      "Iteration 462, loss = 0.22299561\n",
      "Iteration 463, loss = 0.22484593\n",
      "Iteration 464, loss = 0.20152182\n",
      "Iteration 465, loss = 0.28110881\n",
      "Iteration 466, loss = 0.19807864\n",
      "Iteration 467, loss = 0.19648415\n",
      "Iteration 468, loss = 0.15378700\n",
      "Iteration 469, loss = 0.14095097\n",
      "Iteration 470, loss = 0.14129676\n",
      "Iteration 471, loss = 0.18973371\n",
      "Iteration 472, loss = 0.16172881\n",
      "Iteration 473, loss = 0.13450806\n",
      "Iteration 474, loss = 0.15934583\n",
      "Iteration 475, loss = 0.18655052\n",
      "Iteration 476, loss = 0.15708640\n",
      "Iteration 477, loss = 0.14023352\n",
      "Iteration 478, loss = 0.17652129\n",
      "Iteration 479, loss = 0.15946791\n",
      "Iteration 480, loss = 0.20108264\n",
      "Iteration 481, loss = 0.18284074\n",
      "Iteration 482, loss = 0.13699031\n",
      "Iteration 483, loss = 0.13239308\n",
      "Iteration 484, loss = 0.12087665\n",
      "Iteration 485, loss = 0.12928839\n",
      "Iteration 486, loss = 0.16018975\n",
      "Iteration 487, loss = 0.12211231\n",
      "Iteration 488, loss = 0.12086241\n",
      "Iteration 489, loss = 0.12773066\n",
      "Iteration 490, loss = 0.12441239\n",
      "Iteration 491, loss = 0.13814038\n",
      "Iteration 492, loss = 0.13118518\n",
      "Iteration 493, loss = 0.12524639\n",
      "Iteration 494, loss = 0.11425865\n",
      "Iteration 495, loss = 0.16584234\n",
      "Iteration 496, loss = 0.14914308\n",
      "Iteration 497, loss = 0.14266947\n",
      "Iteration 498, loss = 0.13172154\n",
      "Iteration 499, loss = 0.13066973\n",
      "Iteration 500, loss = 0.12531159\n",
      "Iteration 501, loss = 0.12329888\n",
      "Iteration 502, loss = 0.14089441\n",
      "Iteration 503, loss = 0.19825902\n",
      "Iteration 504, loss = 0.16046363\n",
      "Iteration 505, loss = 0.13452408\n",
      "Iteration 506, loss = 0.18854845\n",
      "Iteration 507, loss = 0.14060386\n",
      "Iteration 508, loss = 0.12771023\n",
      "Iteration 509, loss = 0.14806631\n",
      "Iteration 510, loss = 0.15036824\n",
      "Iteration 511, loss = 0.11807363\n",
      "Iteration 512, loss = 0.15571535\n",
      "Iteration 513, loss = 0.14242456\n",
      "Iteration 514, loss = 0.19928283\n",
      "Iteration 515, loss = 0.14908860\n",
      "Iteration 516, loss = 0.14625046\n",
      "Iteration 517, loss = 0.15161645\n",
      "Iteration 518, loss = 0.13373084\n",
      "Iteration 519, loss = 0.13978635\n",
      "Iteration 520, loss = 0.12882938\n",
      "Iteration 521, loss = 0.16948066\n",
      "Iteration 522, loss = 0.13910860\n",
      "Iteration 523, loss = 0.11590043\n",
      "Iteration 524, loss = 0.12028077\n",
      "Iteration 525, loss = 0.12571931\n",
      "Iteration 526, loss = 0.14698072\n",
      "Iteration 527, loss = 0.12769217\n",
      "Iteration 528, loss = 0.13013180\n",
      "Iteration 529, loss = 0.12777849\n",
      "Iteration 530, loss = 0.11767771\n",
      "Iteration 531, loss = 0.12710778\n",
      "Iteration 532, loss = 0.13537293\n",
      "Iteration 533, loss = 0.12756612\n",
      "Iteration 534, loss = 0.14306257\n",
      "Iteration 535, loss = 0.13419204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 536, loss = 0.11888359\n",
      "Iteration 537, loss = 0.11674183\n",
      "Iteration 538, loss = 0.12611528\n",
      "Iteration 539, loss = 0.11749570\n",
      "Iteration 540, loss = 0.11327350\n",
      "Iteration 541, loss = 0.13993411\n",
      "Iteration 542, loss = 0.11713239\n",
      "Iteration 543, loss = 0.11660040\n",
      "Iteration 544, loss = 0.11548781\n",
      "Iteration 545, loss = 0.12909229\n",
      "Iteration 546, loss = 0.13636445\n",
      "Iteration 547, loss = 0.12313272\n",
      "Iteration 548, loss = 0.11809376\n",
      "Iteration 549, loss = 0.12022346\n",
      "Iteration 550, loss = 0.11746999\n",
      "Iteration 551, loss = 0.11408950\n",
      "Iteration 552, loss = 0.12086696\n",
      "Iteration 553, loss = 0.12017111\n",
      "Iteration 554, loss = 0.11415400\n",
      "Iteration 555, loss = 0.11977457\n",
      "Iteration 556, loss = 0.12380861\n",
      "Iteration 557, loss = 0.11919547\n",
      "Iteration 558, loss = 0.11167640\n",
      "Iteration 559, loss = 0.11597495\n",
      "Iteration 560, loss = 0.11305029\n",
      "Iteration 561, loss = 0.11874826\n",
      "Iteration 562, loss = 0.11373617\n",
      "Iteration 563, loss = 0.11266668\n",
      "Iteration 564, loss = 0.12515801\n",
      "Iteration 565, loss = 0.12185985\n",
      "Iteration 566, loss = 0.12510163\n",
      "Iteration 567, loss = 0.12247615\n",
      "Iteration 568, loss = 0.13611710\n",
      "Iteration 569, loss = 0.12252600\n",
      "Iteration 570, loss = 0.13282697\n",
      "Iteration 571, loss = 0.11664988\n",
      "Iteration 572, loss = 0.11690966\n",
      "Iteration 573, loss = 0.12879399\n",
      "Iteration 574, loss = 0.12215885\n",
      "Iteration 575, loss = 0.11031567\n",
      "Iteration 576, loss = 0.11599642\n",
      "Iteration 577, loss = 0.11313809\n",
      "Iteration 578, loss = 0.11791019\n",
      "Iteration 579, loss = 0.12097613\n",
      "Iteration 580, loss = 0.11902804\n",
      "Iteration 581, loss = 0.11515816\n",
      "Iteration 582, loss = 0.17039084\n",
      "Iteration 583, loss = 0.27242508\n",
      "Iteration 584, loss = 0.31154364\n",
      "Iteration 585, loss = 0.22571692\n",
      "Iteration 586, loss = 0.14606047\n",
      "Iteration 587, loss = 0.13221693\n",
      "Iteration 588, loss = 0.15403990\n",
      "Iteration 589, loss = 0.15358280\n",
      "Iteration 590, loss = 0.12647171\n",
      "Iteration 591, loss = 0.12250679\n",
      "Iteration 592, loss = 0.15239906\n",
      "Iteration 593, loss = 0.11516344\n",
      "Iteration 594, loss = 0.11313307\n",
      "Iteration 595, loss = 0.11671028\n",
      "Iteration 596, loss = 0.12387587\n",
      "Iteration 597, loss = 0.13047588\n",
      "Iteration 598, loss = 0.13046223\n",
      "Iteration 599, loss = 0.11974191\n",
      "Iteration 600, loss = 0.14441337\n",
      "Iteration 601, loss = 0.14470966\n",
      "Iteration 602, loss = 0.15369574\n",
      "Iteration 603, loss = 0.11692265\n",
      "Iteration 604, loss = 0.12256929\n",
      "Iteration 605, loss = 0.12884966\n",
      "Iteration 606, loss = 0.11484322\n",
      "Iteration 607, loss = 0.11461598\n",
      "Iteration 608, loss = 0.11912173\n",
      "Iteration 609, loss = 0.12502677\n",
      "Iteration 610, loss = 0.12478704\n",
      "Iteration 611, loss = 0.11590502\n",
      "Iteration 612, loss = 0.12087602\n",
      "Iteration 613, loss = 0.11570175\n",
      "Iteration 614, loss = 0.11858614\n",
      "Iteration 615, loss = 0.11807352\n",
      "Iteration 616, loss = 0.11919063\n",
      "Iteration 617, loss = 0.13195035\n",
      "Iteration 618, loss = 0.12156972\n",
      "Iteration 619, loss = 0.11284284\n",
      "Iteration 620, loss = 0.11356186\n",
      "Iteration 621, loss = 0.12024029\n",
      "Iteration 622, loss = 0.12883891\n",
      "Iteration 623, loss = 0.11873286\n",
      "Iteration 624, loss = 0.11544449\n",
      "Iteration 625, loss = 0.12274607\n",
      "Iteration 626, loss = 0.11789061\n",
      "Iteration 627, loss = 0.11788140\n",
      "Iteration 628, loss = 0.11316128\n",
      "Iteration 629, loss = 0.11758944\n",
      "Iteration 630, loss = 0.11330563\n",
      "Iteration 631, loss = 0.11534452\n",
      "Iteration 632, loss = 0.11341171\n",
      "Iteration 633, loss = 0.11080349\n",
      "Iteration 634, loss = 0.11906606\n",
      "Iteration 635, loss = 0.13065868\n",
      "Iteration 636, loss = 0.11268150\n",
      "Iteration 637, loss = 0.11643469\n",
      "Iteration 638, loss = 0.11578803\n",
      "Iteration 639, loss = 0.11802197\n",
      "Iteration 640, loss = 0.10981773\n",
      "Iteration 641, loss = 0.11480554\n",
      "Iteration 642, loss = 0.12212805\n",
      "Iteration 643, loss = 0.11640171\n",
      "Iteration 644, loss = 0.11753828\n",
      "Iteration 645, loss = 0.11693847\n",
      "Iteration 646, loss = 0.11201780\n",
      "Iteration 647, loss = 0.11814882\n",
      "Iteration 648, loss = 0.11663747\n",
      "Iteration 649, loss = 0.12629697\n",
      "Iteration 650, loss = 0.12103063\n",
      "Iteration 651, loss = 0.11764229\n",
      "Iteration 652, loss = 0.11497809\n",
      "Iteration 653, loss = 0.11403110\n",
      "Iteration 654, loss = 0.11388586\n",
      "Iteration 655, loss = 0.14810794\n",
      "Iteration 656, loss = 0.15170264\n",
      "Iteration 657, loss = 0.20607554\n",
      "Iteration 658, loss = 0.19479745\n",
      "Iteration 659, loss = 0.16546338\n",
      "Iteration 660, loss = 0.12914864\n",
      "Iteration 661, loss = 0.14044279\n",
      "Iteration 662, loss = 0.12659908\n",
      "Iteration 663, loss = 0.14326695\n",
      "Iteration 664, loss = 0.13100595\n",
      "Iteration 665, loss = 0.12432923\n",
      "Iteration 666, loss = 0.11314462\n",
      "Iteration 667, loss = 0.11355981\n",
      "Iteration 668, loss = 0.12486437\n",
      "Iteration 669, loss = 0.11197517\n",
      "Iteration 670, loss = 0.11531206\n",
      "Iteration 671, loss = 0.11591288\n",
      "Iteration 672, loss = 0.11572787\n",
      "Iteration 673, loss = 0.11738406\n",
      "Iteration 674, loss = 0.11899762\n",
      "Iteration 675, loss = 0.11348916\n",
      "Iteration 676, loss = 0.11777148\n",
      "Iteration 677, loss = 0.12807021\n",
      "Iteration 678, loss = 0.16339543\n",
      "Iteration 679, loss = 0.12274029\n",
      "Iteration 680, loss = 0.10890224\n",
      "Iteration 681, loss = 0.14286673\n",
      "Iteration 682, loss = 0.13402784\n",
      "Iteration 683, loss = 0.12619322\n",
      "Iteration 684, loss = 0.18462209\n",
      "Iteration 685, loss = 0.17216189\n",
      "Iteration 686, loss = 0.22995067\n",
      "Iteration 687, loss = 0.15540101\n",
      "Iteration 688, loss = 0.11902002\n",
      "Iteration 689, loss = 0.12462520\n",
      "Iteration 690, loss = 0.13721236\n",
      "Iteration 691, loss = 0.14417406\n",
      "Iteration 692, loss = 0.13842953\n",
      "Iteration 693, loss = 0.11557978\n",
      "Iteration 694, loss = 0.16933837\n",
      "Iteration 695, loss = 0.15509163\n",
      "Iteration 696, loss = 0.16963983\n",
      "Iteration 697, loss = 0.14114323\n",
      "Iteration 698, loss = 0.12772339\n",
      "Iteration 699, loss = 0.12999313\n",
      "Iteration 700, loss = 0.12239157\n",
      "Iteration 701, loss = 0.13968938\n",
      "Iteration 702, loss = 0.11585888\n",
      "Iteration 703, loss = 0.13341967\n",
      "Iteration 704, loss = 0.14315777\n",
      "Iteration 705, loss = 0.15089908\n",
      "Iteration 706, loss = 0.11383742\n",
      "Iteration 707, loss = 0.13323512\n",
      "Iteration 708, loss = 0.12395566\n",
      "Iteration 709, loss = 0.14281640\n",
      "Iteration 710, loss = 0.14669762\n",
      "Iteration 711, loss = 0.16198900\n",
      "Iteration 712, loss = 0.24440575\n",
      "Iteration 713, loss = 0.13902530\n",
      "Iteration 714, loss = 0.12346564\n",
      "Iteration 715, loss = 0.14780819\n",
      "Iteration 716, loss = 0.13001918\n",
      "Iteration 717, loss = 0.12794079\n",
      "Iteration 718, loss = 0.12435839\n",
      "Iteration 719, loss = 0.13976964\n",
      "Iteration 720, loss = 0.16619875\n",
      "Iteration 721, loss = 0.15845886\n",
      "Iteration 722, loss = 0.11969296\n",
      "Iteration 723, loss = 0.14689586\n",
      "Iteration 724, loss = 0.13573986\n",
      "Iteration 725, loss = 0.12138430\n",
      "Iteration 726, loss = 0.11166099\n",
      "Iteration 727, loss = 0.13919631\n",
      "Iteration 728, loss = 0.13208660\n",
      "Iteration 729, loss = 0.12677149\n",
      "Iteration 730, loss = 0.14233731\n",
      "Iteration 731, loss = 0.12034459\n",
      "Iteration 732, loss = 0.11804373\n",
      "Iteration 733, loss = 0.12262203\n",
      "Iteration 734, loss = 0.11343218\n",
      "Iteration 735, loss = 0.11344059\n",
      "Iteration 736, loss = 0.11714746\n",
      "Iteration 737, loss = 0.11397411\n",
      "Iteration 738, loss = 0.11386042\n",
      "Iteration 739, loss = 0.11287975\n",
      "Iteration 740, loss = 0.11325674\n",
      "Iteration 741, loss = 0.11189995\n",
      "Iteration 742, loss = 0.11690648\n",
      "Iteration 743, loss = 0.14713469\n",
      "Iteration 744, loss = 0.11296676\n",
      "Iteration 745, loss = 0.12403068\n",
      "Iteration 746, loss = 0.13470573\n",
      "Iteration 747, loss = 0.12249963\n",
      "Iteration 748, loss = 0.11749442\n",
      "Iteration 749, loss = 0.12449986\n",
      "Iteration 750, loss = 0.11063319\n",
      "Iteration 751, loss = 0.12122244\n",
      "Iteration 752, loss = 0.11642173\n",
      "Iteration 753, loss = 0.11753127\n",
      "Iteration 754, loss = 0.12380420\n",
      "Iteration 755, loss = 0.11290407\n",
      "Iteration 756, loss = 0.11817533\n",
      "Iteration 757, loss = 0.13168704\n",
      "Iteration 758, loss = 0.34761309\n",
      "Iteration 759, loss = 0.93751761\n",
      "Iteration 760, loss = 1.13868027\n",
      "Iteration 761, loss = 0.83352725\n",
      "Iteration 762, loss = 2.96506171\n",
      "Iteration 763, loss = 1.87829245\n",
      "Iteration 764, loss = 5.04936803\n",
      "Iteration 765, loss = 3.12218702\n",
      "Iteration 766, loss = 2.38099998\n",
      "Iteration 767, loss = 2.45922914\n",
      "Iteration 768, loss = 1.69452183\n",
      "Iteration 769, loss = 1.20988146\n",
      "Iteration 770, loss = 0.99402879\n",
      "Iteration 771, loss = 1.73340737\n",
      "Iteration 772, loss = 1.41176112\n",
      "Iteration 773, loss = 1.48331806\n",
      "Iteration 774, loss = 1.39317135\n",
      "Iteration 775, loss = 0.94808127\n",
      "Iteration 776, loss = 1.26278183\n",
      "Iteration 777, loss = 1.87407003\n",
      "Iteration 778, loss = 1.54385216\n",
      "Iteration 779, loss = 0.98389993\n",
      "Iteration 780, loss = 1.71217507\n",
      "Iteration 781, loss = 1.02965899\n",
      "Iteration 782, loss = 1.63821769\n",
      "Iteration 783, loss = 1.10888898\n",
      "Iteration 784, loss = 0.77528251\n",
      "Iteration 785, loss = 1.13058501\n",
      "Iteration 786, loss = 0.74586450\n",
      "Iteration 787, loss = 0.64789398\n",
      "Iteration 788, loss = 0.61363675\n",
      "Iteration 789, loss = 0.65394190\n",
      "Iteration 790, loss = 0.33787622\n",
      "Iteration 791, loss = 0.27313130\n",
      "Iteration 792, loss = 0.32453091\n",
      "Iteration 793, loss = 0.31623834\n",
      "Iteration 794, loss = 0.34279619\n",
      "Iteration 795, loss = 0.45481145\n",
      "Iteration 796, loss = 0.41555408\n",
      "Iteration 797, loss = 0.54661775\n",
      "Iteration 798, loss = 0.42405504\n",
      "Iteration 799, loss = 0.40105871\n",
      "Iteration 800, loss = 0.35779000\n",
      "Iteration 801, loss = 0.41900812\n",
      "Iteration 802, loss = 0.35266783\n",
      "Iteration 803, loss = 0.33787753\n",
      "Iteration 804, loss = 0.26860611\n",
      "Iteration 805, loss = 0.32756906\n",
      "Iteration 806, loss = 0.31072335\n",
      "Iteration 807, loss = 0.20630961\n",
      "Iteration 808, loss = 0.20722553\n",
      "Iteration 809, loss = 0.19885693\n",
      "Iteration 810, loss = 0.21471924\n",
      "Iteration 811, loss = 0.17300431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 812, loss = 0.18598021\n",
      "Iteration 813, loss = 0.18934849\n",
      "Iteration 814, loss = 0.16200035\n",
      "Iteration 815, loss = 0.14624280\n",
      "Iteration 816, loss = 0.16953133\n",
      "Iteration 817, loss = 0.17995144\n",
      "Iteration 818, loss = 0.18758937\n",
      "Iteration 819, loss = 0.12846777\n",
      "Iteration 820, loss = 0.15923371\n",
      "Iteration 821, loss = 0.19428219\n",
      "Iteration 822, loss = 0.14105205\n",
      "Iteration 823, loss = 0.15686594\n",
      "Iteration 824, loss = 0.12663954\n",
      "Iteration 825, loss = 0.19265746\n",
      "Iteration 826, loss = 0.18705459\n",
      "Iteration 827, loss = 0.14392535\n",
      "Iteration 828, loss = 0.21360847\n",
      "Iteration 829, loss = 0.20347818\n",
      "Iteration 830, loss = 0.22447518\n",
      "Iteration 831, loss = 0.25151423\n",
      "Iteration 832, loss = 0.18741087\n",
      "Iteration 833, loss = 0.17418507\n",
      "Iteration 834, loss = 0.21632548\n",
      "Iteration 835, loss = 0.18038731\n",
      "Iteration 836, loss = 0.16034288\n",
      "Iteration 837, loss = 0.19955682\n",
      "Iteration 838, loss = 0.19904513\n",
      "Iteration 839, loss = 0.19856502\n",
      "Iteration 840, loss = 0.17118443\n",
      "Iteration 841, loss = 0.17427150\n",
      "Iteration 842, loss = 0.20832220\n",
      "Iteration 843, loss = 0.16642656\n",
      "Iteration 844, loss = 0.15820672\n",
      "Iteration 845, loss = 0.18451199\n",
      "Iteration 846, loss = 0.12835434\n",
      "Iteration 847, loss = 0.10977127\n",
      "Iteration 848, loss = 0.16787154\n",
      "Iteration 849, loss = 0.12343920\n",
      "Iteration 850, loss = 0.13121780\n",
      "Iteration 851, loss = 0.14332844\n",
      "Iteration 852, loss = 0.14225277\n",
      "Iteration 853, loss = 0.18512790\n",
      "Iteration 854, loss = 0.14973915\n",
      "Iteration 855, loss = 0.13417905\n",
      "Iteration 856, loss = 0.12308141\n",
      "Iteration 857, loss = 0.13066233\n",
      "Iteration 858, loss = 0.16271157\n",
      "Iteration 859, loss = 0.16469297\n",
      "Iteration 860, loss = 0.20629271\n",
      "Iteration 861, loss = 0.20507635\n",
      "Iteration 862, loss = 0.15079849\n",
      "Iteration 863, loss = 0.19875134\n",
      "Iteration 864, loss = 0.18902290\n",
      "Iteration 865, loss = 0.14563289\n",
      "Iteration 866, loss = 0.14859820\n",
      "Iteration 867, loss = 0.13384897\n",
      "Iteration 868, loss = 0.16057462\n",
      "Iteration 869, loss = 0.20331712\n",
      "Iteration 870, loss = 0.16884018\n",
      "Iteration 871, loss = 0.13027882\n",
      "Iteration 872, loss = 0.13904887\n",
      "Iteration 873, loss = 0.13552172\n",
      "Iteration 874, loss = 0.12441123\n",
      "Iteration 875, loss = 0.15034227\n",
      "Iteration 876, loss = 0.13298483\n",
      "Iteration 877, loss = 0.16618836\n",
      "Iteration 878, loss = 0.13395056\n",
      "Iteration 879, loss = 0.12323132\n",
      "Iteration 880, loss = 0.12844907\n",
      "Iteration 881, loss = 0.13115199\n",
      "Iteration 882, loss = 0.14586228\n",
      "Iteration 883, loss = 0.14709042\n",
      "Iteration 884, loss = 0.12928977\n",
      "Iteration 885, loss = 0.16467852\n",
      "Iteration 886, loss = 0.13446297\n",
      "Iteration 887, loss = 0.17960270\n",
      "Iteration 888, loss = 0.16255293\n",
      "Iteration 889, loss = 0.11899689\n",
      "Iteration 890, loss = 0.13507963\n",
      "Iteration 891, loss = 0.11549391\n",
      "Iteration 892, loss = 0.13018873\n",
      "Iteration 893, loss = 0.11910215\n",
      "Iteration 894, loss = 0.11645401\n",
      "Iteration 895, loss = 0.13051735\n",
      "Iteration 896, loss = 0.15862318\n",
      "Iteration 897, loss = 0.11679369\n",
      "Iteration 898, loss = 0.11683142\n",
      "Iteration 899, loss = 0.12926551\n",
      "Iteration 900, loss = 0.12949035\n",
      "Iteration 901, loss = 0.13515365\n",
      "Iteration 902, loss = 0.12393253\n",
      "Iteration 903, loss = 0.11339656\n",
      "Iteration 904, loss = 0.13347002\n",
      "Iteration 905, loss = 0.11301283\n",
      "Iteration 906, loss = 0.11221853\n",
      "Iteration 907, loss = 0.12624509\n",
      "Iteration 908, loss = 0.11965227\n",
      "Iteration 909, loss = 0.11425617\n",
      "Iteration 910, loss = 0.11963909\n",
      "Iteration 911, loss = 0.11509281\n",
      "Iteration 912, loss = 0.11495765\n",
      "Iteration 913, loss = 0.11423360\n",
      "Iteration 914, loss = 0.12192216\n",
      "Iteration 915, loss = 0.11284694\n",
      "Iteration 916, loss = 0.11373993\n",
      "Iteration 917, loss = 0.12091418\n",
      "Iteration 918, loss = 0.11991451\n",
      "Iteration 919, loss = 0.13869743\n",
      "Iteration 920, loss = 0.12467996\n",
      "Iteration 921, loss = 0.13925112\n",
      "Iteration 922, loss = 0.13573415\n",
      "Iteration 923, loss = 0.12553367\n",
      "Iteration 924, loss = 0.12795016\n",
      "Iteration 925, loss = 0.11248820\n",
      "Iteration 926, loss = 0.11932513\n",
      "Iteration 927, loss = 0.12070782\n",
      "Iteration 928, loss = 0.12643542\n",
      "Iteration 929, loss = 0.18686233\n",
      "Iteration 930, loss = 0.14896271\n",
      "Iteration 931, loss = 0.12649103\n",
      "Iteration 932, loss = 0.13225924\n",
      "Iteration 933, loss = 0.12687287\n",
      "Iteration 934, loss = 0.11596716\n",
      "Iteration 935, loss = 0.11668114\n",
      "Iteration 936, loss = 0.11922919\n",
      "Iteration 937, loss = 0.13428532\n",
      "Iteration 938, loss = 0.12438628\n",
      "Iteration 939, loss = 0.14217552\n",
      "Iteration 940, loss = 0.16793195\n",
      "Iteration 941, loss = 0.14025187\n",
      "Iteration 942, loss = 0.12150413\n",
      "Iteration 943, loss = 0.11438278\n",
      "Iteration 944, loss = 0.13713320\n",
      "Iteration 945, loss = 0.13238062\n",
      "Iteration 946, loss = 0.13291921\n",
      "Iteration 947, loss = 0.12835470\n",
      "Iteration 948, loss = 0.12396557\n",
      "Iteration 949, loss = 0.11809856\n",
      "Iteration 950, loss = 0.11742385\n",
      "Iteration 951, loss = 0.11805105\n",
      "Iteration 952, loss = 0.11602609\n",
      "Iteration 953, loss = 0.11810670\n",
      "Iteration 954, loss = 0.12012389\n",
      "Iteration 955, loss = 0.11725904\n",
      "Iteration 956, loss = 0.13614787\n",
      "Iteration 957, loss = 0.11799727\n",
      "Iteration 958, loss = 0.13852884\n",
      "Iteration 959, loss = 0.13673173\n",
      "Iteration 960, loss = 0.13951606\n",
      "Iteration 961, loss = 0.13520594\n",
      "Iteration 962, loss = 0.13519734\n",
      "Iteration 963, loss = 0.12814486\n",
      "Iteration 964, loss = 0.12868219\n",
      "Iteration 965, loss = 0.12126771\n",
      "Iteration 966, loss = 0.13636168\n",
      "Iteration 967, loss = 0.12751468\n",
      "Iteration 968, loss = 0.13968020\n",
      "Iteration 969, loss = 0.12316319\n",
      "Iteration 970, loss = 0.16981164\n",
      "Iteration 971, loss = 0.16429667\n",
      "Iteration 972, loss = 0.12376854\n",
      "Iteration 973, loss = 0.15630768\n",
      "Iteration 974, loss = 0.13125365\n",
      "Iteration 975, loss = 0.11616178\n",
      "Iteration 976, loss = 0.12597954\n",
      "Iteration 977, loss = 0.11592972\n",
      "Iteration 978, loss = 0.12005647\n",
      "Iteration 979, loss = 0.10997595\n",
      "Iteration 980, loss = 0.12009011\n",
      "Iteration 981, loss = 0.12283220\n",
      "Iteration 982, loss = 0.11878104\n",
      "Iteration 983, loss = 0.11878399\n",
      "Iteration 984, loss = 0.11231668\n",
      "Iteration 985, loss = 0.12117776\n",
      "Iteration 986, loss = 0.14039311\n",
      "Iteration 987, loss = 0.12546666\n",
      "Iteration 988, loss = 0.12826639\n",
      "Iteration 989, loss = 0.13185302\n",
      "Iteration 990, loss = 0.12421607\n",
      "Iteration 991, loss = 0.13530046\n",
      "Iteration 992, loss = 0.13593403\n",
      "Iteration 993, loss = 0.14588895\n",
      "Iteration 994, loss = 0.11980052\n",
      "Iteration 995, loss = 0.12184020\n",
      "Iteration 996, loss = 0.11196947\n",
      "Iteration 997, loss = 0.11803276\n",
      "Iteration 998, loss = 0.13689012\n",
      "Iteration 999, loss = 0.11475534\n",
      "Iteration 1000, loss = 0.12083911\n",
      "Iteration 1001, loss = 0.13058586\n",
      "Iteration 1002, loss = 0.12419771\n",
      "Iteration 1003, loss = 0.11260024\n",
      "Iteration 1004, loss = 0.12877325\n",
      "Iteration 1005, loss = 0.15827712\n",
      "Iteration 1006, loss = 0.12469212\n",
      "Iteration 1007, loss = 0.12389377\n",
      "Iteration 1008, loss = 0.13286653\n",
      "Iteration 1009, loss = 0.26645908\n",
      "Iteration 1010, loss = 0.51975375\n",
      "Iteration 1011, loss = 0.61455124\n",
      "Iteration 1012, loss = 0.62005089\n",
      "Iteration 1013, loss = 0.49223299\n",
      "Iteration 1014, loss = 0.37667816\n",
      "Iteration 1015, loss = 0.31317324\n",
      "Iteration 1016, loss = 0.17695394\n",
      "Iteration 1017, loss = 0.15878509\n",
      "Iteration 1018, loss = 0.20859905\n",
      "Iteration 1019, loss = 0.20809386\n",
      "Iteration 1020, loss = 0.16313640\n",
      "Iteration 1021, loss = 0.16427966\n",
      "Iteration 1022, loss = 0.13558335\n",
      "Iteration 1023, loss = 0.19920974\n",
      "Iteration 1024, loss = 0.13160435\n",
      "Iteration 1025, loss = 0.11790835\n",
      "Iteration 1026, loss = 0.11657119\n",
      "Iteration 1027, loss = 0.12635213\n",
      "Iteration 1028, loss = 0.11669393\n",
      "Iteration 1029, loss = 0.11718185\n",
      "Iteration 1030, loss = 0.12056125\n",
      "Iteration 1031, loss = 0.12453538\n",
      "Iteration 1032, loss = 0.12524085\n",
      "Iteration 1033, loss = 0.11953852\n",
      "Iteration 1034, loss = 0.12422408\n",
      "Iteration 1035, loss = 0.12933694\n",
      "Iteration 1036, loss = 0.12061119\n",
      "Iteration 1037, loss = 0.11573791\n",
      "Iteration 1038, loss = 0.12209704\n",
      "Iteration 1039, loss = 0.12030048\n",
      "Iteration 1040, loss = 0.11281461\n",
      "Iteration 1041, loss = 0.11488099\n",
      "Iteration 1042, loss = 0.13084871\n",
      "Iteration 1043, loss = 0.12944033\n",
      "Iteration 1044, loss = 0.11658719\n",
      "Iteration 1045, loss = 0.13700266\n",
      "Iteration 1046, loss = 0.11454887\n",
      "Iteration 1047, loss = 0.12177983\n",
      "Iteration 1048, loss = 0.11668012\n",
      "Iteration 1049, loss = 0.12098782\n",
      "Iteration 1050, loss = 0.11674248\n",
      "Iteration 1051, loss = 0.11730029\n",
      "Iteration 1052, loss = 0.11429599\n",
      "Iteration 1053, loss = 0.11466238\n",
      "Iteration 1054, loss = 0.11561347\n",
      "Iteration 1055, loss = 0.11681902\n",
      "Iteration 1056, loss = 0.11758154\n",
      "Iteration 1057, loss = 0.10956052\n",
      "Iteration 1058, loss = 0.12399614\n",
      "Iteration 1059, loss = 0.12261323\n",
      "Iteration 1060, loss = 0.13287694\n",
      "Iteration 1061, loss = 0.12964971\n",
      "Iteration 1062, loss = 0.11893759\n",
      "Iteration 1063, loss = 0.11759619\n",
      "Iteration 1064, loss = 0.12142781\n",
      "Iteration 1065, loss = 0.12894165\n",
      "Iteration 1066, loss = 0.15223249\n",
      "Iteration 1067, loss = 0.11823838\n",
      "Iteration 1068, loss = 0.12541773\n",
      "Iteration 1069, loss = 0.13467060\n",
      "Iteration 1070, loss = 0.11801102\n",
      "Iteration 1071, loss = 0.11570296\n",
      "Iteration 1072, loss = 0.13754150\n",
      "Iteration 1073, loss = 0.18099158\n",
      "Iteration 1074, loss = 0.18266745\n",
      "Iteration 1075, loss = 0.14644582\n",
      "Iteration 1076, loss = 0.15948333\n",
      "Iteration 1077, loss = 0.15273920\n",
      "Iteration 1078, loss = 0.13414478\n",
      "Iteration 1079, loss = 0.13393603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1080, loss = 0.12205832\n",
      "Iteration 1081, loss = 0.13982774\n",
      "Iteration 1082, loss = 0.15471480\n",
      "Iteration 1083, loss = 0.13098666\n",
      "Iteration 1084, loss = 0.12024931\n",
      "Iteration 1085, loss = 0.14020611\n",
      "Iteration 1086, loss = 0.11855005\n",
      "Iteration 1087, loss = 0.11488566\n",
      "Iteration 1088, loss = 0.13873312\n",
      "Iteration 1089, loss = 0.16466032\n",
      "Iteration 1090, loss = 0.17750578\n",
      "Iteration 1091, loss = 0.32180812\n",
      "Iteration 1092, loss = 0.36290931\n",
      "Iteration 1093, loss = 0.34089842\n",
      "Iteration 1094, loss = 0.21785678\n",
      "Iteration 1095, loss = 0.25235119\n",
      "Iteration 1096, loss = 0.21331386\n",
      "Iteration 1097, loss = 0.22769008\n",
      "Iteration 1098, loss = 0.12246212\n",
      "Iteration 1099, loss = 0.16998289\n",
      "Iteration 1100, loss = 0.15357178\n",
      "Iteration 1101, loss = 0.13369976\n",
      "Iteration 1102, loss = 0.16432454\n",
      "Iteration 1103, loss = 0.13224885\n",
      "Iteration 1104, loss = 0.12194018\n",
      "Iteration 1105, loss = 0.13170664\n",
      "Iteration 1106, loss = 0.11527974\n",
      "Iteration 1107, loss = 0.11785191\n",
      "Iteration 1108, loss = 0.13932889\n",
      "Iteration 1109, loss = 0.12046747\n",
      "Iteration 1110, loss = 0.11366229\n",
      "Iteration 1111, loss = 0.11866642\n",
      "Iteration 1112, loss = 0.12806560\n",
      "Iteration 1113, loss = 0.12730709\n",
      "Iteration 1114, loss = 0.12715729\n",
      "Iteration 1115, loss = 0.13306612\n",
      "Iteration 1116, loss = 0.14917125\n",
      "Iteration 1117, loss = 0.13539810\n",
      "Iteration 1118, loss = 0.12208208\n",
      "Iteration 1119, loss = 0.12233951\n",
      "Iteration 1120, loss = 0.13497003\n",
      "Iteration 1121, loss = 0.12997867\n",
      "Iteration 1122, loss = 0.12021224\n",
      "Iteration 1123, loss = 0.12391798\n",
      "Iteration 1124, loss = 0.11875350\n",
      "Iteration 1125, loss = 0.12229598\n",
      "Iteration 1126, loss = 0.13121566\n",
      "Iteration 1127, loss = 0.13367722\n",
      "Iteration 1128, loss = 0.12069607\n",
      "Iteration 1129, loss = 0.12928546\n",
      "Iteration 1130, loss = 0.12110762\n",
      "Iteration 1131, loss = 0.11676876\n",
      "Iteration 1132, loss = 0.11235470\n",
      "Iteration 1133, loss = 0.13010859\n",
      "Iteration 1134, loss = 0.14355300\n",
      "Iteration 1135, loss = 0.11789903\n",
      "Iteration 1136, loss = 0.13381806\n",
      "Iteration 1137, loss = 0.12960300\n",
      "Iteration 1138, loss = 0.12173616\n",
      "Iteration 1139, loss = 0.11702761\n",
      "Iteration 1140, loss = 0.11794833\n",
      "Iteration 1141, loss = 0.11484172\n",
      "Iteration 1142, loss = 0.12686108\n",
      "Iteration 1143, loss = 0.12934481\n",
      "Iteration 1144, loss = 0.13824072\n",
      "Iteration 1145, loss = 0.14911223\n",
      "Iteration 1146, loss = 0.13852888\n",
      "Iteration 1147, loss = 0.22181510\n",
      "Iteration 1148, loss = 0.30061353\n",
      "Iteration 1149, loss = 0.51484172\n",
      "Iteration 1150, loss = 0.17684675\n",
      "Iteration 1151, loss = 0.18225797\n",
      "Iteration 1152, loss = 0.25612985\n",
      "Iteration 1153, loss = 0.25195713\n",
      "Iteration 1154, loss = 0.23614976\n",
      "Iteration 1155, loss = 0.19776485\n",
      "Iteration 1156, loss = 0.14261728\n",
      "Iteration 1157, loss = 0.14471708\n",
      "Iteration 1158, loss = 0.12417679\n",
      "Iteration 1159, loss = 0.13619199\n",
      "Iteration 1160, loss = 0.13794654\n",
      "Iteration 1161, loss = 0.13185355\n",
      "Iteration 1162, loss = 0.11828135\n",
      "Iteration 1163, loss = 0.14378786\n",
      "Iteration 1164, loss = 0.21897291\n",
      "Iteration 1165, loss = 0.18777073\n",
      "Iteration 1166, loss = 0.12488886\n",
      "Iteration 1167, loss = 0.14469320\n",
      "Iteration 1168, loss = 0.12492004\n",
      "Iteration 1169, loss = 0.12285286\n",
      "Iteration 1170, loss = 0.13768818\n",
      "Iteration 1171, loss = 0.11283369\n",
      "Iteration 1172, loss = 0.12883000\n",
      "Iteration 1173, loss = 0.14180027\n",
      "Iteration 1174, loss = 0.12390076\n",
      "Iteration 1175, loss = 0.14766450\n",
      "Iteration 1176, loss = 0.12153061\n",
      "Iteration 1177, loss = 0.12219009\n",
      "Iteration 1178, loss = 0.12831432\n",
      "Iteration 1179, loss = 0.11839401\n",
      "Iteration 1180, loss = 0.12484530\n",
      "Iteration 1181, loss = 0.13913711\n",
      "Iteration 1182, loss = 0.12716534\n",
      "Iteration 1183, loss = 0.12436228\n",
      "Iteration 1184, loss = 0.13665893\n",
      "Iteration 1185, loss = 0.11382022\n",
      "Iteration 1186, loss = 0.13019110\n",
      "Iteration 1187, loss = 0.12174940\n",
      "Iteration 1188, loss = 0.11322862\n",
      "Iteration 1189, loss = 0.12506754\n",
      "Iteration 1190, loss = 0.11548342\n",
      "Iteration 1191, loss = 0.12885546\n",
      "Iteration 1192, loss = 0.11721410\n",
      "Iteration 1193, loss = 0.12643361\n",
      "Iteration 1194, loss = 0.11892670\n",
      "Iteration 1195, loss = 0.11334874\n",
      "Iteration 1196, loss = 0.13884920\n",
      "Iteration 1197, loss = 0.13504492\n",
      "Iteration 1198, loss = 0.13094052\n",
      "Iteration 1199, loss = 0.12637993\n",
      "Iteration 1200, loss = 0.12693784\n",
      "Iteration 1201, loss = 0.11399327\n",
      "Iteration 1202, loss = 0.11584413\n",
      "Iteration 1203, loss = 0.11989354\n",
      "Iteration 1204, loss = 0.11982711\n",
      "Iteration 1205, loss = 0.12496011\n",
      "Iteration 1206, loss = 0.11775605\n",
      "Iteration 1207, loss = 0.12185723\n",
      "Iteration 1208, loss = 0.12036590\n",
      "Iteration 1209, loss = 0.12484561\n",
      "Iteration 1210, loss = 0.13661719\n",
      "Iteration 1211, loss = 0.12644679\n",
      "Iteration 1212, loss = 0.17055889\n",
      "Iteration 1213, loss = 0.12496226\n",
      "Iteration 1214, loss = 0.11767211\n",
      "Iteration 1215, loss = 0.12137241\n",
      "Iteration 1216, loss = 0.12079695\n",
      "Iteration 1217, loss = 0.15876360\n",
      "Iteration 1218, loss = 0.13840197\n",
      "Iteration 1219, loss = 0.12467524\n",
      "Iteration 1220, loss = 0.13181528\n",
      "Iteration 1221, loss = 0.13829405\n",
      "Iteration 1222, loss = 0.16165993\n",
      "Iteration 1223, loss = 0.11127198\n",
      "Iteration 1224, loss = 0.12130931\n",
      "Iteration 1225, loss = 0.12926400\n",
      "Iteration 1226, loss = 0.11998747\n",
      "Iteration 1227, loss = 0.11902004\n",
      "Iteration 1228, loss = 0.11306267\n",
      "Iteration 1229, loss = 0.11912761\n",
      "Iteration 1230, loss = 0.11979087\n",
      "Iteration 1231, loss = 0.11237955\n",
      "Iteration 1232, loss = 0.11638416\n",
      "Iteration 1233, loss = 0.11189502\n",
      "Iteration 1234, loss = 0.12517999\n",
      "Iteration 1235, loss = 0.12861172\n",
      "Iteration 1236, loss = 0.19723268\n",
      "Iteration 1237, loss = 0.23450242\n",
      "Iteration 1238, loss = 0.18949142\n",
      "Iteration 1239, loss = 0.19150531\n",
      "Iteration 1240, loss = 0.29749516\n",
      "Iteration 1241, loss = 0.27170259\n",
      "Iteration 1242, loss = 0.28768584\n",
      "Iteration 1243, loss = 0.46888736\n",
      "Iteration 1244, loss = 0.29612615\n",
      "Iteration 1245, loss = 0.84957246\n",
      "Iteration 1246, loss = 0.65835496\n",
      "Iteration 1247, loss = 0.71162793\n",
      "Iteration 1248, loss = 0.52131602\n",
      "Iteration 1249, loss = 0.24404928\n",
      "Iteration 1250, loss = 0.27929984\n",
      "Iteration 1251, loss = 0.26207027\n",
      "Iteration 1252, loss = 0.24109332\n",
      "Iteration 1253, loss = 0.33463136\n",
      "Iteration 1254, loss = 0.37957004\n",
      "Iteration 1255, loss = 0.57759240\n",
      "Iteration 1256, loss = 0.52406791\n",
      "Iteration 1257, loss = 0.73994068\n",
      "Iteration 1258, loss = 0.70430202\n",
      "Iteration 1259, loss = 4.54362685\n",
      "Iteration 1260, loss = 1.01396158\n",
      "Iteration 1261, loss = 1.71878911\n",
      "Iteration 1262, loss = 4.69132913\n",
      "Iteration 1263, loss = 2.90246227\n",
      "Iteration 1264, loss = 1.52074424\n",
      "Iteration 1265, loss = 1.01487980\n",
      "Iteration 1266, loss = 1.52586347\n",
      "Iteration 1267, loss = 1.17133553\n",
      "Iteration 1268, loss = 0.99308816\n",
      "Iteration 1269, loss = 1.40740792\n",
      "Iteration 1270, loss = 0.73861149\n",
      "Iteration 1271, loss = 1.03052733\n",
      "Iteration 1272, loss = 0.82763516\n",
      "Iteration 1273, loss = 0.79254704\n",
      "Iteration 1274, loss = 0.79884217\n",
      "Iteration 1275, loss = 0.62059438\n",
      "Iteration 1276, loss = 0.70779665\n",
      "Iteration 1277, loss = 0.98410074\n",
      "Iteration 1278, loss = 0.86737926\n",
      "Iteration 1279, loss = 0.78239326\n",
      "Iteration 1280, loss = 0.71009656\n",
      "Iteration 1281, loss = 0.51716182\n",
      "Iteration 1282, loss = 0.40355443\n",
      "Iteration 1283, loss = 0.21192628\n",
      "Iteration 1284, loss = 0.54629940\n",
      "Iteration 1285, loss = 0.43116506\n",
      "Iteration 1286, loss = 0.42083649\n",
      "Iteration 1287, loss = 0.39460511\n",
      "Iteration 1288, loss = 0.29303310\n",
      "Iteration 1289, loss = 0.19155658\n",
      "Iteration 1290, loss = 0.24252177\n",
      "Iteration 1291, loss = 0.19884391\n",
      "Iteration 1292, loss = 0.15437598\n",
      "Iteration 1293, loss = 0.21016820\n",
      "Iteration 1294, loss = 0.18598014\n",
      "Iteration 1295, loss = 0.13533183\n",
      "Iteration 1296, loss = 0.17297731\n",
      "Iteration 1297, loss = 0.16894985\n",
      "Iteration 1298, loss = 0.36253352\n",
      "Iteration 1299, loss = 0.27865702\n",
      "Iteration 1300, loss = 0.26456136\n",
      "Iteration 1301, loss = 0.20711944\n",
      "Iteration 1302, loss = 0.14438390\n",
      "Iteration 1303, loss = 0.22622963\n",
      "Iteration 1304, loss = 0.25995460\n",
      "Iteration 1305, loss = 0.17471671\n",
      "Iteration 1306, loss = 0.17587879\n",
      "Iteration 1307, loss = 0.17052445\n",
      "Iteration 1308, loss = 0.13333614\n",
      "Iteration 1309, loss = 0.17697403\n",
      "Iteration 1310, loss = 0.32305442\n",
      "Iteration 1311, loss = 0.52169876\n",
      "Iteration 1312, loss = 0.27015724\n",
      "Iteration 1313, loss = 0.23116756\n",
      "Iteration 1314, loss = 0.38560871\n",
      "Iteration 1315, loss = 0.25410289\n",
      "Iteration 1316, loss = 0.16631327\n",
      "Iteration 1317, loss = 0.28518359\n",
      "Iteration 1318, loss = 0.18729575\n",
      "Iteration 1319, loss = 0.14895181\n",
      "Iteration 1320, loss = 0.25246424\n",
      "Iteration 1321, loss = 0.16800115\n",
      "Iteration 1322, loss = 0.19791775\n",
      "Iteration 1323, loss = 0.22635513\n",
      "Iteration 1324, loss = 0.19991686\n",
      "Iteration 1325, loss = 0.13728976\n",
      "Iteration 1326, loss = 0.22884980\n",
      "Iteration 1327, loss = 0.13804356\n",
      "Iteration 1328, loss = 0.15765894\n",
      "Iteration 1329, loss = 0.14503623\n",
      "Iteration 1330, loss = 0.23240664\n",
      "Iteration 1331, loss = 0.16824998\n",
      "Iteration 1332, loss = 0.14334539\n",
      "Iteration 1333, loss = 0.13877065\n",
      "Iteration 1334, loss = 0.15086600\n",
      "Iteration 1335, loss = 0.17439648\n",
      "Iteration 1336, loss = 0.27564358\n",
      "Iteration 1337, loss = 0.26591048\n",
      "Iteration 1338, loss = 0.15443919\n",
      "Iteration 1339, loss = 0.12931398\n",
      "Iteration 1340, loss = 0.12058101\n",
      "Iteration 1341, loss = 0.14788960\n",
      "Iteration 1342, loss = 0.13133569\n",
      "Iteration 1343, loss = 0.14606708\n",
      "Iteration 1344, loss = 0.20666058\n",
      "Iteration 1345, loss = 0.19304258\n",
      "Iteration 1346, loss = 0.19515821\n",
      "Iteration 1347, loss = 0.14128405\n",
      "Iteration 1348, loss = 0.16052519\n",
      "Iteration 1349, loss = 0.16466131\n",
      "Iteration 1350, loss = 0.13880607\n",
      "Iteration 1351, loss = 0.14985076\n",
      "Iteration 1352, loss = 0.12871631\n",
      "Iteration 1353, loss = 0.14382889\n",
      "Iteration 1354, loss = 0.12993536\n",
      "Iteration 1355, loss = 0.13922407\n",
      "Iteration 1356, loss = 0.12903549\n",
      "Iteration 1357, loss = 0.12186248\n",
      "Iteration 1358, loss = 0.13046618\n",
      "Iteration 1359, loss = 0.16006526\n",
      "Iteration 1360, loss = 0.12569587\n",
      "Iteration 1361, loss = 0.15660468\n",
      "Iteration 1362, loss = 0.13789643\n",
      "Iteration 1363, loss = 0.12910196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1364, loss = 0.14400558\n",
      "Iteration 1365, loss = 0.20446772\n",
      "Iteration 1366, loss = 0.26953517\n",
      "Iteration 1367, loss = 0.15313832\n",
      "Iteration 1368, loss = 0.12942118\n",
      "Iteration 1369, loss = 0.14894108\n",
      "Iteration 1370, loss = 0.12705760\n",
      "Iteration 1371, loss = 0.12146445\n",
      "Iteration 1372, loss = 0.11537852\n",
      "Iteration 1373, loss = 0.12172954\n",
      "Iteration 1374, loss = 0.12291224\n",
      "Iteration 1375, loss = 0.16145101\n",
      "Iteration 1376, loss = 0.13285950\n",
      "Iteration 1377, loss = 0.12609912\n",
      "Iteration 1378, loss = 0.12202535\n",
      "Iteration 1379, loss = 0.11859343\n",
      "Iteration 1380, loss = 0.11327291\n",
      "Iteration 1381, loss = 0.12240617\n",
      "Iteration 1382, loss = 0.11606575\n",
      "Iteration 1383, loss = 0.12218778\n",
      "Iteration 1384, loss = 0.12060175\n",
      "Iteration 1385, loss = 0.13508975\n",
      "Iteration 1386, loss = 0.12371069\n",
      "Iteration 1387, loss = 0.15043431\n",
      "Iteration 1388, loss = 0.13148471\n",
      "Iteration 1389, loss = 0.13077549\n",
      "Iteration 1390, loss = 0.11657871\n",
      "Iteration 1391, loss = 0.11426293\n",
      "Iteration 1392, loss = 0.12373437\n",
      "Iteration 1393, loss = 0.14314875\n",
      "Iteration 1394, loss = 0.12992777\n",
      "Iteration 1395, loss = 0.12832193\n",
      "Iteration 1396, loss = 0.12492343\n",
      "Iteration 1397, loss = 0.11891870\n",
      "Iteration 1398, loss = 0.11988745\n",
      "Iteration 1399, loss = 0.12145941\n",
      "Iteration 1400, loss = 0.12812536\n",
      "Iteration 1401, loss = 0.15779584\n",
      "Iteration 1402, loss = 0.13552005\n",
      "Iteration 1403, loss = 0.19177413\n",
      "Iteration 1404, loss = 0.15277242\n",
      "Iteration 1405, loss = 0.12900481\n",
      "Iteration 1406, loss = 0.14738195\n",
      "Iteration 1407, loss = 0.11677055\n",
      "Iteration 1408, loss = 0.11410288\n",
      "Iteration 1409, loss = 0.11656303\n",
      "Iteration 1410, loss = 0.11654303\n",
      "Iteration 1411, loss = 0.11788429\n",
      "Iteration 1412, loss = 0.11950571\n",
      "Iteration 1413, loss = 0.11498347\n",
      "Iteration 1414, loss = 0.12194595\n",
      "Iteration 1415, loss = 0.13153626\n",
      "Iteration 1416, loss = 0.16681380\n",
      "Iteration 1417, loss = 0.12640720\n",
      "Iteration 1418, loss = 0.12259850\n",
      "Iteration 1419, loss = 0.15174023\n",
      "Iteration 1420, loss = 0.12136262\n",
      "Iteration 1421, loss = 0.12923648\n",
      "Iteration 1422, loss = 0.11959900\n",
      "Iteration 1423, loss = 0.12733074\n",
      "Iteration 1424, loss = 0.11746479\n",
      "Iteration 1425, loss = 0.11291183\n",
      "Iteration 1426, loss = 0.11714943\n",
      "Iteration 1427, loss = 0.12251645\n",
      "Iteration 1428, loss = 0.12039413\n",
      "Iteration 1429, loss = 0.13350909\n",
      "Iteration 1430, loss = 0.21127800\n",
      "Iteration 1431, loss = 0.20008077\n",
      "Iteration 1432, loss = 0.32991237\n",
      "Iteration 1433, loss = 0.42719144\n",
      "Iteration 1434, loss = 0.29739513\n",
      "Iteration 1435, loss = 0.19616672\n",
      "Iteration 1436, loss = 0.33245204\n",
      "Iteration 1437, loss = 0.41525734\n",
      "Iteration 1438, loss = 0.31858915\n",
      "Iteration 1439, loss = 0.14934055\n",
      "Iteration 1440, loss = 0.15508107\n",
      "Iteration 1441, loss = 0.22232815\n",
      "Iteration 1442, loss = 0.17571736\n",
      "Iteration 1443, loss = 0.13372742\n",
      "Iteration 1444, loss = 0.12675986\n",
      "Iteration 1445, loss = 0.12261587\n",
      "Iteration 1446, loss = 0.11450560\n",
      "Iteration 1447, loss = 0.13050842\n",
      "Iteration 1448, loss = 0.12515343\n",
      "Iteration 1449, loss = 0.11730043\n",
      "Iteration 1450, loss = 0.11809051\n",
      "Iteration 1451, loss = 0.11602902\n",
      "Iteration 1452, loss = 0.11316399\n",
      "Iteration 1453, loss = 0.11421651\n",
      "Iteration 1454, loss = 0.13459135\n",
      "Iteration 1455, loss = 0.14677532\n",
      "Iteration 1456, loss = 0.12902800\n",
      "Iteration 1457, loss = 0.12530551\n",
      "Iteration 1458, loss = 0.13407531\n",
      "Iteration 1459, loss = 0.13168126\n",
      "Iteration 1460, loss = 0.12573220\n",
      "Iteration 1461, loss = 0.13462228\n",
      "Iteration 1462, loss = 0.13986026\n",
      "Iteration 1463, loss = 0.11081949\n",
      "Iteration 1464, loss = 0.13912217\n",
      "Iteration 1465, loss = 0.16046574\n",
      "Iteration 1466, loss = 0.12547157\n",
      "Iteration 1467, loss = 0.18564232\n",
      "Iteration 1468, loss = 0.13811849\n",
      "Iteration 1469, loss = 0.13405682\n",
      "Iteration 1470, loss = 0.19813731\n",
      "Iteration 1471, loss = 0.13047168\n",
      "Iteration 1472, loss = 0.12294944\n",
      "Iteration 1473, loss = 0.12389608\n",
      "Iteration 1474, loss = 0.12218389\n",
      "Iteration 1475, loss = 0.12350635\n",
      "Iteration 1476, loss = 0.13195562\n",
      "Iteration 1477, loss = 0.12786277\n",
      "Iteration 1478, loss = 0.11255668\n",
      "Iteration 1479, loss = 0.13155953\n",
      "Iteration 1480, loss = 0.12137316\n",
      "Iteration 1481, loss = 0.11774573\n",
      "Iteration 1482, loss = 0.11777989\n",
      "Iteration 1483, loss = 0.11498341\n",
      "Iteration 1484, loss = 0.14727470\n",
      "Iteration 1485, loss = 0.16932150\n",
      "Iteration 1486, loss = 0.16013949\n",
      "Iteration 1487, loss = 0.12198901\n",
      "Iteration 1488, loss = 0.15366014\n",
      "Iteration 1489, loss = 0.13894080\n",
      "Iteration 1490, loss = 0.13688929\n",
      "Iteration 1491, loss = 0.19831456\n",
      "Iteration 1492, loss = 0.16579398\n",
      "Iteration 1493, loss = 0.11785711\n",
      "Iteration 1494, loss = 0.12060601\n",
      "Iteration 1495, loss = 0.11938836\n",
      "Iteration 1496, loss = 0.12904858\n",
      "Iteration 1497, loss = 0.12272891\n",
      "Iteration 1498, loss = 0.11897402\n",
      "Iteration 1499, loss = 0.13392870\n",
      "Iteration 1500, loss = 0.13159918\n",
      "Iteration 1501, loss = 0.11379746\n",
      "Iteration 1502, loss = 0.12653129\n",
      "Iteration 1503, loss = 0.11791042\n",
      "Iteration 1504, loss = 0.12091793\n",
      "Iteration 1505, loss = 0.11609861\n",
      "Iteration 1506, loss = 0.11495087\n",
      "Iteration 1507, loss = 0.11373799\n",
      "Iteration 1508, loss = 0.11729688\n",
      "Iteration 1509, loss = 0.12714615\n",
      "Iteration 1510, loss = 0.14213844\n",
      "Iteration 1511, loss = 0.12182231\n",
      "Iteration 1512, loss = 0.11747728\n",
      "Iteration 1513, loss = 0.11478652\n",
      "Iteration 1514, loss = 0.11480719\n",
      "Iteration 1515, loss = 0.12116905\n",
      "Iteration 1516, loss = 0.11536485\n",
      "Iteration 1517, loss = 0.11471448\n",
      "Iteration 1518, loss = 0.11399066\n",
      "Iteration 1519, loss = 0.11802595\n",
      "Iteration 1520, loss = 0.12158346\n",
      "Iteration 1521, loss = 0.11226435\n",
      "Iteration 1522, loss = 0.11509153\n",
      "Iteration 1523, loss = 0.11787053\n",
      "Iteration 1524, loss = 0.11680461\n",
      "Iteration 1525, loss = 0.11400964\n",
      "Iteration 1526, loss = 0.11218913\n",
      "Iteration 1527, loss = 0.11048730\n",
      "Iteration 1528, loss = 0.11386395\n",
      "Iteration 1529, loss = 0.11358125\n",
      "Iteration 1530, loss = 0.11753925\n",
      "Iteration 1531, loss = 0.12108857\n",
      "Iteration 1532, loss = 0.11715344\n",
      "Iteration 1533, loss = 0.11282804\n",
      "Iteration 1534, loss = 0.12064824\n",
      "Iteration 1535, loss = 0.11434301\n",
      "Iteration 1536, loss = 0.12350460\n",
      "Iteration 1537, loss = 0.12999237\n",
      "Iteration 1538, loss = 0.12148610\n",
      "Iteration 1539, loss = 0.11804746\n",
      "Iteration 1540, loss = 0.11519549\n",
      "Iteration 1541, loss = 0.12604746\n",
      "Iteration 1542, loss = 0.11650403\n",
      "Iteration 1543, loss = 0.12202435\n",
      "Iteration 1544, loss = 0.11540965\n",
      "Iteration 1545, loss = 0.15053700\n",
      "Iteration 1546, loss = 0.14099339\n",
      "Iteration 1547, loss = 0.13610803\n",
      "Iteration 1548, loss = 0.13085007\n",
      "Iteration 1549, loss = 0.13320728\n",
      "Iteration 1550, loss = 0.13475111\n",
      "Iteration 1551, loss = 0.16532913\n",
      "Iteration 1552, loss = 0.13696839\n",
      "Iteration 1553, loss = 0.17029814\n",
      "Iteration 1554, loss = 0.18678469\n",
      "Iteration 1555, loss = 0.13905362\n",
      "Iteration 1556, loss = 0.13644183\n",
      "Iteration 1557, loss = 0.12494611\n",
      "Iteration 1558, loss = 0.12859848\n",
      "Iteration 1559, loss = 0.12894027\n",
      "Iteration 1560, loss = 0.14326837\n",
      "Iteration 1561, loss = 0.14102726\n",
      "Iteration 1562, loss = 0.11955286\n",
      "Iteration 1563, loss = 0.11909148\n",
      "Iteration 1564, loss = 0.17287999\n",
      "Iteration 1565, loss = 0.12383958\n",
      "Iteration 1566, loss = 0.11627962\n",
      "Iteration 1567, loss = 0.12018975\n",
      "Iteration 1568, loss = 0.12649939\n",
      "Iteration 1569, loss = 0.11981758\n",
      "Iteration 1570, loss = 0.11553087\n",
      "Iteration 1571, loss = 0.12344181\n",
      "Iteration 1572, loss = 0.13975647\n",
      "Iteration 1573, loss = 0.14946285\n",
      "Iteration 1574, loss = 0.11808893\n",
      "Iteration 1575, loss = 0.12615254\n",
      "Iteration 1576, loss = 0.12585823\n",
      "Iteration 1577, loss = 0.11556065\n",
      "Iteration 1578, loss = 0.14672053\n",
      "Iteration 1579, loss = 0.20729225\n",
      "Iteration 1580, loss = 0.19299271\n",
      "Iteration 1581, loss = 0.22556236\n",
      "Iteration 1582, loss = 0.37529663\n",
      "Iteration 1583, loss = 0.56793690\n",
      "Iteration 1584, loss = 0.56657894\n",
      "Iteration 1585, loss = 0.63867770\n",
      "Iteration 1586, loss = 0.62267446\n",
      "Iteration 1587, loss = 0.43939196\n",
      "Iteration 1588, loss = 0.51801819\n",
      "Iteration 1589, loss = 0.37497098\n",
      "Iteration 1590, loss = 0.28385674\n",
      "Iteration 1591, loss = 0.31697914\n",
      "Iteration 1592, loss = 0.21302379\n",
      "Iteration 1593, loss = 0.18491651\n",
      "Iteration 1594, loss = 0.23539288\n",
      "Iteration 1595, loss = 0.29328693\n",
      "Iteration 1596, loss = 0.13403876\n",
      "Iteration 1597, loss = 0.14214905\n",
      "Iteration 1598, loss = 0.19772681\n",
      "Iteration 1599, loss = 0.25313870\n",
      "Iteration 1600, loss = 0.24816949\n",
      "Iteration 1601, loss = 0.17350680\n",
      "Iteration 1602, loss = 0.14296582\n",
      "Iteration 1603, loss = 0.13896809\n",
      "Iteration 1604, loss = 0.12057938\n",
      "Iteration 1605, loss = 0.12442848\n",
      "Iteration 1606, loss = 0.11701931\n",
      "Iteration 1607, loss = 0.11909766\n",
      "Iteration 1608, loss = 0.13029691\n",
      "Iteration 1609, loss = 0.13084927\n",
      "Iteration 1610, loss = 0.11629432\n",
      "Iteration 1611, loss = 0.13977271\n",
      "Iteration 1612, loss = 0.17146795\n",
      "Iteration 1613, loss = 0.15647529\n",
      "Iteration 1614, loss = 0.11561805\n",
      "Iteration 1615, loss = 0.12476701\n",
      "Iteration 1616, loss = 0.11758313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1617, loss = 0.11113839\n",
      "Iteration 1618, loss = 0.13555640\n",
      "Iteration 1619, loss = 0.11787880\n",
      "Iteration 1620, loss = 0.11998205\n",
      "Iteration 1621, loss = 0.11345359\n",
      "Iteration 1622, loss = 0.11463910\n",
      "Iteration 1623, loss = 0.11536309\n",
      "Iteration 1624, loss = 0.11483778\n",
      "Iteration 1625, loss = 0.11805767\n",
      "Iteration 1626, loss = 0.14109884\n",
      "Iteration 1627, loss = 0.13044825\n",
      "Iteration 1628, loss = 0.11303253\n",
      "Iteration 1629, loss = 0.11229228\n",
      "Iteration 1630, loss = 0.11374352\n",
      "Iteration 1631, loss = 0.11421812\n",
      "Iteration 1632, loss = 0.11242667\n",
      "Iteration 1633, loss = 0.11662736\n",
      "Iteration 1634, loss = 0.11467306\n",
      "Iteration 1635, loss = 0.12036413\n",
      "Iteration 1636, loss = 0.11815346\n",
      "Iteration 1637, loss = 0.11191797\n",
      "Iteration 1638, loss = 0.11784566\n",
      "Iteration 1639, loss = 0.12465848\n",
      "Iteration 1640, loss = 0.12245771\n",
      "Iteration 1641, loss = 0.11528883\n",
      "Iteration 1642, loss = 0.12017034\n",
      "Iteration 1643, loss = 0.12291249\n",
      "Iteration 1644, loss = 0.10897112\n",
      "Iteration 1645, loss = 0.12337341\n",
      "Iteration 1646, loss = 0.11755355\n",
      "Iteration 1647, loss = 0.11242411\n",
      "Iteration 1648, loss = 0.12644637\n",
      "Iteration 1649, loss = 0.12269806\n",
      "Iteration 1650, loss = 0.12697954\n",
      "Iteration 1651, loss = 0.13057339\n",
      "Iteration 1652, loss = 0.12070407\n",
      "Iteration 1653, loss = 0.11278277\n",
      "Iteration 1654, loss = 0.11462479\n",
      "Iteration 1655, loss = 0.11492838\n",
      "Iteration 1656, loss = 0.11622386\n",
      "Iteration 1657, loss = 0.13047304\n",
      "Iteration 1658, loss = 0.12469916\n",
      "Iteration 1659, loss = 0.13616461\n",
      "Iteration 1660, loss = 0.12667379\n",
      "Iteration 1661, loss = 0.12574007\n",
      "Iteration 1662, loss = 0.14635778\n",
      "Iteration 1663, loss = 0.16142541\n",
      "Iteration 1664, loss = 0.14353599\n",
      "Iteration 1665, loss = 0.17874064\n",
      "Iteration 1666, loss = 0.12547139\n",
      "Iteration 1667, loss = 0.21668838\n",
      "Iteration 1668, loss = 0.19491273\n",
      "Iteration 1669, loss = 0.14681440\n",
      "Iteration 1670, loss = 0.18258368\n",
      "Iteration 1671, loss = 0.13252053\n",
      "Iteration 1672, loss = 0.12287608\n",
      "Iteration 1673, loss = 0.12910583\n",
      "Iteration 1674, loss = 0.13872418\n",
      "Iteration 1675, loss = 0.11242210\n",
      "Iteration 1676, loss = 0.13435661\n",
      "Iteration 1677, loss = 0.11813302\n",
      "Iteration 1678, loss = 0.15909010\n",
      "Iteration 1679, loss = 0.13181456\n",
      "Iteration 1680, loss = 0.15292504\n",
      "Iteration 1681, loss = 0.14797096\n",
      "Iteration 1682, loss = 0.11116916\n",
      "Iteration 1683, loss = 0.13037839\n",
      "Iteration 1684, loss = 0.16326075\n",
      "Iteration 1685, loss = 0.13145455\n",
      "Iteration 1686, loss = 0.11539824\n",
      "Iteration 1687, loss = 0.12729917\n",
      "Iteration 1688, loss = 0.14130109\n",
      "Iteration 1689, loss = 0.13911142\n",
      "Iteration 1690, loss = 0.12901719\n",
      "Iteration 1691, loss = 0.12232045\n",
      "Iteration 1692, loss = 0.11669503\n",
      "Iteration 1693, loss = 0.14875773\n",
      "Iteration 1694, loss = 0.13016866\n",
      "Iteration 1695, loss = 0.13603728\n",
      "Iteration 1696, loss = 0.15249494\n",
      "Iteration 1697, loss = 0.12658917\n",
      "Iteration 1698, loss = 0.11051234\n",
      "Iteration 1699, loss = 0.12014044\n",
      "Iteration 1700, loss = 0.12777280\n",
      "Iteration 1701, loss = 0.15184635\n",
      "Iteration 1702, loss = 0.14780213\n",
      "Iteration 1703, loss = 0.11045919\n",
      "Iteration 1704, loss = 0.12847566\n",
      "Iteration 1705, loss = 0.11778971\n",
      "Iteration 1706, loss = 0.14744187\n",
      "Iteration 1707, loss = 0.13239043\n",
      "Iteration 1708, loss = 0.15665398\n",
      "Iteration 1709, loss = 0.11963254\n",
      "Iteration 1710, loss = 0.11900018\n",
      "Iteration 1711, loss = 0.24801452\n",
      "Iteration 1712, loss = 0.26997730\n",
      "Iteration 1713, loss = 0.25515857\n",
      "Iteration 1714, loss = 0.26653611\n",
      "Iteration 1715, loss = 0.51108872\n",
      "Iteration 1716, loss = 1.02181333\n",
      "Iteration 1717, loss = 0.82996147\n",
      "Iteration 1718, loss = 0.68493078\n",
      "Iteration 1719, loss = 0.74325164\n",
      "Iteration 1720, loss = 0.54974123\n",
      "Iteration 1721, loss = 1.97445463\n",
      "Iteration 1722, loss = 0.64962222\n",
      "Iteration 1723, loss = 1.19720565\n",
      "Iteration 1724, loss = 1.40311092\n",
      "Iteration 1725, loss = 1.43624946\n",
      "Iteration 1726, loss = 1.57699312\n",
      "Iteration 1727, loss = 1.42456441\n",
      "Iteration 1728, loss = 1.74998572\n",
      "Iteration 1729, loss = 0.97394806\n",
      "Iteration 1730, loss = 0.86489928\n",
      "Iteration 1731, loss = 0.74328249\n",
      "Iteration 1732, loss = 1.05463726\n",
      "Iteration 1733, loss = 0.81738636\n",
      "Iteration 1734, loss = 0.75059135\n",
      "Iteration 1735, loss = 0.71175273\n",
      "Iteration 1736, loss = 0.70511516\n",
      "Iteration 1737, loss = 0.45597482\n",
      "Iteration 1738, loss = 0.30580379\n",
      "Iteration 1739, loss = 0.30088650\n",
      "Iteration 1740, loss = 0.21123064\n",
      "Iteration 1741, loss = 0.26745885\n",
      "Iteration 1742, loss = 0.28101362\n",
      "Iteration 1743, loss = 0.38574876\n",
      "Iteration 1744, loss = 0.28494179\n",
      "Iteration 1745, loss = 0.35091351\n",
      "Iteration 1746, loss = 0.16347649\n",
      "Iteration 1747, loss = 0.21485473\n",
      "Iteration 1748, loss = 0.32253604\n",
      "Iteration 1749, loss = 0.23785463\n",
      "Iteration 1750, loss = 0.40825845\n",
      "Iteration 1751, loss = 0.41014769\n",
      "Iteration 1752, loss = 0.31513839\n",
      "Iteration 1753, loss = 0.50416693\n",
      "Iteration 1754, loss = 0.41267398\n",
      "Iteration 1755, loss = 0.44697655\n",
      "Iteration 1756, loss = 0.25438751\n",
      "Iteration 1757, loss = 0.21492194\n",
      "Iteration 1758, loss = 0.16558264\n",
      "Iteration 1759, loss = 0.15307649\n",
      "Iteration 1760, loss = 0.13622099\n",
      "Iteration 1761, loss = 0.12693845\n",
      "Iteration 1762, loss = 0.12493714\n",
      "Iteration 1763, loss = 0.11461332\n",
      "Iteration 1764, loss = 0.12173930\n",
      "Iteration 1765, loss = 0.11817645\n",
      "Iteration 1766, loss = 0.11673501\n",
      "Iteration 1767, loss = 0.12730450\n",
      "Iteration 1768, loss = 0.11164526\n",
      "Iteration 1769, loss = 0.11981820\n",
      "Iteration 1770, loss = 0.12110794\n",
      "Iteration 1771, loss = 0.12700995\n",
      "Iteration 1772, loss = 0.14350082\n",
      "Iteration 1773, loss = 0.11369691\n",
      "Iteration 1774, loss = 0.13133997\n",
      "Iteration 1775, loss = 0.13116616\n",
      "Iteration 1776, loss = 0.12002973\n",
      "Iteration 1777, loss = 0.12112945\n",
      "Iteration 1778, loss = 0.12925797\n",
      "Iteration 1779, loss = 0.16712273\n",
      "Iteration 1780, loss = 0.15806825\n",
      "Iteration 1781, loss = 0.11353180\n",
      "Iteration 1782, loss = 0.12380996\n",
      "Iteration 1783, loss = 0.12226118\n",
      "Iteration 1784, loss = 0.11423944\n",
      "Iteration 1785, loss = 0.11939035\n",
      "Iteration 1786, loss = 0.12221962\n",
      "Iteration 1787, loss = 0.11451727\n",
      "Iteration 1788, loss = 0.12878817\n",
      "Iteration 1789, loss = 0.11925233\n",
      "Iteration 1790, loss = 0.11134200\n",
      "Iteration 1791, loss = 0.11783708\n",
      "Iteration 1792, loss = 0.11802910\n",
      "Iteration 1793, loss = 0.11380253\n",
      "Iteration 1794, loss = 0.11503635\n",
      "Iteration 1795, loss = 0.11187160\n",
      "Iteration 1796, loss = 0.11918095\n",
      "Iteration 1797, loss = 0.11008764\n",
      "Iteration 1798, loss = 0.11975311\n",
      "Iteration 1799, loss = 0.11725031\n",
      "Iteration 1800, loss = 0.11241751\n",
      "Iteration 1801, loss = 0.11508205\n",
      "Iteration 1802, loss = 0.11134111\n",
      "Iteration 1803, loss = 0.12004134\n",
      "Iteration 1804, loss = 0.16512618\n",
      "Iteration 1805, loss = 0.23146367\n",
      "Iteration 1806, loss = 0.15745418\n",
      "Iteration 1807, loss = 0.15241344\n",
      "Iteration 1808, loss = 0.15422169\n",
      "Iteration 1809, loss = 0.12511929\n",
      "Iteration 1810, loss = 0.12387428\n",
      "Iteration 1811, loss = 0.14388571\n",
      "Iteration 1812, loss = 0.13986116\n",
      "Iteration 1813, loss = 0.12393564\n",
      "Iteration 1814, loss = 0.11542732\n",
      "Iteration 1815, loss = 0.12235171\n",
      "Iteration 1816, loss = 0.11388641\n",
      "Iteration 1817, loss = 0.12236201\n",
      "Iteration 1818, loss = 0.11309087\n",
      "Iteration 1819, loss = 0.11627917\n",
      "Iteration 1820, loss = 0.11599914\n",
      "Iteration 1821, loss = 0.11464160\n",
      "Iteration 1822, loss = 0.11945236\n",
      "Iteration 1823, loss = 0.11520246\n",
      "Iteration 1824, loss = 0.12194065\n",
      "Iteration 1825, loss = 0.11492034\n",
      "Iteration 1826, loss = 0.11670276\n",
      "Iteration 1827, loss = 0.13244117\n",
      "Iteration 1828, loss = 0.13843795\n",
      "Iteration 1829, loss = 0.14547729\n",
      "Iteration 1830, loss = 0.11325415\n",
      "Iteration 1831, loss = 0.13025070\n",
      "Iteration 1832, loss = 0.12631778\n",
      "Iteration 1833, loss = 0.11171044\n",
      "Iteration 1834, loss = 0.12135467\n",
      "Iteration 1835, loss = 0.11620922\n",
      "Iteration 1836, loss = 0.11564275\n",
      "Iteration 1837, loss = 0.12388399\n",
      "Iteration 1838, loss = 0.12746780\n",
      "Iteration 1839, loss = 0.12325821\n",
      "Iteration 1840, loss = 0.12737549\n",
      "Iteration 1841, loss = 0.14546485\n",
      "Iteration 1842, loss = 0.11828441\n",
      "Iteration 1843, loss = 0.11513646\n",
      "Iteration 1844, loss = 0.12600090\n",
      "Iteration 1845, loss = 0.12060266\n",
      "Iteration 1846, loss = 0.13224091\n",
      "Iteration 1847, loss = 0.12780544\n",
      "Iteration 1848, loss = 0.11736774\n",
      "Iteration 1849, loss = 0.11573870\n",
      "Iteration 1850, loss = 0.11679494\n",
      "Iteration 1851, loss = 0.13419959\n",
      "Iteration 1852, loss = 0.12792858\n",
      "Iteration 1853, loss = 0.12054815\n",
      "Iteration 1854, loss = 0.11613357\n",
      "Iteration 1855, loss = 0.14042617\n",
      "Iteration 1856, loss = 0.11699683\n",
      "Iteration 1857, loss = 0.12591101\n",
      "Iteration 1858, loss = 0.13074678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1859, loss = 0.11095969\n",
      "Iteration 1860, loss = 0.11423827\n",
      "Iteration 1861, loss = 0.12399108\n",
      "Iteration 1862, loss = 0.12724053\n",
      "Iteration 1863, loss = 0.12618662\n",
      "Iteration 1864, loss = 0.12555101\n",
      "Iteration 1865, loss = 0.12092272\n",
      "Iteration 1866, loss = 0.17557511\n",
      "Iteration 1867, loss = 0.38978370\n",
      "Iteration 1868, loss = 0.34614317\n",
      "Iteration 1869, loss = 0.19461802\n",
      "Iteration 1870, loss = 0.17397399\n",
      "Iteration 1871, loss = 0.14915561\n",
      "Iteration 1872, loss = 0.23157867\n",
      "Iteration 1873, loss = 0.68231636\n",
      "Iteration 1874, loss = 0.53504386\n",
      "Iteration 1875, loss = 0.55500374\n",
      "Iteration 1876, loss = 0.58121722\n",
      "Iteration 1877, loss = 0.38643883\n",
      "Iteration 1878, loss = 0.19568630\n",
      "Iteration 1879, loss = 0.18585835\n",
      "Iteration 1880, loss = 0.34440067\n",
      "Iteration 1881, loss = 0.39635333\n",
      "Iteration 1882, loss = 0.27104865\n",
      "Iteration 1883, loss = 0.28863917\n",
      "Iteration 1884, loss = 0.16232773\n",
      "Iteration 1885, loss = 0.17147865\n",
      "Iteration 1886, loss = 0.14957681\n",
      "Iteration 1887, loss = 0.12399813\n",
      "Iteration 1888, loss = 0.15156103\n",
      "Iteration 1889, loss = 0.11658691\n",
      "Iteration 1890, loss = 0.12702572\n",
      "Iteration 1891, loss = 0.13345142\n",
      "Iteration 1892, loss = 0.13093515\n",
      "Iteration 1893, loss = 0.12844223\n",
      "Iteration 1894, loss = 0.11726064\n",
      "Iteration 1895, loss = 0.12488783\n",
      "Iteration 1896, loss = 0.11669876\n",
      "Iteration 1897, loss = 0.11571974\n",
      "Iteration 1898, loss = 0.11418570\n",
      "Iteration 1899, loss = 0.14376875\n",
      "Iteration 1900, loss = 0.13219690\n",
      "Iteration 1901, loss = 0.15323295\n",
      "Iteration 1902, loss = 0.17145094\n",
      "Iteration 1903, loss = 0.21858717\n",
      "Iteration 1904, loss = 0.20701593\n",
      "Iteration 1905, loss = 0.31427654\n",
      "Iteration 1906, loss = 0.13537903\n",
      "Iteration 1907, loss = 0.19405669\n",
      "Iteration 1908, loss = 0.15283252\n",
      "Iteration 1909, loss = 0.61501312\n",
      "Iteration 1910, loss = 0.80558410\n",
      "Iteration 1911, loss = 0.78968811\n",
      "Iteration 1912, loss = 0.73881000\n",
      "Iteration 1913, loss = 0.45706555\n",
      "Iteration 1914, loss = 1.52838003\n",
      "Iteration 1915, loss = 0.94165967\n",
      "Iteration 1916, loss = 0.74870545\n",
      "Iteration 1917, loss = 0.60388168\n",
      "Iteration 1918, loss = 0.87702341\n",
      "Iteration 1919, loss = 0.38260459\n",
      "Iteration 1920, loss = 0.33122037\n",
      "Iteration 1921, loss = 0.59385270\n",
      "Iteration 1922, loss = 0.61371661\n",
      "Iteration 1923, loss = 0.41479770\n",
      "Iteration 1924, loss = 0.25058606\n",
      "Iteration 1925, loss = 0.25413064\n",
      "Iteration 1926, loss = 0.28470739\n",
      "Iteration 1927, loss = 0.26982182\n",
      "Iteration 1928, loss = 0.17072429\n",
      "Iteration 1929, loss = 0.24710552\n",
      "Iteration 1930, loss = 0.37796982\n",
      "Iteration 1931, loss = 0.39044080\n",
      "Iteration 1932, loss = 1.75435912\n",
      "Iteration 1933, loss = 3.76185752\n",
      "Iteration 1934, loss = 1.39275757\n",
      "Iteration 1935, loss = 1.89187016\n",
      "Iteration 1936, loss = 1.16050164\n",
      "Iteration 1937, loss = 1.09488234\n",
      "Iteration 1938, loss = 0.84991800\n",
      "Iteration 1939, loss = 1.14654462\n",
      "Iteration 1940, loss = 1.24294704\n",
      "Iteration 1941, loss = 1.00789104\n",
      "Iteration 1942, loss = 0.79538751\n",
      "Iteration 1943, loss = 1.00559351\n",
      "Iteration 1944, loss = 0.67528644\n",
      "Iteration 1945, loss = 0.72063863\n",
      "Iteration 1946, loss = 0.67396114\n",
      "Iteration 1947, loss = 0.64488040\n",
      "Iteration 1948, loss = 0.79806729\n",
      "Iteration 1949, loss = 0.37979345\n",
      "Iteration 1950, loss = 0.43318172\n",
      "Iteration 1951, loss = 0.35906900\n",
      "Iteration 1952, loss = 0.20731114\n",
      "Iteration 1953, loss = 0.32362944\n",
      "Iteration 1954, loss = 0.28946527\n",
      "Iteration 1955, loss = 0.25984215\n",
      "Iteration 1956, loss = 0.31477825\n",
      "Iteration 1957, loss = 0.21786155\n",
      "Iteration 1958, loss = 0.21664272\n",
      "Iteration 1959, loss = 0.33116609\n",
      "Iteration 1960, loss = 0.20264039\n",
      "Iteration 1961, loss = 0.22276326\n",
      "Iteration 1962, loss = 0.16608718\n",
      "Iteration 1963, loss = 0.18836316\n",
      "Iteration 1964, loss = 0.28921015\n",
      "Iteration 1965, loss = 0.23239371\n",
      "Iteration 1966, loss = 0.23492902\n",
      "Iteration 1967, loss = 0.25581039\n",
      "Iteration 1968, loss = 0.16426768\n",
      "Iteration 1969, loss = 0.13241189\n",
      "Iteration 1970, loss = 0.16335660\n",
      "Iteration 1971, loss = 0.19229182\n",
      "Iteration 1972, loss = 0.21235180\n",
      "Iteration 1973, loss = 0.13594728\n",
      "Iteration 1974, loss = 0.16806827\n",
      "Iteration 1975, loss = 0.34632123\n",
      "Iteration 1976, loss = 0.56410924\n",
      "Iteration 1977, loss = 0.38217805\n",
      "Iteration 1978, loss = 0.45813768\n",
      "Iteration 1979, loss = 0.62105333\n",
      "Iteration 1980, loss = 0.59555558\n",
      "Iteration 1981, loss = 0.43429458\n",
      "Iteration 1982, loss = 0.33347611\n",
      "Iteration 1983, loss = 0.42267622\n",
      "Iteration 1984, loss = 0.48111392\n",
      "Iteration 1985, loss = 0.32124617\n",
      "Iteration 1986, loss = 0.49006997\n",
      "Iteration 1987, loss = 0.52324980\n",
      "Iteration 1988, loss = 0.58248486\n",
      "Iteration 1989, loss = 1.13208636\n",
      "Iteration 1990, loss = 0.58924370\n",
      "Iteration 1991, loss = 0.85172315\n",
      "Iteration 1992, loss = 0.33630956\n",
      "Iteration 1993, loss = 0.29350754\n",
      "Iteration 1994, loss = 1.13854491\n",
      "Iteration 1995, loss = 0.80298665\n",
      "Iteration 1996, loss = 0.68706920\n",
      "Iteration 1997, loss = 0.74274278\n",
      "Iteration 1998, loss = 0.65861914\n",
      "Iteration 1999, loss = 0.31561206\n",
      "Iteration 2000, loss = 0.26420775\n",
      "Iteration 2001, loss = 0.23206117\n",
      "Iteration 2002, loss = 0.15437534\n",
      "Iteration 2003, loss = 0.19354624\n",
      "Iteration 2004, loss = 0.24098027\n",
      "Iteration 2005, loss = 0.17555641\n",
      "Iteration 2006, loss = 0.15884246\n",
      "Iteration 2007, loss = 0.14071106\n",
      "Iteration 2008, loss = 0.15829772\n",
      "Iteration 2009, loss = 0.14288558\n",
      "Iteration 2010, loss = 0.12729072\n",
      "Iteration 2011, loss = 0.12715684\n",
      "Iteration 2012, loss = 0.12888137\n",
      "Iteration 2013, loss = 0.12383956\n",
      "Iteration 2014, loss = 0.12267308\n",
      "Iteration 2015, loss = 0.13835456\n",
      "Iteration 2016, loss = 0.11169722\n",
      "Iteration 2017, loss = 0.11631680\n",
      "Iteration 2018, loss = 0.12163165\n",
      "Iteration 2019, loss = 0.11401441\n",
      "Iteration 2020, loss = 0.10987914\n",
      "Iteration 2021, loss = 0.11497166\n",
      "Iteration 2022, loss = 0.11524493\n",
      "Iteration 2023, loss = 0.12278445\n",
      "Iteration 2024, loss = 0.12126742\n",
      "Iteration 2025, loss = 0.12062591\n",
      "Iteration 2026, loss = 0.11645576\n",
      "Iteration 2027, loss = 0.11572302\n",
      "Iteration 2028, loss = 0.11113038\n",
      "Iteration 2029, loss = 0.12906243\n",
      "Iteration 2030, loss = 0.12436072\n",
      "Iteration 2031, loss = 0.13376100\n",
      "Iteration 2032, loss = 0.11943913\n",
      "Iteration 2033, loss = 0.11567105\n",
      "Iteration 2034, loss = 0.11941508\n",
      "Iteration 2035, loss = 0.13772834\n",
      "Iteration 2036, loss = 0.11995865\n",
      "Iteration 2037, loss = 0.12179284\n",
      "Iteration 2038, loss = 0.11465440\n",
      "Iteration 2039, loss = 0.11729749\n",
      "Iteration 2040, loss = 0.11660420\n",
      "Iteration 2041, loss = 0.11400911\n",
      "Iteration 2042, loss = 0.12841729\n",
      "Iteration 2043, loss = 0.12256333\n",
      "Iteration 2044, loss = 0.13835204\n",
      "Iteration 2045, loss = 0.11902831\n",
      "Iteration 2046, loss = 0.12377565\n",
      "Iteration 2047, loss = 0.12968541\n",
      "Iteration 2048, loss = 0.11576060\n",
      "Iteration 2049, loss = 0.11834750\n",
      "Iteration 2050, loss = 0.11399000\n",
      "Iteration 2051, loss = 0.11576056\n",
      "Iteration 2052, loss = 0.12051231\n",
      "Iteration 2053, loss = 0.11539977\n",
      "Iteration 2054, loss = 0.11738744\n",
      "Iteration 2055, loss = 0.11325258\n",
      "Iteration 2056, loss = 0.16048751\n",
      "Iteration 2057, loss = 0.18865575\n",
      "Iteration 2058, loss = 0.15253769\n",
      "Iteration 2059, loss = 0.12620821\n",
      "Iteration 2060, loss = 0.12648135\n",
      "Iteration 2061, loss = 0.13233108\n",
      "Iteration 2062, loss = 0.11884614\n",
      "Iteration 2063, loss = 0.12394822\n",
      "Iteration 2064, loss = 0.12287825\n",
      "Iteration 2065, loss = 0.12374480\n",
      "Iteration 2066, loss = 0.12110511\n",
      "Iteration 2067, loss = 0.12549187\n",
      "Iteration 2068, loss = 0.14693852\n",
      "Iteration 2069, loss = 0.13226335\n",
      "Iteration 2070, loss = 0.13461795\n",
      "Iteration 2071, loss = 0.13420021\n",
      "Iteration 2072, loss = 0.11459172\n",
      "Iteration 2073, loss = 0.11823349\n",
      "Iteration 2074, loss = 0.12384262\n",
      "Iteration 2075, loss = 0.11404900\n",
      "Iteration 2076, loss = 0.11810834\n",
      "Iteration 2077, loss = 0.12034942\n",
      "Iteration 2078, loss = 0.11982067\n",
      "Iteration 2079, loss = 0.11666236\n",
      "Iteration 2080, loss = 0.12068383\n",
      "Iteration 2081, loss = 0.12203234\n",
      "Iteration 2082, loss = 0.11937298\n",
      "Iteration 2083, loss = 0.11954290\n",
      "Iteration 2084, loss = 0.13105195\n",
      "Iteration 2085, loss = 0.12129060\n",
      "Iteration 2086, loss = 0.11053301\n",
      "Iteration 2087, loss = 0.11937993\n",
      "Iteration 2088, loss = 0.12128363\n",
      "Iteration 2089, loss = 0.15020498\n",
      "Iteration 2090, loss = 0.14338638\n",
      "Iteration 2091, loss = 0.11635515\n",
      "Iteration 2092, loss = 0.12701829\n",
      "Iteration 2093, loss = 0.11706851\n",
      "Iteration 2094, loss = 0.11833179\n",
      "Iteration 2095, loss = 0.12122377\n",
      "Iteration 2096, loss = 0.11366277\n",
      "Iteration 2097, loss = 0.12246988\n",
      "Iteration 2098, loss = 0.11493038\n",
      "Iteration 2099, loss = 0.11622798\n",
      "Iteration 2100, loss = 0.11367900\n",
      "Iteration 2101, loss = 0.11577566\n",
      "Iteration 2102, loss = 0.12254105\n",
      "Iteration 2103, loss = 0.11163360\n",
      "Iteration 2104, loss = 0.12171009\n",
      "Iteration 2105, loss = 0.12232861\n",
      "Iteration 2106, loss = 0.11838098\n",
      "Iteration 2107, loss = 0.11487328\n",
      "Iteration 2108, loss = 0.11184942\n",
      "Iteration 2109, loss = 0.11172557\n",
      "Iteration 2110, loss = 0.11106756\n",
      "Iteration 2111, loss = 0.11952491\n",
      "Iteration 2112, loss = 0.11564000\n",
      "Iteration 2113, loss = 0.11657858\n",
      "Iteration 2114, loss = 0.12202922\n",
      "Iteration 2115, loss = 0.12141740\n",
      "Iteration 2116, loss = 0.11232742\n",
      "Iteration 2117, loss = 0.12022850\n",
      "Iteration 2118, loss = 0.12855907\n",
      "Iteration 2119, loss = 0.11500029\n",
      "Iteration 2120, loss = 0.11933206\n",
      "Iteration 2121, loss = 0.12364830\n",
      "Iteration 2122, loss = 0.12793256\n",
      "Iteration 2123, loss = 0.12381969\n",
      "Iteration 2124, loss = 0.12340680\n",
      "Iteration 2125, loss = 0.12145271\n",
      "Iteration 2126, loss = 0.12113700\n",
      "Iteration 2127, loss = 0.12607426\n",
      "Iteration 2128, loss = 0.11844892\n",
      "Iteration 2129, loss = 0.11993057\n",
      "Iteration 2130, loss = 0.12364555\n",
      "Iteration 2131, loss = 0.11243335\n",
      "Iteration 2132, loss = 0.11113502\n",
      "Iteration 2133, loss = 0.11283660\n",
      "Iteration 2134, loss = 0.12039043\n",
      "Iteration 2135, loss = 0.11417155\n",
      "Iteration 2136, loss = 0.11472886\n",
      "Iteration 2137, loss = 0.11111652\n",
      "Iteration 2138, loss = 0.12697995\n",
      "Iteration 2139, loss = 0.13111478\n",
      "Iteration 2140, loss = 0.13465941\n",
      "Iteration 2141, loss = 0.11673462\n",
      "Iteration 2142, loss = 0.11365906\n",
      "Iteration 2143, loss = 0.12159381\n",
      "Iteration 2144, loss = 0.13400903\n",
      "Iteration 2145, loss = 0.12607718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2146, loss = 0.13015338\n",
      "Iteration 2147, loss = 0.11910829\n",
      "Iteration 2148, loss = 0.12316095\n",
      "Iteration 2149, loss = 0.11749649\n",
      "Iteration 2150, loss = 0.11425849\n",
      "Iteration 2151, loss = 0.11821756\n",
      "Iteration 2152, loss = 0.11397218\n",
      "Iteration 2153, loss = 0.11517109\n",
      "Iteration 2154, loss = 0.11189117\n",
      "Iteration 2155, loss = 0.11371680\n",
      "Iteration 2156, loss = 0.11138607\n",
      "Iteration 2157, loss = 0.10905800\n",
      "Iteration 2158, loss = 0.11183900\n",
      "Iteration 2159, loss = 0.11502337\n",
      "Iteration 2160, loss = 0.11232466\n",
      "Iteration 2161, loss = 0.11600374\n",
      "Iteration 2162, loss = 0.11495245\n",
      "Iteration 2163, loss = 0.11227966\n",
      "Iteration 2164, loss = 0.11442310\n",
      "Iteration 2165, loss = 0.11290323\n",
      "Iteration 2166, loss = 0.13250078\n",
      "Iteration 2167, loss = 0.13605598\n",
      "Iteration 2168, loss = 0.18953127\n",
      "Iteration 2169, loss = 0.17216578\n",
      "Iteration 2170, loss = 0.12756019\n",
      "Iteration 2171, loss = 0.11494326\n",
      "Iteration 2172, loss = 0.14083444\n",
      "Iteration 2173, loss = 0.13376823\n",
      "Iteration 2174, loss = 0.13270433\n",
      "Iteration 2175, loss = 0.11782699\n",
      "Iteration 2176, loss = 0.11818599\n",
      "Iteration 2177, loss = 0.11743019\n",
      "Iteration 2178, loss = 0.12574780\n",
      "Iteration 2179, loss = 0.12583811\n",
      "Iteration 2180, loss = 0.10948636\n",
      "Iteration 2181, loss = 0.12360295\n",
      "Iteration 2182, loss = 0.12500318\n",
      "Iteration 2183, loss = 0.11801081\n",
      "Iteration 2184, loss = 0.11400481\n",
      "Iteration 2185, loss = 0.11893537\n",
      "Iteration 2186, loss = 0.12843288\n",
      "Iteration 2187, loss = 0.11797919\n",
      "Iteration 2188, loss = 0.12506367\n",
      "Iteration 2189, loss = 0.11402636\n",
      "Iteration 2190, loss = 0.11754411\n",
      "Iteration 2191, loss = 0.11707884\n",
      "Iteration 2192, loss = 0.11974263\n",
      "Iteration 2193, loss = 0.16492652\n",
      "Iteration 2194, loss = 0.26456214\n",
      "Iteration 2195, loss = 0.29763594\n",
      "Iteration 2196, loss = 0.68618490\n",
      "Iteration 2197, loss = 1.28784702\n",
      "Iteration 2198, loss = 1.80168087\n",
      "Iteration 2199, loss = 2.26871566\n",
      "Iteration 2200, loss = 2.88136306\n",
      "Iteration 2201, loss = 1.43993094\n",
      "Iteration 2202, loss = 0.99001554\n",
      "Iteration 2203, loss = 0.91213839\n",
      "Iteration 2204, loss = 1.44909403\n",
      "Iteration 2205, loss = 1.32196573\n",
      "Iteration 2206, loss = 1.33948115\n",
      "Iteration 2207, loss = 1.08532988\n",
      "Iteration 2208, loss = 0.99434990\n",
      "Iteration 2209, loss = 1.52689096\n",
      "Iteration 2210, loss = 0.84138268\n",
      "Iteration 2211, loss = 0.77509814\n",
      "Iteration 2212, loss = 0.62778962\n",
      "Iteration 2213, loss = 0.74486844\n",
      "Iteration 2214, loss = 0.63251646\n",
      "Iteration 2215, loss = 0.86534091\n",
      "Iteration 2216, loss = 0.80320801\n",
      "Iteration 2217, loss = 0.73123733\n",
      "Iteration 2218, loss = 0.60918848\n",
      "Iteration 2219, loss = 0.62033395\n",
      "Iteration 2220, loss = 0.43333003\n",
      "Iteration 2221, loss = 0.32213625\n",
      "Iteration 2222, loss = 0.30001678\n",
      "Iteration 2223, loss = 0.23430051\n",
      "Iteration 2224, loss = 0.39729008\n",
      "Iteration 2225, loss = 0.45652649\n",
      "Iteration 2226, loss = 0.35178671\n",
      "Iteration 2227, loss = 0.38024203\n",
      "Iteration 2228, loss = 0.38353731\n",
      "Iteration 2229, loss = 0.15774544\n",
      "Iteration 2230, loss = 0.20792704\n",
      "Iteration 2231, loss = 0.17037081\n",
      "Iteration 2232, loss = 0.11895097\n",
      "Iteration 2233, loss = 0.17821634\n",
      "Iteration 2234, loss = 0.16343628\n",
      "Iteration 2235, loss = 0.14156706\n",
      "Iteration 2236, loss = 0.14027556\n",
      "Iteration 2237, loss = 0.17365380\n",
      "Iteration 2238, loss = 0.18863531\n",
      "Iteration 2239, loss = 0.27194021\n",
      "Iteration 2240, loss = 0.59547866\n",
      "Iteration 2241, loss = 0.55691564\n",
      "Iteration 2242, loss = 0.40648603\n",
      "Iteration 2243, loss = 0.31077916\n",
      "Iteration 2244, loss = 0.48103970\n",
      "Iteration 2245, loss = 0.38187686\n",
      "Iteration 2246, loss = 0.21773966\n",
      "Iteration 2247, loss = 0.14721602\n",
      "Iteration 2248, loss = 0.18296316\n",
      "Iteration 2249, loss = 0.15917948\n",
      "Iteration 2250, loss = 0.12599654\n",
      "Iteration 2251, loss = 0.13481516\n",
      "Iteration 2252, loss = 0.12556497\n",
      "Iteration 2253, loss = 0.11642122\n",
      "Iteration 2254, loss = 0.13642925\n",
      "Iteration 2255, loss = 0.13680094\n",
      "Iteration 2256, loss = 0.11757749\n",
      "Iteration 2257, loss = 0.13178013\n",
      "Iteration 2258, loss = 0.13621691\n",
      "Iteration 2259, loss = 0.17722350\n",
      "Iteration 2260, loss = 0.15897665\n",
      "Iteration 2261, loss = 0.11730927\n",
      "Iteration 2262, loss = 0.13769849\n",
      "Iteration 2263, loss = 0.13642885\n",
      "Iteration 2264, loss = 0.12682193\n",
      "Iteration 2265, loss = 0.11663548\n",
      "Iteration 2266, loss = 0.11669471\n",
      "Iteration 2267, loss = 0.12093215\n",
      "Iteration 2268, loss = 0.11820196\n",
      "Iteration 2269, loss = 0.11870575\n",
      "Iteration 2270, loss = 0.11478070\n",
      "Iteration 2271, loss = 0.11646869\n",
      "Iteration 2272, loss = 0.11207496\n",
      "Iteration 2273, loss = 0.11985553\n",
      "Iteration 2274, loss = 0.12002338\n",
      "Iteration 2275, loss = 0.12394471\n",
      "Iteration 2276, loss = 0.16545196\n",
      "Iteration 2277, loss = 0.14215022\n",
      "Iteration 2278, loss = 0.11903583\n",
      "Iteration 2279, loss = 0.12517824\n",
      "Iteration 2280, loss = 0.12333788\n",
      "Iteration 2281, loss = 0.12031153\n",
      "Iteration 2282, loss = 0.12379644\n",
      "Iteration 2283, loss = 0.12528524\n",
      "Iteration 2284, loss = 0.12076427\n",
      "Iteration 2285, loss = 0.17672282\n",
      "Iteration 2286, loss = 0.32563665\n",
      "Iteration 2287, loss = 0.34878858\n",
      "Iteration 2288, loss = 0.18976963\n",
      "Iteration 2289, loss = 0.14438999\n",
      "Iteration 2290, loss = 0.15879277\n",
      "Iteration 2291, loss = 0.14486375\n",
      "Iteration 2292, loss = 0.16708955\n",
      "Iteration 2293, loss = 0.16150347\n",
      "Iteration 2294, loss = 0.15027273\n",
      "Iteration 2295, loss = 0.12637688\n",
      "Iteration 2296, loss = 0.13048969\n",
      "Iteration 2297, loss = 0.11508792\n",
      "Iteration 2298, loss = 0.12490223\n",
      "Iteration 2299, loss = 0.11679729\n",
      "Iteration 2300, loss = 0.12015466\n",
      "Iteration 2301, loss = 0.11774274\n",
      "Iteration 2302, loss = 0.11915460\n",
      "Iteration 2303, loss = 0.12254235\n",
      "Iteration 2304, loss = 0.12579114\n",
      "Iteration 2305, loss = 0.12552132\n",
      "Iteration 2306, loss = 0.11579555\n",
      "Iteration 2307, loss = 0.11529653\n",
      "Iteration 2308, loss = 0.11652152\n",
      "Iteration 2309, loss = 0.11974802\n",
      "Iteration 2310, loss = 0.13904830\n",
      "Iteration 2311, loss = 0.12841743\n",
      "Iteration 2312, loss = 0.12470804\n",
      "Iteration 2313, loss = 0.11593430\n",
      "Iteration 2314, loss = 0.11353667\n",
      "Iteration 2315, loss = 0.11927670\n",
      "Iteration 2316, loss = 0.13009724\n",
      "Iteration 2317, loss = 0.14203595\n",
      "Iteration 2318, loss = 0.12482370\n",
      "Iteration 2319, loss = 0.10991646\n",
      "Iteration 2320, loss = 0.12753581\n",
      "Iteration 2321, loss = 0.12454185\n",
      "Iteration 2322, loss = 0.14270434\n",
      "Iteration 2323, loss = 0.15067838\n",
      "Iteration 2324, loss = 0.11929627\n",
      "Iteration 2325, loss = 0.13261466\n",
      "Iteration 2326, loss = 0.12447485\n",
      "Iteration 2327, loss = 0.13828006\n",
      "Iteration 2328, loss = 0.11280525\n",
      "Iteration 2329, loss = 0.11963644\n",
      "Iteration 2330, loss = 0.11852113\n",
      "Iteration 2331, loss = 0.12468289\n",
      "Iteration 2332, loss = 0.11807745\n",
      "Iteration 2333, loss = 0.11750466\n",
      "Iteration 2334, loss = 0.11900924\n",
      "Iteration 2335, loss = 0.13300324\n",
      "Iteration 2336, loss = 0.13086451\n",
      "Iteration 2337, loss = 0.11599461\n",
      "Iteration 2338, loss = 0.12190685\n",
      "Iteration 2339, loss = 0.12370801\n",
      "Iteration 2340, loss = 0.11495734\n",
      "Iteration 2341, loss = 0.11881066\n",
      "Iteration 2342, loss = 0.12208414\n",
      "Iteration 2343, loss = 0.11672454\n",
      "Iteration 2344, loss = 0.12167927\n",
      "Iteration 2345, loss = 0.12373827\n",
      "Iteration 2346, loss = 0.11641278\n",
      "Iteration 2347, loss = 0.11798410\n",
      "Iteration 2348, loss = 0.11415834\n",
      "Iteration 2349, loss = 0.11336870\n",
      "Iteration 2350, loss = 0.11449088\n",
      "Iteration 2351, loss = 0.11907724\n",
      "Iteration 2352, loss = 0.12708801\n",
      "Iteration 2353, loss = 0.12198331\n",
      "Iteration 2354, loss = 0.11131949\n",
      "Iteration 2355, loss = 0.11429368\n",
      "Iteration 2356, loss = 0.11241769\n",
      "Iteration 2357, loss = 0.11710236\n",
      "Iteration 2358, loss = 0.12047258\n",
      "Iteration 2359, loss = 0.11476191\n",
      "Iteration 2360, loss = 0.14725187\n",
      "Iteration 2361, loss = 0.11979474\n",
      "Iteration 2362, loss = 0.14251034\n",
      "Iteration 2363, loss = 0.15490255\n",
      "Iteration 2364, loss = 0.11990603\n",
      "Iteration 2365, loss = 0.13463831\n",
      "Iteration 2366, loss = 0.12014038\n",
      "Iteration 2367, loss = 0.11851029\n",
      "Iteration 2368, loss = 0.12729463\n",
      "Iteration 2369, loss = 0.11948624\n",
      "Iteration 2370, loss = 0.11302536\n",
      "Iteration 2371, loss = 0.12539665\n",
      "Iteration 2372, loss = 0.14326239\n",
      "Iteration 2373, loss = 0.12591203\n",
      "Iteration 2374, loss = 0.12942810\n",
      "Iteration 2375, loss = 0.13812345\n",
      "Iteration 2376, loss = 0.13134570\n",
      "Iteration 2377, loss = 0.11855962\n",
      "Iteration 2378, loss = 0.14805157\n",
      "Iteration 2379, loss = 0.13247086\n",
      "Iteration 2380, loss = 0.12702046\n",
      "Iteration 2381, loss = 0.12127777\n",
      "Iteration 2382, loss = 0.15068580\n",
      "Iteration 2383, loss = 0.20519618\n",
      "Iteration 2384, loss = 0.13804875\n",
      "Iteration 2385, loss = 0.17805634\n",
      "Iteration 2386, loss = 0.18911227\n",
      "Iteration 2387, loss = 0.13048225\n",
      "Iteration 2388, loss = 0.12133222\n",
      "Iteration 2389, loss = 0.13026534\n",
      "Iteration 2390, loss = 0.14916803\n",
      "Iteration 2391, loss = 0.18168846\n",
      "Iteration 2392, loss = 0.15355720\n",
      "Iteration 2393, loss = 0.13087359\n",
      "Iteration 2394, loss = 0.14858398\n",
      "Iteration 2395, loss = 0.11665773\n",
      "Iteration 2396, loss = 0.13533906\n",
      "Iteration 2397, loss = 0.12237527\n",
      "Iteration 2398, loss = 0.13316570\n",
      "Iteration 2399, loss = 0.12046302\n",
      "Iteration 2400, loss = 0.13658418\n",
      "Iteration 2401, loss = 0.14953183\n",
      "Iteration 2402, loss = 0.12134947\n",
      "Iteration 2403, loss = 0.11469706\n",
      "Iteration 2404, loss = 0.12806298\n",
      "Iteration 2405, loss = 0.12040806\n",
      "Iteration 2406, loss = 0.12686197\n",
      "Iteration 2407, loss = 0.11970711\n",
      "Iteration 2408, loss = 0.11078637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2409, loss = 0.11232285\n",
      "Iteration 2410, loss = 0.11869016\n",
      "Iteration 2411, loss = 0.12419668\n",
      "Iteration 2412, loss = 0.14497448\n",
      "Iteration 2413, loss = 0.12467210\n",
      "Iteration 2414, loss = 0.13323189\n",
      "Iteration 2415, loss = 0.13782849\n",
      "Iteration 2416, loss = 0.19038187\n",
      "Iteration 2417, loss = 0.14833473\n",
      "Iteration 2418, loss = 0.12541708\n",
      "Iteration 2419, loss = 0.11848055\n",
      "Iteration 2420, loss = 0.13034702\n",
      "Iteration 2421, loss = 0.11430437\n",
      "Iteration 2422, loss = 0.12970135\n",
      "Iteration 2423, loss = 0.12594795\n",
      "Iteration 2424, loss = 0.11883256\n",
      "Iteration 2425, loss = 0.13617807\n",
      "Iteration 2426, loss = 0.12269647\n",
      "Iteration 2427, loss = 0.11446983\n",
      "Iteration 2428, loss = 0.11461696\n",
      "Iteration 2429, loss = 0.11146759\n",
      "Iteration 2430, loss = 0.12063271\n",
      "Iteration 2431, loss = 0.11770272\n",
      "Iteration 2432, loss = 0.11906746\n",
      "Iteration 2433, loss = 0.13534372\n",
      "Iteration 2434, loss = 0.11952546\n",
      "Iteration 2435, loss = 0.11408029\n",
      "Iteration 2436, loss = 0.11482442\n",
      "Iteration 2437, loss = 0.12651642\n",
      "Iteration 2438, loss = 0.12776976\n",
      "Iteration 2439, loss = 0.11310115\n",
      "Iteration 2440, loss = 0.11867948\n",
      "Iteration 2441, loss = 0.12648780\n",
      "Iteration 2442, loss = 0.11493569\n",
      "Iteration 2443, loss = 0.11728539\n",
      "Iteration 2444, loss = 0.12799537\n",
      "Iteration 2445, loss = 0.12875820\n",
      "Iteration 2446, loss = 0.15960418\n",
      "Iteration 2447, loss = 0.11959958\n",
      "Iteration 2448, loss = 0.13971748\n",
      "Iteration 2449, loss = 0.15593777\n",
      "Iteration 2450, loss = 0.11986735\n",
      "Iteration 2451, loss = 0.15300954\n",
      "Iteration 2452, loss = 0.15345189\n",
      "Iteration 2453, loss = 0.18307448\n",
      "Iteration 2454, loss = 0.12946943\n",
      "Iteration 2455, loss = 0.11803417\n",
      "Iteration 2456, loss = 0.14634261\n",
      "Iteration 2457, loss = 0.11972679\n",
      "Iteration 2458, loss = 0.11809535\n",
      "Iteration 2459, loss = 0.11694593\n",
      "Iteration 2460, loss = 0.11821393\n",
      "Iteration 2461, loss = 0.12286546\n",
      "Iteration 2462, loss = 0.13020245\n",
      "Iteration 2463, loss = 0.11127202\n",
      "Iteration 2464, loss = 0.12902552\n",
      "Iteration 2465, loss = 0.11858207\n",
      "Iteration 2466, loss = 0.14205069\n",
      "Iteration 2467, loss = 0.13080089\n",
      "Iteration 2468, loss = 0.14550216\n",
      "Iteration 2469, loss = 0.14768046\n",
      "Iteration 2470, loss = 0.14525021\n",
      "Iteration 2471, loss = 0.14252691\n",
      "Iteration 2472, loss = 0.12923344\n",
      "Iteration 2473, loss = 0.12105982\n",
      "Iteration 2474, loss = 0.12396013\n",
      "Iteration 2475, loss = 0.13150801\n",
      "Iteration 2476, loss = 0.31269166\n",
      "Iteration 2477, loss = 0.23679163\n",
      "Iteration 2478, loss = 0.22521965\n",
      "Iteration 2479, loss = 0.15778791\n",
      "Iteration 2480, loss = 0.13402589\n",
      "Iteration 2481, loss = 0.13268999\n",
      "Iteration 2482, loss = 0.12453817\n",
      "Iteration 2483, loss = 0.11743981\n",
      "Iteration 2484, loss = 0.11802598\n",
      "Iteration 2485, loss = 0.12639651\n",
      "Iteration 2486, loss = 0.12583903\n",
      "Iteration 2487, loss = 0.12243984\n",
      "Iteration 2488, loss = 0.13412546\n",
      "Iteration 2489, loss = 0.12776396\n",
      "Iteration 2490, loss = 0.12881486\n",
      "Iteration 2491, loss = 0.11802539\n",
      "Iteration 2492, loss = 0.12511465\n",
      "Iteration 2493, loss = 0.13094041\n",
      "Iteration 2494, loss = 0.12240407\n",
      "Iteration 2495, loss = 0.11483814\n",
      "Iteration 2496, loss = 0.15578667\n",
      "Iteration 2497, loss = 0.11646482\n",
      "Iteration 2498, loss = 0.17352614\n",
      "Iteration 2499, loss = 0.24445152\n",
      "Iteration 2500, loss = 0.15300898\n",
      "Iteration 2501, loss = 0.17093159\n",
      "Iteration 2502, loss = 0.12146860\n",
      "Iteration 2503, loss = 0.12998659\n",
      "Iteration 2504, loss = 0.13917794\n",
      "Iteration 2505, loss = 0.11850603\n",
      "Iteration 2506, loss = 0.12852247\n",
      "Iteration 2507, loss = 0.24731253\n",
      "Iteration 2508, loss = 0.55085996\n",
      "Iteration 2509, loss = 0.70785194\n",
      "Iteration 2510, loss = 0.63150567\n",
      "Iteration 2511, loss = 0.52282216\n",
      "Iteration 2512, loss = 0.54218637\n",
      "Iteration 2513, loss = 0.22725599\n",
      "Iteration 2514, loss = 0.23812032\n",
      "Iteration 2515, loss = 0.38327768\n",
      "Iteration 2516, loss = 0.31037202\n",
      "Iteration 2517, loss = 0.24572448\n",
      "Iteration 2518, loss = 0.23616061\n",
      "Iteration 2519, loss = 0.33297890\n",
      "Iteration 2520, loss = 0.18096093\n",
      "Iteration 2521, loss = 0.19181286\n",
      "Iteration 2522, loss = 0.15101264\n",
      "Iteration 2523, loss = 0.14929693\n",
      "Iteration 2524, loss = 0.13831708\n",
      "Iteration 2525, loss = 0.19532616\n",
      "Iteration 2526, loss = 0.16035749\n",
      "Iteration 2527, loss = 0.12314075\n",
      "Iteration 2528, loss = 0.12433610\n",
      "Iteration 2529, loss = 0.13378853\n",
      "Iteration 2530, loss = 0.15334412\n",
      "Iteration 2531, loss = 0.21707251\n",
      "Iteration 2532, loss = 0.24899371\n",
      "Iteration 2533, loss = 0.30680772\n",
      "Iteration 2534, loss = 0.19844056\n",
      "Iteration 2535, loss = 0.36163155\n",
      "Iteration 2536, loss = 0.21136019\n",
      "Iteration 2537, loss = 0.18217047\n",
      "Iteration 2538, loss = 0.21729535\n",
      "Iteration 2539, loss = 0.18558312\n",
      "Iteration 2540, loss = 0.25892596\n",
      "Iteration 2541, loss = 0.23352347\n",
      "Iteration 2542, loss = 0.16116983\n",
      "Iteration 2543, loss = 0.41088768\n",
      "Iteration 2544, loss = 0.22120552\n",
      "Iteration 2545, loss = 1.24062205\n",
      "Iteration 2546, loss = 1.33554275\n",
      "Iteration 2547, loss = 1.28467808\n",
      "Iteration 2548, loss = 1.00294258\n",
      "Iteration 2549, loss = 1.23694276\n",
      "Iteration 2550, loss = 0.87183759\n",
      "Iteration 2551, loss = 0.89670979\n",
      "Iteration 2552, loss = 1.00848521\n",
      "Iteration 2553, loss = 1.28403549\n",
      "Iteration 2554, loss = 2.15196151\n",
      "Iteration 2555, loss = 2.95523101\n",
      "Iteration 2556, loss = 2.74883029\n",
      "Iteration 2557, loss = 1.73730907\n",
      "Iteration 2558, loss = 1.60184707\n",
      "Iteration 2559, loss = 1.00059437\n",
      "Iteration 2560, loss = 1.20965965\n",
      "Iteration 2561, loss = 1.10880652\n",
      "Iteration 2562, loss = 1.72666949\n",
      "Iteration 2563, loss = 1.00706318\n",
      "Iteration 2564, loss = 0.98870095\n",
      "Iteration 2565, loss = 1.27649969\n",
      "Iteration 2566, loss = 0.89362703\n",
      "Iteration 2567, loss = 0.99586295\n",
      "Iteration 2568, loss = 1.09495977\n",
      "Iteration 2569, loss = 0.84435344\n",
      "Iteration 2570, loss = 1.14691610\n",
      "Iteration 2571, loss = 0.92836516\n",
      "Iteration 2572, loss = 0.93304007\n",
      "Iteration 2573, loss = 0.71676101\n",
      "Iteration 2574, loss = 0.71760534\n",
      "Iteration 2575, loss = 0.72720583\n",
      "Iteration 2576, loss = 0.75982339\n",
      "Iteration 2577, loss = 0.51331087\n",
      "Iteration 2578, loss = 0.40618233\n",
      "Iteration 2579, loss = 0.26601607\n",
      "Iteration 2580, loss = 0.28375197\n",
      "Iteration 2581, loss = 0.36233184\n",
      "Iteration 2582, loss = 0.34780915\n",
      "Iteration 2583, loss = 0.32167020\n",
      "Iteration 2584, loss = 0.23694330\n",
      "Iteration 2585, loss = 0.30274195\n",
      "Iteration 2586, loss = 0.19605690\n",
      "Iteration 2587, loss = 0.20133282\n",
      "Iteration 2588, loss = 0.18394269\n",
      "Iteration 2589, loss = 0.17885433\n",
      "Iteration 2590, loss = 0.12843781\n",
      "Iteration 2591, loss = 0.12409053\n",
      "Iteration 2592, loss = 0.12794669\n",
      "Iteration 2593, loss = 0.14637931\n",
      "Iteration 2594, loss = 0.11870754\n",
      "Iteration 2595, loss = 0.12376373\n",
      "Iteration 2596, loss = 0.12845846\n",
      "Iteration 2597, loss = 0.11964208\n",
      "Iteration 2598, loss = 0.11549727\n",
      "Iteration 2599, loss = 0.13729618\n",
      "Iteration 2600, loss = 0.12513006\n",
      "Iteration 2601, loss = 0.11997336\n",
      "Iteration 2602, loss = 0.12901462\n",
      "Iteration 2603, loss = 0.11807554\n",
      "Iteration 2604, loss = 0.15359840\n",
      "Iteration 2605, loss = 0.13839594\n",
      "Iteration 2606, loss = 0.14610145\n",
      "Iteration 2607, loss = 0.14366880\n",
      "Iteration 2608, loss = 0.13823837\n",
      "Iteration 2609, loss = 0.12905886\n",
      "Iteration 2610, loss = 0.13400764\n",
      "Iteration 2611, loss = 0.15039348\n",
      "Iteration 2612, loss = 0.15573796\n",
      "Iteration 2613, loss = 0.13211078\n",
      "Iteration 2614, loss = 0.14526406\n",
      "Iteration 2615, loss = 0.13290220\n",
      "Iteration 2616, loss = 0.12343916\n",
      "Iteration 2617, loss = 0.13743013\n",
      "Iteration 2618, loss = 0.12061100\n",
      "Iteration 2619, loss = 0.11803473\n",
      "Iteration 2620, loss = 0.13424332\n",
      "Iteration 2621, loss = 0.11417215\n",
      "Iteration 2622, loss = 0.11569950\n",
      "Iteration 2623, loss = 0.12416820\n",
      "Iteration 2624, loss = 0.11840614\n",
      "Iteration 2625, loss = 0.12220352\n",
      "Iteration 2626, loss = 0.12641978\n",
      "Iteration 2627, loss = 0.13275010\n",
      "Iteration 2628, loss = 0.13226523\n",
      "Iteration 2629, loss = 0.12742148\n",
      "Iteration 2630, loss = 0.12358609\n",
      "Iteration 2631, loss = 0.11981263\n",
      "Iteration 2632, loss = 0.12665160\n",
      "Iteration 2633, loss = 0.11774383\n",
      "Iteration 2634, loss = 0.11995404\n",
      "Iteration 2635, loss = 0.12622192\n",
      "Iteration 2636, loss = 0.15406910\n",
      "Iteration 2637, loss = 0.15283446\n",
      "Iteration 2638, loss = 0.13004120\n",
      "Iteration 2639, loss = 0.13040629\n",
      "Iteration 2640, loss = 0.13197268\n",
      "Iteration 2641, loss = 0.12102983\n",
      "Iteration 2642, loss = 0.12692072\n",
      "Iteration 2643, loss = 0.11930686\n",
      "Iteration 2644, loss = 0.12239139\n",
      "Iteration 2645, loss = 0.11390281\n",
      "Iteration 2646, loss = 0.11583996\n",
      "Iteration 2647, loss = 0.11844793\n",
      "Iteration 2648, loss = 0.11394939\n",
      "Iteration 2649, loss = 0.11717310\n",
      "Iteration 2650, loss = 0.11735850\n",
      "Iteration 2651, loss = 0.11852051\n",
      "Iteration 2652, loss = 0.11305450\n",
      "Iteration 2653, loss = 0.12851964\n",
      "Iteration 2654, loss = 0.11182739\n",
      "Iteration 2655, loss = 0.14828640\n",
      "Iteration 2656, loss = 0.13412418\n",
      "Iteration 2657, loss = 0.14632561\n",
      "Iteration 2658, loss = 0.16797343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2659, loss = 0.17559069\n",
      "Iteration 2660, loss = 0.17809173\n",
      "Iteration 2661, loss = 0.13318182\n",
      "Iteration 2662, loss = 0.18287259\n",
      "Iteration 2663, loss = 0.15796875\n",
      "Iteration 2664, loss = 0.12588945\n",
      "Iteration 2665, loss = 0.13667898\n",
      "Iteration 2666, loss = 0.15597166\n",
      "Iteration 2667, loss = 0.11589386\n",
      "Iteration 2668, loss = 0.12229627\n",
      "Iteration 2669, loss = 0.12239606\n",
      "Iteration 2670, loss = 0.11534010\n",
      "Iteration 2671, loss = 0.10957875\n",
      "Iteration 2672, loss = 0.11678380\n",
      "Iteration 2673, loss = 0.13330514\n",
      "Iteration 2674, loss = 0.15871492\n",
      "Iteration 2675, loss = 0.15241154\n",
      "Iteration 2676, loss = 0.13009403\n",
      "Iteration 2677, loss = 0.14996244\n",
      "Iteration 2678, loss = 0.14607999\n",
      "Iteration 2679, loss = 0.12591782\n",
      "Iteration 2680, loss = 0.11837351\n",
      "Iteration 2681, loss = 0.15974283\n",
      "Iteration 2682, loss = 0.13791818\n",
      "Iteration 2683, loss = 0.12480726\n",
      "Iteration 2684, loss = 0.12817027\n",
      "Iteration 2685, loss = 0.11582724\n",
      "Iteration 2686, loss = 0.12643223\n",
      "Iteration 2687, loss = 0.11638661\n",
      "Iteration 2688, loss = 0.14685990\n",
      "Iteration 2689, loss = 0.12661152\n",
      "Iteration 2690, loss = 0.12427718\n",
      "Iteration 2691, loss = 0.12515261\n",
      "Iteration 2692, loss = 0.11413756\n",
      "Iteration 2693, loss = 0.12791776\n",
      "Iteration 2694, loss = 0.11029891\n",
      "Iteration 2695, loss = 0.12764334\n",
      "Iteration 2696, loss = 0.12695591\n",
      "Iteration 2697, loss = 0.13631007\n",
      "Iteration 2698, loss = 0.13501402\n",
      "Iteration 2699, loss = 0.12124897\n",
      "Iteration 2700, loss = 0.11926124\n",
      "Iteration 2701, loss = 0.11824957\n",
      "Iteration 2702, loss = 0.13046576\n",
      "Iteration 2703, loss = 0.14281790\n",
      "Iteration 2704, loss = 0.12321058\n",
      "Iteration 2705, loss = 0.12243382\n",
      "Iteration 2706, loss = 0.11631306\n",
      "Iteration 2707, loss = 0.11386063\n",
      "Iteration 2708, loss = 0.12249058\n",
      "Iteration 2709, loss = 0.13401075\n",
      "Iteration 2710, loss = 0.12156878\n",
      "Iteration 2711, loss = 0.11429377\n",
      "Iteration 2712, loss = 0.11851807\n",
      "Iteration 2713, loss = 0.11339866\n",
      "Iteration 2714, loss = 0.12197534\n",
      "Iteration 2715, loss = 0.12258756\n",
      "Iteration 2716, loss = 0.11442171\n",
      "Iteration 2717, loss = 0.11854279\n",
      "Iteration 2718, loss = 0.12056689\n",
      "Iteration 2719, loss = 0.12174005\n",
      "Iteration 2720, loss = 0.12298923\n",
      "Iteration 2721, loss = 0.12201189\n",
      "Iteration 2722, loss = 0.11894914\n",
      "Iteration 2723, loss = 0.13200431\n",
      "Iteration 2724, loss = 0.12393229\n",
      "Iteration 2725, loss = 0.11515363\n",
      "Iteration 2726, loss = 0.12569600\n",
      "Iteration 2727, loss = 0.12226636\n",
      "Iteration 2728, loss = 0.11023156\n",
      "Iteration 2729, loss = 0.11880372\n",
      "Iteration 2730, loss = 0.12560184\n",
      "Iteration 2731, loss = 0.13527565\n",
      "Iteration 2732, loss = 0.11850097\n",
      "Iteration 2733, loss = 0.11223539\n",
      "Iteration 2734, loss = 0.11727029\n",
      "Iteration 2735, loss = 0.11452950\n",
      "Iteration 2736, loss = 0.12187726\n",
      "Iteration 2737, loss = 0.11478981\n",
      "Iteration 2738, loss = 0.11128853\n",
      "Iteration 2739, loss = 0.11306759\n",
      "Iteration 2740, loss = 0.11044098\n",
      "Iteration 2741, loss = 0.11355817\n",
      "Iteration 2742, loss = 0.11401096\n",
      "Iteration 2743, loss = 0.11132382\n",
      "Iteration 2744, loss = 0.11168362\n",
      "Iteration 2745, loss = 0.11453420\n",
      "Iteration 2746, loss = 0.11525563\n",
      "Iteration 2747, loss = 0.11585656\n",
      "Iteration 2748, loss = 0.11582531\n",
      "Iteration 2749, loss = 0.11327723\n",
      "Iteration 2750, loss = 0.11162018\n",
      "Iteration 2751, loss = 0.12289047\n",
      "Iteration 2752, loss = 0.12282128\n",
      "Iteration 2753, loss = 0.11003391\n",
      "Iteration 2754, loss = 0.11461655\n",
      "Iteration 2755, loss = 0.11396001\n",
      "Iteration 2756, loss = 0.11246627\n",
      "Iteration 2757, loss = 0.11163268\n",
      "Iteration 2758, loss = 0.12742158\n",
      "Iteration 2759, loss = 0.11055604\n",
      "Iteration 2760, loss = 0.12440269\n",
      "Iteration 2761, loss = 0.11560850\n",
      "Iteration 2762, loss = 0.15144731\n",
      "Iteration 2763, loss = 0.12755993\n",
      "Iteration 2764, loss = 0.13743256\n",
      "Iteration 2765, loss = 0.14050210\n",
      "Iteration 2766, loss = 0.12171089\n",
      "Iteration 2767, loss = 0.12830962\n",
      "Iteration 2768, loss = 0.12104401\n",
      "Iteration 2769, loss = 0.12348421\n",
      "Iteration 2770, loss = 0.12438446\n",
      "Iteration 2771, loss = 0.12820052\n",
      "Iteration 2772, loss = 0.11507597\n",
      "Iteration 2773, loss = 0.11499200\n",
      "Iteration 2774, loss = 0.11900440\n",
      "Iteration 2775, loss = 0.12932360\n",
      "Iteration 2776, loss = 0.11773159\n",
      "Iteration 2777, loss = 0.13878866\n",
      "Iteration 2778, loss = 0.13656775\n",
      "Iteration 2779, loss = 0.14134004\n",
      "Iteration 2780, loss = 0.15401214\n",
      "Iteration 2781, loss = 0.15588650\n",
      "Iteration 2782, loss = 0.12918196\n",
      "Iteration 2783, loss = 0.14345511\n",
      "Iteration 2784, loss = 0.12515712\n",
      "Iteration 2785, loss = 0.16230112\n",
      "Iteration 2786, loss = 0.12783154\n",
      "Iteration 2787, loss = 0.13685638\n",
      "Iteration 2788, loss = 0.13209561\n",
      "Iteration 2789, loss = 0.11672333\n",
      "Iteration 2790, loss = 0.12512138\n",
      "Iteration 2791, loss = 0.12570901\n",
      "Iteration 2792, loss = 0.11961305\n",
      "Iteration 2793, loss = 0.12172581\n",
      "Iteration 2794, loss = 0.11810057\n",
      "Iteration 2795, loss = 0.11514185\n",
      "Iteration 2796, loss = 0.11727216\n",
      "Iteration 2797, loss = 0.13426987\n",
      "Iteration 2798, loss = 0.12835229\n",
      "Iteration 2799, loss = 0.11898242\n",
      "Iteration 2800, loss = 0.12421076\n",
      "Iteration 2801, loss = 0.13233114\n",
      "Iteration 2802, loss = 0.14735486\n",
      "Iteration 2803, loss = 0.11787057\n",
      "Iteration 2804, loss = 0.11365335\n",
      "Iteration 2805, loss = 0.12796302\n",
      "Iteration 2806, loss = 0.13517729\n",
      "Iteration 2807, loss = 0.12259390\n",
      "Iteration 2808, loss = 0.12761683\n",
      "Iteration 2809, loss = 0.12784728\n",
      "Iteration 2810, loss = 0.12231333\n",
      "Iteration 2811, loss = 0.12613726\n",
      "Iteration 2812, loss = 0.12230292\n",
      "Iteration 2813, loss = 0.11202560\n",
      "Iteration 2814, loss = 0.11852998\n",
      "Iteration 2815, loss = 0.11246961\n",
      "Iteration 2816, loss = 0.11340310\n",
      "Iteration 2817, loss = 0.12010739\n",
      "Iteration 2818, loss = 0.11130930\n",
      "Iteration 2819, loss = 0.11420346\n",
      "Iteration 2820, loss = 0.12600923\n",
      "Iteration 2821, loss = 0.13416334\n",
      "Iteration 2822, loss = 0.11484126\n",
      "Iteration 2823, loss = 0.11793085\n",
      "Iteration 2824, loss = 0.12555864\n",
      "Iteration 2825, loss = 0.11777291\n",
      "Iteration 2826, loss = 0.11113457\n",
      "Iteration 2827, loss = 0.12102996\n",
      "Iteration 2828, loss = 0.13143015\n",
      "Iteration 2829, loss = 0.11735577\n",
      "Iteration 2830, loss = 0.11299696\n",
      "Iteration 2831, loss = 0.11433347\n",
      "Iteration 2832, loss = 0.11051514\n",
      "Iteration 2833, loss = 0.11448947\n",
      "Iteration 2834, loss = 0.11622365\n",
      "Iteration 2835, loss = 0.11394907\n",
      "Iteration 2836, loss = 0.11285935\n",
      "Iteration 2837, loss = 0.12291559\n",
      "Iteration 2838, loss = 0.11684534\n",
      "Iteration 2839, loss = 0.11529926\n",
      "Iteration 2840, loss = 0.11394791\n",
      "Iteration 2841, loss = 0.11513823\n",
      "Iteration 2842, loss = 0.12006623\n",
      "Iteration 2843, loss = 0.12757390\n",
      "Iteration 2844, loss = 0.13126545\n",
      "Iteration 2845, loss = 0.14239545\n",
      "Iteration 2846, loss = 0.20233406\n",
      "Iteration 2847, loss = 0.14497577\n",
      "Iteration 2848, loss = 0.21115226\n",
      "Iteration 2849, loss = 0.17805556\n",
      "Iteration 2850, loss = 0.14948518\n",
      "Iteration 2851, loss = 0.12726436\n",
      "Iteration 2852, loss = 0.11677748\n",
      "Iteration 2853, loss = 0.11658492\n",
      "Iteration 2854, loss = 0.11908099\n",
      "Iteration 2855, loss = 0.15458207\n",
      "Iteration 2856, loss = 0.15101085\n",
      "Iteration 2857, loss = 0.12405308\n",
      "Iteration 2858, loss = 0.11714805\n",
      "Iteration 2859, loss = 0.14310836\n",
      "Iteration 2860, loss = 0.13515035\n",
      "Iteration 2861, loss = 0.12932908\n",
      "Iteration 2862, loss = 0.12373919\n",
      "Iteration 2863, loss = 0.14468305\n",
      "Iteration 2864, loss = 0.12791269\n",
      "Iteration 2865, loss = 0.12362008\n",
      "Iteration 2866, loss = 0.12064433\n",
      "Iteration 2867, loss = 0.12093038\n",
      "Iteration 2868, loss = 0.13011145\n",
      "Iteration 2869, loss = 0.12655611\n",
      "Iteration 2870, loss = 0.13987632\n",
      "Iteration 2871, loss = 0.13235629\n",
      "Iteration 2872, loss = 0.13041048\n",
      "Iteration 2873, loss = 0.15029064\n",
      "Iteration 2874, loss = 0.22182420\n",
      "Iteration 2875, loss = 0.22700246\n",
      "Iteration 2876, loss = 0.16852356\n",
      "Iteration 2877, loss = 0.20983851\n",
      "Iteration 2878, loss = 0.29367104\n",
      "Iteration 2879, loss = 0.12105453\n",
      "Iteration 2880, loss = 0.13196226\n",
      "Iteration 2881, loss = 0.14235420\n",
      "Iteration 2882, loss = 0.12240348\n",
      "Iteration 2883, loss = 0.12296038\n",
      "Iteration 2884, loss = 0.11912125\n",
      "Iteration 2885, loss = 0.11210107\n",
      "Iteration 2886, loss = 0.11700678\n",
      "Iteration 2887, loss = 0.11479363\n",
      "Iteration 2888, loss = 0.11265525\n",
      "Iteration 2889, loss = 0.11343828\n",
      "Iteration 2890, loss = 0.12003158\n",
      "Iteration 2891, loss = 0.11768108\n",
      "Iteration 2892, loss = 0.14060711\n",
      "Iteration 2893, loss = 0.11556926\n",
      "Iteration 2894, loss = 0.11374649\n",
      "Iteration 2895, loss = 0.12554476\n",
      "Iteration 2896, loss = 0.11754944\n",
      "Iteration 2897, loss = 0.11872802\n",
      "Iteration 2898, loss = 0.12437526\n",
      "Iteration 2899, loss = 0.11175422\n",
      "Iteration 2900, loss = 0.11792638\n",
      "Iteration 2901, loss = 0.11289133\n",
      "Iteration 2902, loss = 0.12749967\n",
      "Iteration 2903, loss = 0.21382898\n",
      "Iteration 2904, loss = 0.20603337\n",
      "Iteration 2905, loss = 0.12432840\n",
      "Iteration 2906, loss = 0.12465478\n",
      "Iteration 2907, loss = 0.12233074\n",
      "Iteration 2908, loss = 0.11471569\n",
      "Iteration 2909, loss = 0.11275794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2910, loss = 0.11541779\n",
      "Iteration 2911, loss = 0.14136818\n",
      "Iteration 2912, loss = 0.20020498\n",
      "Iteration 2913, loss = 0.15702466\n",
      "Iteration 2914, loss = 0.11779521\n",
      "Iteration 2915, loss = 0.12970737\n",
      "Iteration 2916, loss = 0.12170119\n",
      "Iteration 2917, loss = 0.11240619\n",
      "Iteration 2918, loss = 0.12454906\n",
      "Iteration 2919, loss = 0.21446117\n",
      "Iteration 2920, loss = 0.32646721\n",
      "Iteration 2921, loss = 0.26592439\n",
      "Iteration 2922, loss = 0.19673161\n",
      "Iteration 2923, loss = 0.14045506\n",
      "Iteration 2924, loss = 0.14915077\n",
      "Iteration 2925, loss = 0.12384899\n",
      "Iteration 2926, loss = 0.11948588\n",
      "Iteration 2927, loss = 0.12664951\n",
      "Iteration 2928, loss = 0.14777610\n",
      "Iteration 2929, loss = 0.14001179\n",
      "Iteration 2930, loss = 0.14817577\n",
      "Iteration 2931, loss = 0.14088013\n",
      "Iteration 2932, loss = 0.11827441\n",
      "Iteration 2933, loss = 0.12556610\n",
      "Iteration 2934, loss = 0.11916651\n",
      "Iteration 2935, loss = 0.12937447\n",
      "Iteration 2936, loss = 0.11324146\n",
      "Iteration 2937, loss = 0.11619258\n",
      "Iteration 2938, loss = 0.11878867\n",
      "Iteration 2939, loss = 0.13902791\n",
      "Iteration 2940, loss = 0.17414489\n",
      "Iteration 2941, loss = 0.13632895\n",
      "Iteration 2942, loss = 0.13417273\n",
      "Iteration 2943, loss = 0.14922337\n",
      "Iteration 2944, loss = 0.12010929\n",
      "Iteration 2945, loss = 0.13092247\n",
      "Iteration 2946, loss = 0.12266355\n",
      "Iteration 2947, loss = 0.14527212\n",
      "Iteration 2948, loss = 0.13880299\n",
      "Iteration 2949, loss = 0.14258239\n",
      "Iteration 2950, loss = 0.16785808\n",
      "Iteration 2951, loss = 0.19941684\n",
      "Iteration 2952, loss = 0.14167350\n",
      "Iteration 2953, loss = 0.13832698\n",
      "Iteration 2954, loss = 0.13028109\n",
      "Iteration 2955, loss = 0.13352651\n",
      "Iteration 2956, loss = 0.14719787\n",
      "Iteration 2957, loss = 0.14778599\n",
      "Iteration 2958, loss = 0.14804890\n",
      "Iteration 2959, loss = 0.11898363\n",
      "Iteration 2960, loss = 0.13581912\n",
      "Iteration 2961, loss = 0.14211160\n",
      "Iteration 2962, loss = 0.12816378\n",
      "Iteration 2963, loss = 0.11186861\n",
      "Iteration 2964, loss = 0.11556194\n",
      "Iteration 2965, loss = 0.12076446\n",
      "Iteration 2966, loss = 0.11281038\n",
      "Iteration 2967, loss = 0.11506367\n",
      "Iteration 2968, loss = 0.12391969\n",
      "Iteration 2969, loss = 0.12519349\n",
      "Iteration 2970, loss = 0.11710752\n",
      "Iteration 2971, loss = 0.13167019\n",
      "Iteration 2972, loss = 0.11827266\n",
      "Iteration 2973, loss = 0.13165493\n",
      "Iteration 2974, loss = 0.12334757\n",
      "Iteration 2975, loss = 0.12649208\n",
      "Iteration 2976, loss = 0.14075652\n",
      "Iteration 2977, loss = 0.11977389\n",
      "Iteration 2978, loss = 0.12560369\n",
      "Iteration 2979, loss = 0.14819622\n",
      "Iteration 2980, loss = 0.12104862\n",
      "Iteration 2981, loss = 0.14125073\n",
      "Iteration 2982, loss = 0.12762286\n",
      "Iteration 2983, loss = 0.11695500\n",
      "Iteration 2984, loss = 0.12072465\n",
      "Iteration 2985, loss = 0.13745771\n",
      "Iteration 2986, loss = 0.12843327\n",
      "Iteration 2987, loss = 0.11721615\n",
      "Iteration 2988, loss = 0.12206811\n",
      "Iteration 2989, loss = 0.11772874\n",
      "Iteration 2990, loss = 0.11363271\n",
      "Iteration 2991, loss = 0.11593264\n",
      "Iteration 2992, loss = 0.12944869\n",
      "Iteration 2993, loss = 0.11424078\n",
      "Iteration 2994, loss = 0.12291167\n",
      "Iteration 2995, loss = 0.11936707\n",
      "Iteration 2996, loss = 0.11694774\n",
      "Iteration 2997, loss = 0.12846664\n",
      "Iteration 2998, loss = 0.14213133\n",
      "Iteration 2999, loss = 0.12160380\n",
      "Iteration 3000, loss = 0.13445489\n",
      "Iteration 3001, loss = 0.12638362\n",
      "Iteration 3002, loss = 0.12287805\n",
      "Iteration 3003, loss = 0.12073703\n",
      "Iteration 3004, loss = 0.11288769\n",
      "Iteration 3005, loss = 0.11158832\n",
      "Iteration 3006, loss = 0.13217869\n",
      "Iteration 3007, loss = 0.14937623\n",
      "Iteration 3008, loss = 0.13158948\n",
      "Iteration 3009, loss = 0.12476587\n",
      "Iteration 3010, loss = 0.11890706\n",
      "Iteration 3011, loss = 0.17072750\n",
      "Iteration 3012, loss = 0.15596650\n",
      "Iteration 3013, loss = 0.20111903\n",
      "Iteration 3014, loss = 0.20383854\n",
      "Iteration 3015, loss = 0.22072200\n",
      "Iteration 3016, loss = 0.19725578\n",
      "Iteration 3017, loss = 0.24172062\n",
      "Iteration 3018, loss = 0.31914273\n",
      "Iteration 3019, loss = 0.28366558\n",
      "Iteration 3020, loss = 0.16159367\n",
      "Iteration 3021, loss = 0.14286324\n",
      "Iteration 3022, loss = 0.14536468\n",
      "Iteration 3023, loss = 0.12899949\n",
      "Iteration 3024, loss = 0.12246600\n",
      "Iteration 3025, loss = 0.12701913\n",
      "Iteration 3026, loss = 0.14335989\n",
      "Iteration 3027, loss = 0.18631738\n",
      "Iteration 3028, loss = 0.12595047\n",
      "Iteration 3029, loss = 0.13647142\n",
      "Iteration 3030, loss = 0.13270002\n",
      "Iteration 3031, loss = 0.12694738\n",
      "Iteration 3032, loss = 0.12828329\n",
      "Iteration 3033, loss = 0.14006483\n",
      "Iteration 3034, loss = 0.12842011\n",
      "Iteration 3035, loss = 0.15009930\n",
      "Iteration 3036, loss = 0.13883126\n",
      "Iteration 3037, loss = 0.13294470\n",
      "Iteration 3038, loss = 0.12704987\n",
      "Iteration 3039, loss = 0.13622311\n",
      "Iteration 3040, loss = 0.11790699\n",
      "Iteration 3041, loss = 0.11664106\n",
      "Iteration 3042, loss = 0.12656722\n",
      "Iteration 3043, loss = 0.12408839\n",
      "Iteration 3044, loss = 0.12481877\n",
      "Iteration 3045, loss = 0.12180293\n",
      "Iteration 3046, loss = 0.12065212\n",
      "Iteration 3047, loss = 0.12290779\n",
      "Iteration 3048, loss = 0.11863129\n",
      "Iteration 3049, loss = 0.12489257\n",
      "Iteration 3050, loss = 0.11854887\n",
      "Iteration 3051, loss = 0.14142831\n",
      "Iteration 3052, loss = 0.15735011\n",
      "Iteration 3053, loss = 0.12766323\n",
      "Iteration 3054, loss = 0.27381001\n",
      "Iteration 3055, loss = 0.38739341\n",
      "Iteration 3056, loss = 1.18547800\n",
      "Iteration 3057, loss = 2.31395433\n",
      "Iteration 3058, loss = 4.97183377\n",
      "Iteration 3059, loss = 2.91274120\n",
      "Iteration 3060, loss = 2.25162118\n",
      "Iteration 3061, loss = 2.85630502\n",
      "Iteration 3062, loss = 2.14000223\n",
      "Iteration 3063, loss = 1.13746617\n",
      "Iteration 3064, loss = 1.49036888\n",
      "Iteration 3065, loss = 1.08853196\n",
      "Iteration 3066, loss = 0.85924788\n",
      "Iteration 3067, loss = 1.21507804\n",
      "Iteration 3068, loss = 1.19217699\n",
      "Iteration 3069, loss = 1.99302669\n",
      "Iteration 3070, loss = 1.06474668\n",
      "Iteration 3071, loss = 0.95066260\n",
      "Iteration 3072, loss = 1.26407519\n",
      "Iteration 3073, loss = 1.04959441\n",
      "Iteration 3074, loss = 0.97082528\n",
      "Iteration 3075, loss = 1.21298882\n",
      "Iteration 3076, loss = 0.99502423\n",
      "Iteration 3077, loss = 0.80820124\n",
      "Iteration 3078, loss = 0.67032724\n",
      "Iteration 3079, loss = 0.62757693\n",
      "Iteration 3080, loss = 0.71119897\n",
      "Iteration 3081, loss = 0.60968144\n",
      "Iteration 3082, loss = 0.49030966\n",
      "Iteration 3083, loss = 0.44051421\n",
      "Iteration 3084, loss = 0.45213530\n",
      "Iteration 3085, loss = 0.30602141\n",
      "Iteration 3086, loss = 0.43241155\n",
      "Iteration 3087, loss = 0.47570398\n",
      "Iteration 3088, loss = 0.27981707\n",
      "Iteration 3089, loss = 0.28460886\n",
      "Iteration 3090, loss = 0.48931435\n",
      "Iteration 3091, loss = 0.32327898\n",
      "Iteration 3092, loss = 0.37190721\n",
      "Iteration 3093, loss = 0.41432172\n",
      "Iteration 3094, loss = 0.33398463\n",
      "Iteration 3095, loss = 0.40554645\n",
      "Iteration 3096, loss = 0.33549143\n",
      "Iteration 3097, loss = 0.22901590\n",
      "Iteration 3098, loss = 0.20650189\n",
      "Iteration 3099, loss = 0.30340947\n",
      "Iteration 3100, loss = 0.25125005\n",
      "Iteration 3101, loss = 0.22096525\n",
      "Iteration 3102, loss = 0.22332775\n",
      "Iteration 3103, loss = 0.19274140\n",
      "Iteration 3104, loss = 0.16162103\n",
      "Iteration 3105, loss = 0.17833313\n",
      "Iteration 3106, loss = 0.16497820\n",
      "Iteration 3107, loss = 0.13159923\n",
      "Iteration 3108, loss = 0.12616280\n",
      "Iteration 3109, loss = 0.20734869\n",
      "Iteration 3110, loss = 0.18802063\n",
      "Iteration 3111, loss = 0.18033776\n",
      "Iteration 3112, loss = 0.13034560\n",
      "Iteration 3113, loss = 0.71100719\n",
      "Iteration 3114, loss = 0.89626396\n",
      "Iteration 3115, loss = 0.79836127\n",
      "Iteration 3116, loss = 0.73072353\n",
      "Iteration 3117, loss = 0.49425887\n",
      "Iteration 3118, loss = 0.41218281\n",
      "Iteration 3119, loss = 0.39181918\n",
      "Iteration 3120, loss = 0.39624711\n",
      "Iteration 3121, loss = 0.32033808\n",
      "Iteration 3122, loss = 0.21250632\n",
      "Iteration 3123, loss = 0.15728195\n",
      "Iteration 3124, loss = 0.19600689\n",
      "Iteration 3125, loss = 0.17768641\n",
      "Iteration 3126, loss = 0.14054147\n",
      "Iteration 3127, loss = 0.15187271\n",
      "Iteration 3128, loss = 0.15558691\n",
      "Iteration 3129, loss = 0.16379261\n",
      "Iteration 3130, loss = 0.14802642\n",
      "Iteration 3131, loss = 0.15356547\n",
      "Iteration 3132, loss = 0.12998962\n",
      "Iteration 3133, loss = 0.14341766\n",
      "Iteration 3134, loss = 0.15666118\n",
      "Iteration 3135, loss = 0.16602110\n",
      "Iteration 3136, loss = 0.13199486\n",
      "Iteration 3137, loss = 0.12406690\n",
      "Iteration 3138, loss = 0.14271032\n",
      "Iteration 3139, loss = 0.13733574\n",
      "Iteration 3140, loss = 0.11904069\n",
      "Iteration 3141, loss = 0.13427711\n",
      "Iteration 3142, loss = 0.14282542\n",
      "Iteration 3143, loss = 0.13074019\n",
      "Iteration 3144, loss = 0.12979135\n",
      "Iteration 3145, loss = 0.11838102\n",
      "Iteration 3146, loss = 0.13527820\n",
      "Iteration 3147, loss = 0.11844011\n",
      "Iteration 3148, loss = 0.11380197\n",
      "Iteration 3149, loss = 0.11930804\n",
      "Iteration 3150, loss = 0.12586874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3151, loss = 0.14819104\n",
      "Iteration 3152, loss = 0.14646942\n",
      "Iteration 3153, loss = 0.17854149\n",
      "Iteration 3154, loss = 0.13062264\n",
      "Iteration 3155, loss = 0.12613605\n",
      "Iteration 3156, loss = 0.12890898\n",
      "Iteration 3157, loss = 0.12645644\n",
      "Iteration 3158, loss = 0.11272126\n",
      "Iteration 3159, loss = 0.12939882\n",
      "Iteration 3160, loss = 0.11194259\n",
      "Iteration 3161, loss = 0.13480328\n",
      "Iteration 3162, loss = 0.12815664\n",
      "Iteration 3163, loss = 0.19485039\n",
      "Iteration 3164, loss = 0.15401498\n",
      "Iteration 3165, loss = 0.22767711\n",
      "Iteration 3166, loss = 0.30783927\n",
      "Iteration 3167, loss = 0.18999090\n",
      "Iteration 3168, loss = 0.17570789\n",
      "Iteration 3169, loss = 0.19271686\n",
      "Iteration 3170, loss = 0.13695051\n",
      "Iteration 3171, loss = 0.13734702\n",
      "Iteration 3172, loss = 0.13243040\n",
      "Iteration 3173, loss = 0.17007621\n",
      "Iteration 3174, loss = 0.13170240\n",
      "Iteration 3175, loss = 0.14069468\n",
      "Iteration 3176, loss = 0.13541528\n",
      "Iteration 3177, loss = 0.13612627\n",
      "Iteration 3178, loss = 0.18722501\n",
      "Iteration 3179, loss = 0.14459285\n",
      "Iteration 3180, loss = 0.12348680\n",
      "Iteration 3181, loss = 0.12832323\n",
      "Iteration 3182, loss = 0.13654918\n",
      "Iteration 3183, loss = 0.12026887\n",
      "Iteration 3184, loss = 0.12105978\n",
      "Iteration 3185, loss = 0.12005616\n",
      "Iteration 3186, loss = 0.11798762\n",
      "Iteration 3187, loss = 0.11918271\n",
      "Iteration 3188, loss = 0.14660126\n",
      "Iteration 3189, loss = 0.14872083\n",
      "Iteration 3190, loss = 0.11568339\n",
      "Iteration 3191, loss = 0.13508281\n",
      "Iteration 3192, loss = 0.11761641\n",
      "Iteration 3193, loss = 0.11843508\n",
      "Iteration 3194, loss = 0.12510600\n",
      "Iteration 3195, loss = 0.11628138\n",
      "Iteration 3196, loss = 0.13143580\n",
      "Iteration 3197, loss = 0.12187503\n",
      "Iteration 3198, loss = 0.13296563\n",
      "Iteration 3199, loss = 0.12060356\n",
      "Iteration 3200, loss = 0.12481611\n",
      "Iteration 3201, loss = 0.21410794\n",
      "Iteration 3202, loss = 0.17562064\n",
      "Iteration 3203, loss = 0.20331303\n",
      "Iteration 3204, loss = 0.28124275\n",
      "Iteration 3205, loss = 0.19296878\n",
      "Iteration 3206, loss = 0.12698238\n",
      "Iteration 3207, loss = 0.13757137\n",
      "Iteration 3208, loss = 0.15051171\n",
      "Iteration 3209, loss = 0.14597363\n",
      "Iteration 3210, loss = 0.11369265\n",
      "Iteration 3211, loss = 0.13834109\n",
      "Iteration 3212, loss = 0.11755207\n",
      "Iteration 3213, loss = 0.13909003\n",
      "Iteration 3214, loss = 0.12258600\n",
      "Iteration 3215, loss = 0.13391911\n",
      "Iteration 3216, loss = 0.12013501\n",
      "Iteration 3217, loss = 0.12846349\n",
      "Iteration 3218, loss = 0.14201858\n",
      "Iteration 3219, loss = 0.12946260\n",
      "Iteration 3220, loss = 0.15593123\n",
      "Iteration 3221, loss = 0.15963610\n",
      "Iteration 3222, loss = 0.13391997\n",
      "Iteration 3223, loss = 0.11763037\n",
      "Iteration 3224, loss = 0.11908230\n",
      "Iteration 3225, loss = 0.13658183\n",
      "Iteration 3226, loss = 0.16906559\n",
      "Iteration 3227, loss = 0.13156806\n",
      "Iteration 3228, loss = 0.11780100\n",
      "Iteration 3229, loss = 0.11737383\n",
      "Iteration 3230, loss = 0.11768275\n",
      "Iteration 3231, loss = 0.11617775\n",
      "Iteration 3232, loss = 0.11379720\n",
      "Iteration 3233, loss = 0.12710330\n",
      "Iteration 3234, loss = 0.12303852\n",
      "Iteration 3235, loss = 0.11742736\n",
      "Iteration 3236, loss = 0.11169921\n",
      "Iteration 3237, loss = 0.12961322\n",
      "Iteration 3238, loss = 0.11188332\n",
      "Iteration 3239, loss = 0.12596976\n",
      "Iteration 3240, loss = 0.11733353\n",
      "Iteration 3241, loss = 0.12229070\n",
      "Iteration 3242, loss = 0.11527242\n",
      "Iteration 3243, loss = 0.11969252\n",
      "Iteration 3244, loss = 0.11480814\n",
      "Iteration 3245, loss = 0.11859803\n",
      "Iteration 3246, loss = 0.11774887\n",
      "Iteration 3247, loss = 0.11193808\n",
      "Iteration 3248, loss = 0.13085802\n",
      "Iteration 3249, loss = 0.12663947\n",
      "Iteration 3250, loss = 0.16020623\n",
      "Iteration 3251, loss = 0.13458083\n",
      "Iteration 3252, loss = 0.22112864\n",
      "Iteration 3253, loss = 0.16058558\n",
      "Iteration 3254, loss = 0.18702618\n",
      "Iteration 3255, loss = 0.21368917\n",
      "Iteration 3256, loss = 0.14341098\n",
      "Iteration 3257, loss = 0.17838856\n",
      "Iteration 3258, loss = 0.18555908\n",
      "Iteration 3259, loss = 0.13112581\n",
      "Iteration 3260, loss = 0.16230069\n",
      "Iteration 3261, loss = 0.14450371\n",
      "Iteration 3262, loss = 0.12488773\n",
      "Iteration 3263, loss = 0.11264011\n",
      "Iteration 3264, loss = 0.12788447\n",
      "Iteration 3265, loss = 0.12309970\n",
      "Iteration 3266, loss = 0.13818621\n",
      "Iteration 3267, loss = 0.11744931\n",
      "Iteration 3268, loss = 0.12532595\n",
      "Iteration 3269, loss = 0.12611315\n",
      "Iteration 3270, loss = 0.11952327\n",
      "Iteration 3271, loss = 0.12039329\n",
      "Iteration 3272, loss = 0.11512445\n",
      "Iteration 3273, loss = 0.11523227\n",
      "Iteration 3274, loss = 0.16795121\n",
      "Iteration 3275, loss = 0.19112100\n",
      "Iteration 3276, loss = 0.14681763\n",
      "Iteration 3277, loss = 0.13641498\n",
      "Iteration 3278, loss = 0.15115013\n",
      "Iteration 3279, loss = 0.19021431\n",
      "Iteration 3280, loss = 0.17310944\n",
      "Iteration 3281, loss = 0.13023568\n",
      "Iteration 3282, loss = 0.12717184\n",
      "Iteration 3283, loss = 0.11215440\n",
      "Iteration 3284, loss = 0.13646838\n",
      "Iteration 3285, loss = 0.11431012\n",
      "Iteration 3286, loss = 0.11950236\n",
      "Iteration 3287, loss = 0.11483241\n",
      "Iteration 3288, loss = 0.13280198\n",
      "Iteration 3289, loss = 0.26821147\n",
      "Iteration 3290, loss = 0.47250988\n",
      "Iteration 3291, loss = 0.54472013\n",
      "Iteration 3292, loss = 0.49157062\n",
      "Iteration 3293, loss = 0.33608984\n",
      "Iteration 3294, loss = 0.15987343\n",
      "Iteration 3295, loss = 0.11620477\n",
      "Iteration 3296, loss = 0.13087477\n",
      "Iteration 3297, loss = 0.12741285\n",
      "Iteration 3298, loss = 0.11076343\n",
      "Iteration 3299, loss = 0.11746614\n",
      "Iteration 3300, loss = 0.11347119\n",
      "Iteration 3301, loss = 0.11561457\n",
      "Iteration 3302, loss = 0.11914443\n",
      "Iteration 3303, loss = 0.14799284\n",
      "Iteration 3304, loss = 0.23103753\n",
      "Iteration 3305, loss = 0.17175480\n",
      "Iteration 3306, loss = 0.30292806\n",
      "Iteration 3307, loss = 0.52814765\n",
      "Iteration 3308, loss = 0.78161263\n",
      "Iteration 3309, loss = 0.64389932\n",
      "Iteration 3310, loss = 0.49854145\n",
      "Iteration 3311, loss = 0.40101995\n",
      "Iteration 3312, loss = 0.14864600\n",
      "Iteration 3313, loss = 0.22890436\n",
      "Iteration 3314, loss = 0.35519873\n",
      "Iteration 3315, loss = 0.28153414\n",
      "Iteration 3316, loss = 0.21417830\n",
      "Iteration 3317, loss = 0.20432377\n",
      "Iteration 3318, loss = 0.24309444\n",
      "Iteration 3319, loss = 0.19238933\n",
      "Iteration 3320, loss = 0.12440609\n",
      "Iteration 3321, loss = 0.12783652\n",
      "Iteration 3322, loss = 0.12470312\n",
      "Iteration 3323, loss = 0.12642479\n",
      "Iteration 3324, loss = 0.13404472\n",
      "Iteration 3325, loss = 0.12410226\n",
      "Iteration 3326, loss = 0.14294540\n",
      "Iteration 3327, loss = 0.14488712\n",
      "Iteration 3328, loss = 0.12051140\n",
      "Iteration 3329, loss = 0.11100644\n",
      "Iteration 3330, loss = 0.12725462\n",
      "Iteration 3331, loss = 0.12427658\n",
      "Iteration 3332, loss = 0.13905247\n",
      "Iteration 3333, loss = 0.17509318\n",
      "Iteration 3334, loss = 0.14714102\n",
      "Iteration 3335, loss = 0.19233077\n",
      "Iteration 3336, loss = 0.25247288\n",
      "Iteration 3337, loss = 0.29879964\n",
      "Iteration 3338, loss = 0.16132310\n",
      "Iteration 3339, loss = 0.14170587\n",
      "Iteration 3340, loss = 0.13176822\n",
      "Iteration 3341, loss = 0.19230429\n",
      "Iteration 3342, loss = 0.23256854\n",
      "Iteration 3343, loss = 0.24866680\n",
      "Iteration 3344, loss = 0.15361341\n",
      "Iteration 3345, loss = 0.17520836\n",
      "Iteration 3346, loss = 0.21350379\n",
      "Iteration 3347, loss = 0.14048335\n",
      "Iteration 3348, loss = 0.15254460\n",
      "Iteration 3349, loss = 0.23205816\n",
      "Iteration 3350, loss = 0.19773162\n",
      "Iteration 3351, loss = 0.16889823\n",
      "Iteration 3352, loss = 0.15133128\n",
      "Iteration 3353, loss = 0.13209344\n",
      "Iteration 3354, loss = 0.11351457\n",
      "Iteration 3355, loss = 0.12165305\n",
      "Iteration 3356, loss = 0.11800502\n",
      "Iteration 3357, loss = 0.11786287\n",
      "Iteration 3358, loss = 0.11212322\n",
      "Iteration 3359, loss = 0.11844154\n",
      "Iteration 3360, loss = 0.11674162\n",
      "Iteration 3361, loss = 0.12471727\n",
      "Iteration 3362, loss = 0.12438277\n",
      "Iteration 3363, loss = 0.15587298\n",
      "Iteration 3364, loss = 0.16099862\n",
      "Iteration 3365, loss = 0.12239050\n",
      "Iteration 3366, loss = 0.13159318\n",
      "Iteration 3367, loss = 0.12414700\n",
      "Iteration 3368, loss = 0.11428416\n",
      "Iteration 3369, loss = 0.12172488\n",
      "Iteration 3370, loss = 0.14826116\n",
      "Iteration 3371, loss = 0.11814902\n",
      "Iteration 3372, loss = 0.11882196\n",
      "Iteration 3373, loss = 0.12383494\n",
      "Iteration 3374, loss = 0.11549702\n",
      "Iteration 3375, loss = 0.10952984\n",
      "Iteration 3376, loss = 0.12197407\n",
      "Iteration 3377, loss = 0.12555308\n",
      "Iteration 3378, loss = 0.12668242\n",
      "Iteration 3379, loss = 0.12301353\n",
      "Iteration 3380, loss = 0.11347492\n",
      "Iteration 3381, loss = 0.11934000\n",
      "Iteration 3382, loss = 0.13124487\n",
      "Iteration 3383, loss = 0.12490900\n",
      "Iteration 3384, loss = 0.11201002\n",
      "Iteration 3385, loss = 0.12205195\n",
      "Iteration 3386, loss = 0.11285399\n",
      "Iteration 3387, loss = 0.12345982\n",
      "Iteration 3388, loss = 0.11511263\n",
      "Iteration 3389, loss = 0.12083750\n",
      "Iteration 3390, loss = 0.11563161\n",
      "Iteration 3391, loss = 0.10976113\n",
      "Iteration 3392, loss = 0.11378067\n",
      "Iteration 3393, loss = 0.11298009\n",
      "Iteration 3394, loss = 0.11875943\n",
      "Iteration 3395, loss = 0.11259745\n",
      "Iteration 3396, loss = 0.12644617\n",
      "Iteration 3397, loss = 0.13377520\n",
      "Iteration 3398, loss = 0.11873444\n",
      "Iteration 3399, loss = 0.12670633\n",
      "Iteration 3400, loss = 0.11745308\n",
      "Iteration 3401, loss = 0.11307263\n",
      "Iteration 3402, loss = 0.11346285\n",
      "Iteration 3403, loss = 0.12000092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3404, loss = 0.11981977\n",
      "Iteration 3405, loss = 0.11537450\n",
      "Iteration 3406, loss = 0.11697256\n",
      "Iteration 3407, loss = 0.10809909\n",
      "Iteration 3408, loss = 0.12287604\n",
      "Iteration 3409, loss = 0.11691240\n",
      "Iteration 3410, loss = 0.11151892\n",
      "Iteration 3411, loss = 0.11607384\n",
      "Iteration 3412, loss = 0.11504674\n",
      "Iteration 3413, loss = 0.11868963\n",
      "Iteration 3414, loss = 0.12152146\n",
      "Iteration 3415, loss = 0.11900492\n",
      "Iteration 3416, loss = 0.12004007\n",
      "Iteration 3417, loss = 0.12722149\n",
      "Iteration 3418, loss = 0.11914302\n",
      "Iteration 3419, loss = 0.14600460\n",
      "Iteration 3420, loss = 0.13279391\n",
      "Iteration 3421, loss = 0.11475401\n",
      "Iteration 3422, loss = 0.11398510\n",
      "Iteration 3423, loss = 0.11317789\n",
      "Iteration 3424, loss = 0.13234605\n",
      "Iteration 3425, loss = 0.12721641\n",
      "Iteration 3426, loss = 0.11503649\n",
      "Iteration 3427, loss = 0.12737119\n",
      "Iteration 3428, loss = 0.13135521\n",
      "Iteration 3429, loss = 0.12000151\n",
      "Iteration 3430, loss = 0.14244739\n",
      "Iteration 3431, loss = 0.19234434\n",
      "Iteration 3432, loss = 0.16136390\n",
      "Iteration 3433, loss = 0.14572928\n",
      "Iteration 3434, loss = 0.15980865\n",
      "Iteration 3435, loss = 0.14951084\n",
      "Iteration 3436, loss = 0.14181384\n",
      "Iteration 3437, loss = 0.25533120\n",
      "Iteration 3438, loss = 0.22259655\n",
      "Iteration 3439, loss = 0.17786674\n",
      "Iteration 3440, loss = 0.20750800\n",
      "Iteration 3441, loss = 0.17270340\n",
      "Iteration 3442, loss = 0.19654693\n",
      "Iteration 3443, loss = 0.21344908\n",
      "Iteration 3444, loss = 0.36302798\n",
      "Iteration 3445, loss = 0.43187986\n",
      "Iteration 3446, loss = 0.32943627\n",
      "Iteration 3447, loss = 0.20237262\n",
      "Iteration 3448, loss = 0.17382657\n",
      "Iteration 3449, loss = 0.19241838\n",
      "Iteration 3450, loss = 0.14531651\n",
      "Iteration 3451, loss = 0.13080666\n",
      "Iteration 3452, loss = 0.13643910\n",
      "Iteration 3453, loss = 0.12804357\n",
      "Iteration 3454, loss = 0.11448597\n",
      "Iteration 3455, loss = 0.11330127\n",
      "Iteration 3456, loss = 0.12279280\n",
      "Iteration 3457, loss = 0.11305331\n",
      "Iteration 3458, loss = 0.12047790\n",
      "Iteration 3459, loss = 0.13276601\n",
      "Iteration 3460, loss = 0.14785884\n",
      "Iteration 3461, loss = 0.13846575\n",
      "Iteration 3462, loss = 0.12210396\n",
      "Iteration 3463, loss = 0.13037949\n",
      "Iteration 3464, loss = 0.11764379\n",
      "Iteration 3465, loss = 0.12962916\n",
      "Iteration 3466, loss = 0.11299379\n",
      "Iteration 3467, loss = 0.11940359\n",
      "Iteration 3468, loss = 0.13200740\n",
      "Iteration 3469, loss = 0.13919955\n",
      "Iteration 3470, loss = 0.12729640\n",
      "Iteration 3471, loss = 0.13285681\n",
      "Iteration 3472, loss = 0.12384522\n",
      "Iteration 3473, loss = 0.13018993\n",
      "Iteration 3474, loss = 0.11889134\n",
      "Iteration 3475, loss = 0.11791248\n",
      "Iteration 3476, loss = 0.11722729\n",
      "Iteration 3477, loss = 0.11168086\n",
      "Iteration 3478, loss = 0.11610915\n",
      "Iteration 3479, loss = 0.12680325\n",
      "Iteration 3480, loss = 0.14150398\n",
      "Iteration 3481, loss = 0.13232575\n",
      "Iteration 3482, loss = 0.14060787\n",
      "Iteration 3483, loss = 0.18870944\n",
      "Iteration 3484, loss = 0.16854546\n",
      "Iteration 3485, loss = 0.14590252\n",
      "Iteration 3486, loss = 0.15736723\n",
      "Iteration 3487, loss = 0.12740357\n",
      "Iteration 3488, loss = 0.14770881\n",
      "Iteration 3489, loss = 0.14213923\n",
      "Iteration 3490, loss = 0.14253780\n",
      "Iteration 3491, loss = 0.13270756\n",
      "Iteration 3492, loss = 0.13572711\n",
      "Iteration 3493, loss = 0.13158999\n",
      "Iteration 3494, loss = 0.13031350\n",
      "Iteration 3495, loss = 0.12922893\n",
      "Iteration 3496, loss = 0.13180244\n",
      "Iteration 3497, loss = 0.11735760\n",
      "Iteration 3498, loss = 0.13406065\n",
      "Iteration 3499, loss = 0.12600210\n",
      "Iteration 3500, loss = 0.15815784\n",
      "Iteration 3501, loss = 0.15670563\n",
      "Iteration 3502, loss = 0.12863668\n",
      "Iteration 3503, loss = 0.14737748\n",
      "Iteration 3504, loss = 0.12625073\n",
      "Iteration 3505, loss = 0.11756924\n",
      "Iteration 3506, loss = 0.13003363\n",
      "Iteration 3507, loss = 0.15068133\n",
      "Iteration 3508, loss = 0.13299550\n",
      "Iteration 3509, loss = 0.12581023\n",
      "Iteration 3510, loss = 0.12096129\n",
      "Iteration 3511, loss = 0.13922215\n",
      "Iteration 3512, loss = 0.13070598\n",
      "Iteration 3513, loss = 0.12077562\n",
      "Iteration 3514, loss = 0.12757940\n",
      "Iteration 3515, loss = 0.17740500\n",
      "Iteration 3516, loss = 0.11763049\n",
      "Iteration 3517, loss = 0.13959031\n",
      "Iteration 3518, loss = 0.16631268\n",
      "Iteration 3519, loss = 0.15586868\n",
      "Iteration 3520, loss = 0.11636957\n",
      "Iteration 3521, loss = 0.12136058\n",
      "Iteration 3522, loss = 0.11955665\n",
      "Iteration 3523, loss = 0.11402120\n",
      "Iteration 3524, loss = 0.11499471\n",
      "Iteration 3525, loss = 0.11307467\n",
      "Iteration 3526, loss = 0.13334542\n",
      "Iteration 3527, loss = 0.13878417\n",
      "Iteration 3528, loss = 0.13054587\n",
      "Iteration 3529, loss = 0.18950336\n",
      "Iteration 3530, loss = 0.16844756\n",
      "Iteration 3531, loss = 0.14554573\n",
      "Iteration 3532, loss = 0.12565852\n",
      "Iteration 3533, loss = 0.13985658\n",
      "Iteration 3534, loss = 0.14082875\n",
      "Iteration 3535, loss = 0.12172503\n",
      "Iteration 3536, loss = 0.11085716\n",
      "Iteration 3537, loss = 0.13000860\n",
      "Iteration 3538, loss = 0.13034203\n",
      "Iteration 3539, loss = 0.11746189\n",
      "Iteration 3540, loss = 0.12805932\n",
      "Iteration 3541, loss = 0.13040270\n",
      "Iteration 3542, loss = 0.12863025\n",
      "Iteration 3543, loss = 0.12549977\n",
      "Iteration 3544, loss = 0.15050269\n",
      "Iteration 3545, loss = 0.12313036\n",
      "Iteration 3546, loss = 0.12639729\n",
      "Iteration 3547, loss = 0.13723301\n",
      "Iteration 3548, loss = 0.11744022\n",
      "Iteration 3549, loss = 0.11771968\n",
      "Iteration 3550, loss = 0.11389439\n",
      "Iteration 3551, loss = 0.11854643\n",
      "Iteration 3552, loss = 0.11973220\n",
      "Iteration 3553, loss = 0.11745022\n",
      "Iteration 3554, loss = 0.13332262\n",
      "Iteration 3555, loss = 0.13133427\n",
      "Iteration 3556, loss = 0.12315891\n",
      "Iteration 3557, loss = 0.13391560\n",
      "Iteration 3558, loss = 0.12620502\n",
      "Iteration 3559, loss = 0.13860528\n",
      "Iteration 3560, loss = 0.12997995\n",
      "Iteration 3561, loss = 0.13266743\n",
      "Iteration 3562, loss = 0.11935454\n",
      "Iteration 3563, loss = 0.12607861\n",
      "Iteration 3564, loss = 0.10999670\n",
      "Iteration 3565, loss = 0.12393975\n",
      "Iteration 3566, loss = 0.12113688\n",
      "Iteration 3567, loss = 0.11589249\n",
      "Iteration 3568, loss = 0.11822197\n",
      "Iteration 3569, loss = 0.12076491\n",
      "Iteration 3570, loss = 0.11868494\n",
      "Iteration 3571, loss = 0.11939812\n",
      "Iteration 3572, loss = 0.12228444\n",
      "Iteration 3573, loss = 0.11819531\n",
      "Iteration 3574, loss = 0.12906979\n",
      "Iteration 3575, loss = 0.11289279\n",
      "Iteration 3576, loss = 0.11591813\n",
      "Iteration 3577, loss = 0.11749001\n",
      "Iteration 3578, loss = 0.13167142\n",
      "Iteration 3579, loss = 0.11467599\n",
      "Iteration 3580, loss = 0.11206163\n",
      "Iteration 3581, loss = 0.11675387\n",
      "Iteration 3582, loss = 0.11286575\n",
      "Iteration 3583, loss = 0.12204968\n",
      "Iteration 3584, loss = 0.11535710\n",
      "Iteration 3585, loss = 0.11417657\n",
      "Iteration 3586, loss = 0.11422152\n",
      "Iteration 3587, loss = 0.12488921\n",
      "Iteration 3588, loss = 0.12212383\n",
      "Iteration 3589, loss = 0.11178755\n",
      "Iteration 3590, loss = 0.11678084\n",
      "Iteration 3591, loss = 0.11318209\n",
      "Iteration 3592, loss = 0.12242914\n",
      "Iteration 3593, loss = 0.11278272\n",
      "Iteration 3594, loss = 0.12084551\n",
      "Iteration 3595, loss = 0.25102761\n",
      "Iteration 3596, loss = 0.46365223\n",
      "Iteration 3597, loss = 0.58228702\n",
      "Iteration 3598, loss = 0.62039561\n",
      "Iteration 3599, loss = 0.44552942\n",
      "Iteration 3600, loss = 0.28436165\n",
      "Iteration 3601, loss = 0.16766568\n",
      "Iteration 3602, loss = 0.14686105\n",
      "Iteration 3603, loss = 0.17153562\n",
      "Iteration 3604, loss = 0.12237948\n",
      "Iteration 3605, loss = 0.15092915\n",
      "Iteration 3606, loss = 0.13471243\n",
      "Iteration 3607, loss = 0.11535182\n",
      "Iteration 3608, loss = 0.14962169\n",
      "Iteration 3609, loss = 0.14450453\n",
      "Iteration 3610, loss = 0.12085028\n",
      "Iteration 3611, loss = 0.12664139\n",
      "Iteration 3612, loss = 0.12048202\n",
      "Iteration 3613, loss = 0.12054950\n",
      "Iteration 3614, loss = 0.13986377\n",
      "Iteration 3615, loss = 0.11123282\n",
      "Iteration 3616, loss = 0.12139809\n",
      "Iteration 3617, loss = 0.16981739\n",
      "Iteration 3618, loss = 0.20130969\n",
      "Iteration 3619, loss = 0.20101734\n",
      "Iteration 3620, loss = 0.17530138\n",
      "Iteration 3621, loss = 0.13539826\n",
      "Iteration 3622, loss = 0.12126940\n",
      "Iteration 3623, loss = 0.13777012\n",
      "Iteration 3624, loss = 0.12658371\n",
      "Iteration 3625, loss = 0.12481154\n",
      "Iteration 3626, loss = 0.16344562\n",
      "Iteration 3627, loss = 0.18670629\n",
      "Iteration 3628, loss = 0.22579736\n",
      "Iteration 3629, loss = 0.17669949\n",
      "Iteration 3630, loss = 0.19146354\n",
      "Iteration 3631, loss = 0.20986650\n",
      "Iteration 3632, loss = 0.33806880\n",
      "Iteration 3633, loss = 0.29719905\n",
      "Iteration 3634, loss = 0.45155374\n",
      "Iteration 3635, loss = 0.29840213\n",
      "Iteration 3636, loss = 0.29279745\n",
      "Iteration 3637, loss = 0.19562621\n",
      "Iteration 3638, loss = 0.27082348\n",
      "Iteration 3639, loss = 0.61631836\n",
      "Iteration 3640, loss = 0.46681775\n",
      "Iteration 3641, loss = 0.46558257\n",
      "Iteration 3642, loss = 0.43345601\n",
      "Iteration 3643, loss = 0.33152143\n",
      "Iteration 3644, loss = 0.33967001\n",
      "Iteration 3645, loss = 0.27692916\n",
      "Iteration 3646, loss = 1.61493118\n",
      "Iteration 3647, loss = 1.65007219\n",
      "Iteration 3648, loss = 2.22979822\n",
      "Iteration 3649, loss = 0.94714694\n",
      "Iteration 3650, loss = 1.43713702\n",
      "Iteration 3651, loss = 1.10442371\n",
      "Iteration 3652, loss = 1.28742816\n",
      "Iteration 3653, loss = 1.48156490\n",
      "Iteration 3654, loss = 2.05187502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3655, loss = 1.66177284\n",
      "Iteration 3656, loss = 0.84390689\n",
      "Iteration 3657, loss = 0.96488867\n",
      "Iteration 3658, loss = 0.91661338\n",
      "Iteration 3659, loss = 1.21861466\n",
      "Iteration 3660, loss = 1.07037670\n",
      "Iteration 3661, loss = 0.70923907\n",
      "Iteration 3662, loss = 1.31324787\n",
      "Iteration 3663, loss = 1.20271914\n",
      "Iteration 3664, loss = 0.73713657\n",
      "Iteration 3665, loss = 1.19271812\n",
      "Iteration 3666, loss = 0.70898788\n",
      "Iteration 3667, loss = 0.64655549\n",
      "Iteration 3668, loss = 0.76821783\n",
      "Iteration 3669, loss = 0.56555054\n",
      "Iteration 3670, loss = 0.55856940\n",
      "Iteration 3671, loss = 0.55159467\n",
      "Iteration 3672, loss = 0.41577987\n",
      "Iteration 3673, loss = 0.33862763\n",
      "Iteration 3674, loss = 0.26692608\n",
      "Iteration 3675, loss = 0.18797252\n",
      "Iteration 3676, loss = 0.23328987\n",
      "Iteration 3677, loss = 0.18849859\n",
      "Iteration 3678, loss = 0.15270914\n",
      "Iteration 3679, loss = 0.19109919\n",
      "Iteration 3680, loss = 0.14602188\n",
      "Iteration 3681, loss = 0.12906387\n",
      "Iteration 3682, loss = 0.15680797\n",
      "Iteration 3683, loss = 0.19841553\n",
      "Iteration 3684, loss = 0.13891901\n",
      "Iteration 3685, loss = 0.11458802\n",
      "Iteration 3686, loss = 0.13025718\n",
      "Iteration 3687, loss = 0.12871673\n",
      "Iteration 3688, loss = 0.12526259\n",
      "Iteration 3689, loss = 0.15595864\n",
      "Iteration 3690, loss = 0.11815164\n",
      "Iteration 3691, loss = 0.13368186\n",
      "Iteration 3692, loss = 0.12039103\n",
      "Iteration 3693, loss = 0.13513902\n",
      "Iteration 3694, loss = 0.13918280\n",
      "Iteration 3695, loss = 0.12442524\n",
      "Iteration 3696, loss = 0.11235773\n",
      "Iteration 3697, loss = 0.12982976\n",
      "Iteration 3698, loss = 0.13323829\n",
      "Iteration 3699, loss = 0.11768397\n",
      "Iteration 3700, loss = 0.11629371\n",
      "Iteration 3701, loss = 0.12479121\n",
      "Iteration 3702, loss = 0.12624477\n",
      "Iteration 3703, loss = 0.21776362\n",
      "Iteration 3704, loss = 0.14060143\n",
      "Iteration 3705, loss = 0.15005557\n",
      "Iteration 3706, loss = 0.15842224\n",
      "Iteration 3707, loss = 0.17884076\n",
      "Iteration 3708, loss = 0.16440413\n",
      "Iteration 3709, loss = 0.16842291\n",
      "Iteration 3710, loss = 0.15598218\n",
      "Iteration 3711, loss = 0.15953881\n",
      "Iteration 3712, loss = 0.16512038\n",
      "Iteration 3713, loss = 0.13522349\n",
      "Iteration 3714, loss = 0.13504502\n",
      "Iteration 3715, loss = 0.11606511\n",
      "Iteration 3716, loss = 0.13253134\n",
      "Iteration 3717, loss = 0.12697484\n",
      "Iteration 3718, loss = 0.13295344\n",
      "Iteration 3719, loss = 0.14270221\n",
      "Iteration 3720, loss = 0.14067417\n",
      "Iteration 3721, loss = 0.11916974\n",
      "Iteration 3722, loss = 0.11750020\n",
      "Iteration 3723, loss = 0.14720499\n",
      "Iteration 3724, loss = 0.13501808\n",
      "Iteration 3725, loss = 0.12173765\n",
      "Iteration 3726, loss = 0.12459629\n",
      "Iteration 3727, loss = 0.11286152\n",
      "Iteration 3728, loss = 0.11361434\n",
      "Iteration 3729, loss = 0.11286681\n",
      "Iteration 3730, loss = 0.11113603\n",
      "Iteration 3731, loss = 0.11527883\n",
      "Iteration 3732, loss = 0.11557622\n",
      "Iteration 3733, loss = 0.11520781\n",
      "Iteration 3734, loss = 0.11618596\n",
      "Iteration 3735, loss = 0.11446516\n",
      "Iteration 3736, loss = 0.11100589\n",
      "Iteration 3737, loss = 0.12013869\n",
      "Iteration 3738, loss = 0.12167786\n",
      "Iteration 3739, loss = 0.12486508\n",
      "Iteration 3740, loss = 0.14576668\n",
      "Iteration 3741, loss = 0.12602527\n",
      "Iteration 3742, loss = 0.11275702\n",
      "Iteration 3743, loss = 0.12571481\n",
      "Iteration 3744, loss = 0.11818661\n",
      "Iteration 3745, loss = 0.11576239\n",
      "Iteration 3746, loss = 0.11728030\n",
      "Iteration 3747, loss = 0.12294693\n",
      "Iteration 3748, loss = 0.10910443\n",
      "Iteration 3749, loss = 0.12040040\n",
      "Iteration 3750, loss = 0.11891830\n",
      "Iteration 3751, loss = 0.13334880\n",
      "Iteration 3752, loss = 0.11566454\n",
      "Iteration 3753, loss = 0.11169106\n",
      "Iteration 3754, loss = 0.12001771\n",
      "Iteration 3755, loss = 0.12473210\n",
      "Iteration 3756, loss = 0.11276292\n",
      "Iteration 3757, loss = 0.11224343\n",
      "Iteration 3758, loss = 0.12820533\n",
      "Iteration 3759, loss = 0.11739037\n",
      "Iteration 3760, loss = 0.11360176\n",
      "Iteration 3761, loss = 0.13173283\n",
      "Iteration 3762, loss = 0.16022381\n",
      "Iteration 3763, loss = 0.13323812\n",
      "Iteration 3764, loss = 0.12613180\n",
      "Iteration 3765, loss = 0.11865864\n",
      "Iteration 3766, loss = 0.12355537\n",
      "Iteration 3767, loss = 0.11835010\n",
      "Iteration 3768, loss = 0.12684913\n",
      "Iteration 3769, loss = 0.11498738\n",
      "Iteration 3770, loss = 0.11784865\n",
      "Iteration 3771, loss = 0.14245575\n",
      "Iteration 3772, loss = 0.18041664\n",
      "Iteration 3773, loss = 0.13362588\n",
      "Iteration 3774, loss = 0.14623057\n",
      "Iteration 3775, loss = 0.12983279\n",
      "Iteration 3776, loss = 0.11460550\n",
      "Iteration 3777, loss = 0.12951815\n",
      "Iteration 3778, loss = 0.11797644\n",
      "Iteration 3779, loss = 0.11663929\n",
      "Iteration 3780, loss = 0.11648569\n",
      "Iteration 3781, loss = 0.11229073\n",
      "Iteration 3782, loss = 0.13555560\n",
      "Iteration 3783, loss = 0.13116204\n",
      "Iteration 3784, loss = 0.13055595\n",
      "Iteration 3785, loss = 0.11351558\n",
      "Iteration 3786, loss = 0.11050870\n",
      "Iteration 3787, loss = 0.12685940\n",
      "Iteration 3788, loss = 0.13301664\n",
      "Iteration 3789, loss = 0.13178511\n",
      "Iteration 3790, loss = 0.12696210\n",
      "Iteration 3791, loss = 0.12240190\n",
      "Iteration 3792, loss = 0.12809240\n",
      "Iteration 3793, loss = 0.13168141\n",
      "Iteration 3794, loss = 0.12766581\n",
      "Iteration 3795, loss = 0.12057458\n",
      "Iteration 3796, loss = 0.11738632\n",
      "Iteration 3797, loss = 0.11750611\n",
      "Iteration 3798, loss = 0.11216239\n",
      "Iteration 3799, loss = 0.11785913\n",
      "Iteration 3800, loss = 0.11521677\n",
      "Iteration 3801, loss = 0.11321221\n",
      "Iteration 3802, loss = 0.11679925\n",
      "Iteration 3803, loss = 0.11861294\n",
      "Iteration 3804, loss = 0.12332165\n",
      "Iteration 3805, loss = 0.12040912\n",
      "Iteration 3806, loss = 0.12285365\n",
      "Iteration 3807, loss = 0.12339470\n",
      "Iteration 3808, loss = 0.17620757\n",
      "Iteration 3809, loss = 0.22405626\n",
      "Iteration 3810, loss = 0.19360025\n",
      "Iteration 3811, loss = 0.18345033\n",
      "Iteration 3812, loss = 0.15199152\n",
      "Iteration 3813, loss = 0.12878446\n",
      "Iteration 3814, loss = 0.17999635\n",
      "Iteration 3815, loss = 0.16289819\n",
      "Iteration 3816, loss = 0.25339981\n",
      "Iteration 3817, loss = 0.21095125\n",
      "Iteration 3818, loss = 0.14101044\n",
      "Iteration 3819, loss = 0.16947101\n",
      "Iteration 3820, loss = 0.24730075\n",
      "Iteration 3821, loss = 0.12327648\n",
      "Iteration 3822, loss = 0.15704469\n",
      "Iteration 3823, loss = 0.16830035\n",
      "Iteration 3824, loss = 0.12930399\n",
      "Iteration 3825, loss = 0.12757494\n",
      "Iteration 3826, loss = 0.12181111\n",
      "Iteration 3827, loss = 0.12216942\n",
      "Iteration 3828, loss = 0.11213659\n",
      "Iteration 3829, loss = 0.11551439\n",
      "Iteration 3830, loss = 0.10870499\n",
      "Iteration 3831, loss = 0.11907063\n",
      "Iteration 3832, loss = 0.11416499\n",
      "Iteration 3833, loss = 0.11286505\n",
      "Iteration 3834, loss = 0.11925120\n",
      "Iteration 3835, loss = 0.12239478\n",
      "Iteration 3836, loss = 0.11923961\n",
      "Iteration 3837, loss = 0.12126997\n",
      "Iteration 3838, loss = 0.12512759\n",
      "Iteration 3839, loss = 0.14662037\n",
      "Iteration 3840, loss = 0.12444267\n",
      "Iteration 3841, loss = 0.11442959\n",
      "Iteration 3842, loss = 0.12472387\n",
      "Iteration 3843, loss = 0.11421752\n",
      "Iteration 3844, loss = 0.11103484\n",
      "Iteration 3845, loss = 0.11527452\n",
      "Iteration 3846, loss = 0.11455811\n",
      "Iteration 3847, loss = 0.11334223\n",
      "Iteration 3848, loss = 0.11819007\n",
      "Iteration 3849, loss = 0.14308400\n",
      "Iteration 3850, loss = 0.11866302\n",
      "Iteration 3851, loss = 0.12672994\n",
      "Iteration 3852, loss = 0.12095828\n",
      "Iteration 3853, loss = 0.12549260\n",
      "Iteration 3854, loss = 0.11866771\n",
      "Iteration 3855, loss = 0.12304186\n",
      "Iteration 3856, loss = 0.11494320\n",
      "Iteration 3857, loss = 0.12110118\n",
      "Iteration 3858, loss = 0.11366432\n",
      "Iteration 3859, loss = 0.11771525\n",
      "Iteration 3860, loss = 0.12888982\n",
      "Iteration 3861, loss = 0.12024759\n",
      "Iteration 3862, loss = 0.12408297\n",
      "Iteration 3863, loss = 0.12136418\n",
      "Iteration 3864, loss = 0.11978928\n",
      "Iteration 3865, loss = 0.12248605\n",
      "Iteration 3866, loss = 0.11890822\n",
      "Iteration 3867, loss = 0.15616983\n",
      "Iteration 3868, loss = 0.14739155\n",
      "Iteration 3869, loss = 0.20311036\n",
      "Iteration 3870, loss = 0.15819466\n",
      "Iteration 3871, loss = 0.11227310\n",
      "Iteration 3872, loss = 0.18729176\n",
      "Iteration 3873, loss = 0.19250201\n",
      "Iteration 3874, loss = 0.12579945\n",
      "Iteration 3875, loss = 0.14948407\n",
      "Iteration 3876, loss = 0.13337809\n",
      "Iteration 3877, loss = 0.12649691\n",
      "Iteration 3878, loss = 0.14105925\n",
      "Iteration 3879, loss = 0.12386108\n",
      "Iteration 3880, loss = 0.13102626\n",
      "Iteration 3881, loss = 0.11609254\n",
      "Iteration 3882, loss = 0.13183821\n",
      "Iteration 3883, loss = 0.16041002\n",
      "Iteration 3884, loss = 0.13548640\n",
      "Iteration 3885, loss = 0.14935486\n",
      "Iteration 3886, loss = 0.13173674\n",
      "Iteration 3887, loss = 0.13561581\n",
      "Iteration 3888, loss = 0.17085365\n",
      "Iteration 3889, loss = 0.11626243\n",
      "Iteration 3890, loss = 0.11316430\n",
      "Iteration 3891, loss = 0.12264209\n",
      "Iteration 3892, loss = 0.11222674\n",
      "Iteration 3893, loss = 0.12043563\n",
      "Iteration 3894, loss = 0.17413965\n",
      "Iteration 3895, loss = 0.14574558\n",
      "Iteration 3896, loss = 0.15077947\n",
      "Iteration 3897, loss = 0.14740091\n",
      "Iteration 3898, loss = 0.11514415\n",
      "Iteration 3899, loss = 0.14526788\n",
      "Iteration 3900, loss = 0.13460077\n",
      "CV Accuracy: 0.964286\n"
     ]
    }
   ],
   "source": [
    "#%run group9_dividedata.ipynb\n",
    "%run group9_train_nn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found :\n",
      "\n",
      "{'C': 32, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.934 (+/-0.067) for {'C': 0.125, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.963 (+/-0.053) for {'C': 0.125, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.961 (+/-0.061) for {'C': 0.125, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.319 (+/-0.006) for {'C': 0.125, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.918 (+/-0.058) for {'C': 0.125, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.891 (+/-0.072) for {'C': 0.125, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.319 (+/-0.006) for {'C': 0.125, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.319 (+/-0.006) for {'C': 0.125, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.961 (+/-0.054) for {'C': 0.5, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.965 (+/-0.049) for {'C': 0.5, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.964 (+/-0.051) for {'C': 0.5, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.519 (+/-0.496) for {'C': 0.5, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.937 (+/-0.091) for {'C': 0.5, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.865 (+/-0.064) for {'C': 0.5, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.519 (+/-0.496) for {'C': 0.5, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.519 (+/-0.496) for {'C': 0.5, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.965 (+/-0.049) for {'C': 2, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.966 (+/-0.053) for {'C': 2, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.966 (+/-0.055) for {'C': 2, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 2, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.081) for {'C': 2, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.909 (+/-0.065) for {'C': 2, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.076) for {'C': 2, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 2, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.966 (+/-0.053) for {'C': 8, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.964 (+/-0.051) for {'C': 8, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.964 (+/-0.050) for {'C': 8, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 8, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.081) for {'C': 8, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.909 (+/-0.065) for {'C': 8, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.076) for {'C': 8, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 8, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.966 (+/-0.045) for {'C': 32, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.049) for {'C': 32, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.061) for {'C': 32, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 32, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.081) for {'C': 32, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.909 (+/-0.065) for {'C': 32, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.076) for {'C': 32, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 32, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.964 (+/-0.051) for {'C': 128, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.960 (+/-0.059) for {'C': 128, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.949 (+/-0.075) for {'C': 128, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 128, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.081) for {'C': 128, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.909 (+/-0.065) for {'C': 128, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.076) for {'C': 128, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 128, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.049) for {'C': 512, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.949 (+/-0.062) for {'C': 512, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.062) for {'C': 512, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 512, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.081) for {'C': 512, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.909 (+/-0.065) for {'C': 512, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.076) for {'C': 512, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 512, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.960 (+/-0.059) for {'C': 2048, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.943 (+/-0.063) for {'C': 2048, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.062) for {'C': 2048, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 2048, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.081) for {'C': 2048, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.909 (+/-0.065) for {'C': 2048, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.859 (+/-0.076) for {'C': 2048, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.732 (+/-0.277) for {'C': 2048, 'gamma': 8, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.98      0.97      0.97        98\n",
      "           4       0.93      0.95      0.94        42\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       140\n",
      "   macro avg       0.95      0.96      0.96       140\n",
      "weighted avg       0.96      0.96      0.96       140\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found :\n",
      "\n",
      "{'C': 32, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.891 (+/-0.109) for {'C': 0.125, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.956 (+/-0.066) for {'C': 0.125, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.963 (+/-0.050) for {'C': 0.125, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 0.125, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.942 (+/-0.056) for {'C': 0.125, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.918 (+/-0.072) for {'C': 0.125, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 0.125, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 0.125, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.072) for {'C': 0.5, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.964 (+/-0.049) for {'C': 0.5, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.040) for {'C': 0.5, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.513 (+/-0.033) for {'C': 0.5, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.085) for {'C': 0.5, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.891 (+/-0.069) for {'C': 0.5, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.513 (+/-0.033) for {'C': 0.5, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.513 (+/-0.033) for {'C': 0.5, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.964 (+/-0.049) for {'C': 2, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.053) for {'C': 2, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.970 (+/-0.045) for {'C': 2, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 2, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.069) for {'C': 2, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.056) for {'C': 2, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.884 (+/-0.087) for {'C': 2, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 2, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.053) for {'C': 8, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.040) for {'C': 8, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.965 (+/-0.048) for {'C': 8, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 8, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.069) for {'C': 8, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.056) for {'C': 8, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.884 (+/-0.087) for {'C': 8, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 8, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.968 (+/-0.037) for {'C': 32, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.972 (+/-0.042) for {'C': 32, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.949 (+/-0.069) for {'C': 32, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 32, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.069) for {'C': 32, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.056) for {'C': 32, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.884 (+/-0.087) for {'C': 32, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 32, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.040) for {'C': 128, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.959 (+/-0.060) for {'C': 128, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.939 (+/-0.081) for {'C': 128, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 128, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.069) for {'C': 128, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.056) for {'C': 128, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.884 (+/-0.087) for {'C': 128, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 128, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.972 (+/-0.042) for {'C': 512, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.942 (+/-0.068) for {'C': 512, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.940 (+/-0.063) for {'C': 512, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 512, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.069) for {'C': 512, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.056) for {'C': 512, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.884 (+/-0.087) for {'C': 512, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 512, 'gamma': 8, 'kernel': 'rbf'}\n",
      "0.959 (+/-0.060) for {'C': 2048, 'gamma': 0.00048828125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.054) for {'C': 2048, 'gamma': 0.001953125, 'kernel': 'rbf'}\n",
      "0.940 (+/-0.063) for {'C': 2048, 'gamma': 0.0078125, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 2048, 'gamma': 32, 'kernel': 'rbf'}\n",
      "0.967 (+/-0.069) for {'C': 2048, 'gamma': 0.125, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.056) for {'C': 2048, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "0.884 (+/-0.087) for {'C': 2048, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.742 (+/-0.177) for {'C': 2048, 'gamma': 8, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.98      0.97      0.97        98\n",
      "           4       0.93      0.95      0.94        42\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       140\n",
      "   macro avg       0.95      0.96      0.96       140\n",
      "weighted avg       0.96      0.96      0.96       140\n",
      "\n",
      "\n",
      "CV Accuracy: 97.14285714285714\n",
      "[[95  3]\n",
      " [ 1 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      0.97      0.98        98\n",
      "           4       0.93      0.98      0.95        42\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       140\n",
      "   macro avg       0.96      0.97      0.97       140\n",
      "weighted avg       0.97      0.97      0.97       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run group9_train_svm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.964286\n"
     ]
    }
   ],
   "source": [
    "%run group9_train_logreg.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get Predicted Values\n",
    "#mlp.fit(X_test, y_test)\n",
    "#mlp.fit(X_train,y_train)\n",
    "predicted_values = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 4, 2, 4, 4, 2,\n",
       "       2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 2,\n",
       "       4, 4, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4,\n",
       "       4, 4, 2, 2, 4, 2, 2, 2, 4, 4, 2, 2, 2, 4, 4, 2, 4, 2, 2, 2, 4, 4,\n",
       "       4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2,\n",
       "       4, 4, 2, 2, 2, 2, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy=98.0 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "overallAccuracy=round(accuracy_score(y_test, predicted_values)*100,0)\n",
    "print(\"Overall Accuracy=%s \"% overallAccuracy)\n",
    "#print(confusion_matrix(y_test, predicted_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89  1]\n",
      " [ 2 48]]\n"
     ]
    }
   ],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = confusion_matrix(y_test, predicted_values)\n",
    "print(confusion)\n",
    "#[row, column]\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.66      0.63      0.65        95\n",
      "           4       0.29      0.31      0.30        45\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       140\n",
      "   macro avg       0.47      0.47      0.47       140\n",
      "weighted avg       0.54      0.53      0.53       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_cv, predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9785714285714285\n"
     ]
    }
   ],
   "source": [
    "# use float to perform true division, not integer division\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print((TP + TN) / float(TP + TN + FP + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error=2.142857142857143 \n"
     ]
    }
   ],
   "source": [
    "classification_error = ((FP + FN) / float(TP + TN + FP + FN))*100\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Classification Error=%s \"%classification_error)\n",
    "#print(100 - overallAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operations Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'False Positive Rate')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc,roc_curve\n",
    "false_positive_rate1,true_positive_rate1, thresholds1 =roc_curve(y_test,predicted_values,pos_label=4)\n",
    "roc_auc1=auc(false_positive_rate1,true_positive_rate1)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.title('Receiver Operations Characteristic')\n",
    "plt.plot(false_positive_rate1,true_positive_rate1, color='red',label ='AUC=%0.2f' % roc_auc1)\n",
    "plt.legend(loc= 'lower_right')\n",
    "plt.plot([0,1],[0,1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_curve():\n",
    "    # instantiate\n",
    "    lg = mlp\n",
    "    # fit\n",
    "    lg.fit(X, y)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(lg, X, y, n_jobs=-1, cv=5, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # box-like grid\n",
    "    plt.grid()\n",
    "    \n",
    "    # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # plot the average training and test score lines at each training set size\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # sizes the window for readability and displays the plot\n",
    "    # shows error from 0 to 1.1\n",
    "    plt.ylim(0.8,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.68347656\n",
      "Iteration 2, loss = 5.06771032\n",
      "Iteration 3, loss = 2.17432361\n",
      "Iteration 4, loss = 1.00186119\n",
      "Iteration 5, loss = 0.65656225\n",
      "Iteration 6, loss = 0.38822925\n",
      "Iteration 7, loss = 0.33042238\n",
      "Iteration 8, loss = 0.24823552\n",
      "Iteration 9, loss = 0.19920758\n",
      "Iteration 10, loss = 0.14185152\n",
      "Iteration 11, loss = 0.12028660\n",
      "Iteration 12, loss = 0.11477565\n",
      "Iteration 13, loss = 0.14018046\n",
      "Iteration 14, loss = 0.14236190\n",
      "Iteration 15, loss = 0.11685689\n",
      "Iteration 16, loss = 0.12498303\n",
      "Iteration 17, loss = 0.13661008\n",
      "Iteration 18, loss = 0.12480988\n",
      "Iteration 19, loss = 0.12431041\n",
      "Iteration 20, loss = 0.13307962\n",
      "Iteration 21, loss = 0.12181117\n",
      "Iteration 22, loss = 0.12405194\n",
      "Iteration 23, loss = 0.11260232\n",
      "Iteration 24, loss = 0.13086651\n",
      "Iteration 25, loss = 0.11214873\n",
      "Iteration 26, loss = 0.10840553\n",
      "Iteration 27, loss = 0.11574146\n",
      "Iteration 28, loss = 0.11856828\n",
      "Iteration 29, loss = 0.11356532\n",
      "Iteration 30, loss = 0.12799118\n",
      "Iteration 31, loss = 0.10961796\n",
      "Iteration 32, loss = 0.12102285\n",
      "Iteration 33, loss = 0.11365255\n",
      "Iteration 34, loss = 0.12398565\n",
      "Iteration 35, loss = 0.11674890\n",
      "Iteration 36, loss = 0.12080296\n",
      "Iteration 37, loss = 0.12351539\n",
      "Iteration 38, loss = 0.12658018\n",
      "Iteration 39, loss = 0.11310916\n",
      "Iteration 40, loss = 0.12793505\n",
      "Iteration 41, loss = 0.11055767\n",
      "Iteration 42, loss = 0.11178272\n",
      "Iteration 43, loss = 0.10662540\n",
      "Iteration 44, loss = 0.12519586\n",
      "Iteration 45, loss = 0.12381036\n",
      "Iteration 46, loss = 0.14046557\n",
      "Iteration 47, loss = 0.12224492\n",
      "Iteration 48, loss = 0.11353486\n",
      "Iteration 49, loss = 0.10514448\n",
      "Iteration 50, loss = 0.11534100\n",
      "Iteration 51, loss = 0.13510734\n",
      "Iteration 52, loss = 0.14117255\n",
      "Iteration 53, loss = 0.11315141\n",
      "Iteration 54, loss = 0.10508745\n",
      "Iteration 55, loss = 0.10732589\n",
      "Iteration 56, loss = 0.10286921\n",
      "Iteration 57, loss = 0.10608662\n",
      "Iteration 58, loss = 0.13126978\n",
      "Iteration 59, loss = 0.12593027\n",
      "Iteration 60, loss = 0.12121080\n",
      "Iteration 61, loss = 0.12297437\n",
      "Iteration 62, loss = 0.12146081\n",
      "Iteration 63, loss = 0.11035931\n",
      "Iteration 64, loss = 0.10988792\n",
      "Iteration 65, loss = 0.11300077\n",
      "Iteration 66, loss = 0.10743848\n",
      "Iteration 67, loss = 0.10735093\n",
      "Iteration 68, loss = 0.10218804\n",
      "Iteration 69, loss = 0.10600619\n",
      "Iteration 70, loss = 0.11057575\n",
      "Iteration 71, loss = 0.12196834\n",
      "Iteration 72, loss = 0.11193264\n",
      "Iteration 73, loss = 0.12021283\n",
      "Iteration 74, loss = 0.11325467\n",
      "Iteration 75, loss = 0.11955572\n",
      "Iteration 76, loss = 0.10511131\n",
      "Iteration 77, loss = 0.10861018\n",
      "Iteration 78, loss = 0.10798784\n",
      "Iteration 79, loss = 0.11447248\n",
      "Iteration 80, loss = 0.12041399\n",
      "Iteration 81, loss = 0.10608939\n",
      "Iteration 82, loss = 0.11002868\n",
      "Iteration 83, loss = 0.12305258\n",
      "Iteration 84, loss = 0.12338289\n",
      "Iteration 85, loss = 0.12388940\n",
      "Iteration 86, loss = 0.10900271\n",
      "Iteration 87, loss = 0.10913651\n",
      "Iteration 88, loss = 0.11081915\n",
      "Iteration 89, loss = 0.10447896\n",
      "Iteration 90, loss = 0.10564999\n",
      "Iteration 91, loss = 0.10472375\n",
      "Iteration 92, loss = 0.10838098\n",
      "Iteration 93, loss = 0.11021335\n",
      "Iteration 94, loss = 0.11461768\n",
      "Iteration 95, loss = 0.09994645\n",
      "Iteration 96, loss = 0.11095790\n",
      "Iteration 97, loss = 0.10599066\n",
      "Iteration 98, loss = 0.10332348\n",
      "Iteration 99, loss = 0.10575640\n",
      "Iteration 100, loss = 0.10313702\n",
      "Iteration 101, loss = 0.10936511\n",
      "Iteration 102, loss = 0.11176000\n",
      "Iteration 103, loss = 0.11703436\n",
      "Iteration 104, loss = 0.11355686\n",
      "Iteration 105, loss = 0.13423224\n",
      "Iteration 106, loss = 0.11574349\n",
      "Iteration 107, loss = 0.12393585\n",
      "Iteration 108, loss = 0.11991014\n",
      "Iteration 109, loss = 0.10620402\n",
      "Iteration 110, loss = 0.10547531\n",
      "Iteration 111, loss = 0.10551530\n",
      "Iteration 112, loss = 0.10552323\n",
      "Iteration 113, loss = 0.10637080\n",
      "Iteration 114, loss = 0.11470980\n",
      "Iteration 115, loss = 0.12409871\n",
      "Iteration 116, loss = 0.14238259\n",
      "Iteration 117, loss = 0.11920762\n",
      "Iteration 118, loss = 0.10873971\n",
      "Iteration 119, loss = 0.12120325\n",
      "Iteration 120, loss = 0.11387207\n",
      "Iteration 121, loss = 0.12340725\n",
      "Iteration 122, loss = 0.11662984\n",
      "Iteration 123, loss = 0.11029666\n",
      "Iteration 124, loss = 0.11612817\n",
      "Iteration 125, loss = 0.11083101\n",
      "Iteration 126, loss = 0.11200056\n",
      "Iteration 127, loss = 0.11809837\n",
      "Iteration 128, loss = 0.10458529\n",
      "Iteration 129, loss = 0.10850761\n",
      "Iteration 130, loss = 0.12184329\n",
      "Iteration 131, loss = 0.13038269\n",
      "Iteration 132, loss = 0.11498960\n",
      "Iteration 133, loss = 0.11053864\n",
      "Iteration 134, loss = 0.12852336\n",
      "Iteration 135, loss = 0.10659805\n",
      "Iteration 136, loss = 0.10751964\n",
      "Iteration 137, loss = 0.11073603\n",
      "Iteration 138, loss = 0.11420008\n",
      "Iteration 139, loss = 0.10597819\n",
      "Iteration 140, loss = 0.12417745\n",
      "Iteration 141, loss = 0.12638261\n",
      "Iteration 142, loss = 0.11831216\n",
      "Iteration 143, loss = 0.10751597\n",
      "Iteration 144, loss = 0.11019385\n",
      "Iteration 145, loss = 0.12574788\n",
      "Iteration 146, loss = 0.11458059\n",
      "Iteration 147, loss = 0.11084727\n",
      "Iteration 148, loss = 0.11554854\n",
      "Iteration 149, loss = 0.10840713\n",
      "Iteration 150, loss = 0.11295781\n",
      "Iteration 151, loss = 0.12096374\n",
      "Iteration 152, loss = 0.10589073\n",
      "Iteration 153, loss = 0.16751297\n",
      "Iteration 154, loss = 0.14184740\n",
      "Iteration 155, loss = 0.18295537\n",
      "Iteration 156, loss = 0.13252399\n",
      "Iteration 157, loss = 0.17886360\n",
      "Iteration 158, loss = 0.17358676\n",
      "Iteration 159, loss = 0.15836945\n",
      "Iteration 160, loss = 0.16397985\n",
      "Iteration 161, loss = 0.13309756\n",
      "Iteration 162, loss = 0.10738758\n",
      "Iteration 163, loss = 0.13490737\n",
      "Iteration 164, loss = 0.13411577\n",
      "Iteration 165, loss = 0.17006401\n",
      "Iteration 166, loss = 0.13184317\n",
      "Iteration 167, loss = 0.13909181\n",
      "Iteration 168, loss = 0.11417145\n",
      "Iteration 169, loss = 0.10510524\n",
      "Iteration 170, loss = 0.10891336\n",
      "Iteration 171, loss = 0.10720637\n",
      "Iteration 172, loss = 0.10850628\n",
      "Iteration 173, loss = 0.10951618\n",
      "Iteration 174, loss = 0.10406712\n",
      "Iteration 175, loss = 0.11703881\n",
      "Iteration 176, loss = 0.10664132\n",
      "Iteration 177, loss = 0.10785322\n",
      "Iteration 178, loss = 0.10751307\n",
      "Iteration 179, loss = 0.10737856\n",
      "Iteration 180, loss = 0.11099309\n",
      "Iteration 181, loss = 0.10790783\n",
      "Iteration 182, loss = 0.10489314\n",
      "Iteration 183, loss = 0.11043305\n",
      "Iteration 184, loss = 0.10469463\n",
      "Iteration 185, loss = 0.10481153\n",
      "Iteration 186, loss = 0.10717770\n",
      "Iteration 187, loss = 0.10459693\n",
      "Iteration 188, loss = 0.10361675\n",
      "Iteration 189, loss = 0.10394690\n",
      "Iteration 190, loss = 0.10622328\n",
      "Iteration 191, loss = 0.10254307\n",
      "Iteration 192, loss = 0.10776824\n",
      "Iteration 193, loss = 0.13012789\n",
      "Iteration 194, loss = 0.12049589\n",
      "Iteration 195, loss = 0.11831012\n",
      "Iteration 196, loss = 0.11398514\n",
      "Iteration 197, loss = 0.11556105\n",
      "Iteration 198, loss = 0.13818407\n",
      "Iteration 199, loss = 0.12062274\n",
      "Iteration 200, loss = 0.12363423\n",
      "Iteration 201, loss = 0.13656276\n",
      "Iteration 202, loss = 0.11138700\n",
      "Iteration 203, loss = 0.11429326\n",
      "Iteration 204, loss = 0.11852017\n",
      "Iteration 205, loss = 0.12577190\n",
      "Iteration 206, loss = 0.13243403\n",
      "Iteration 207, loss = 0.14076110\n",
      "Iteration 208, loss = 0.16485093\n",
      "Iteration 209, loss = 0.12765857\n",
      "Iteration 210, loss = 0.13322204\n",
      "Iteration 211, loss = 0.10225906\n",
      "Iteration 212, loss = 0.11586326\n",
      "Iteration 213, loss = 0.14194098\n",
      "Iteration 214, loss = 0.12170801\n",
      "Iteration 215, loss = 0.13047243\n",
      "Iteration 216, loss = 0.11857775\n",
      "Iteration 217, loss = 0.11320200\n",
      "Iteration 218, loss = 0.10849720\n",
      "Iteration 219, loss = 0.10703956\n",
      "Iteration 220, loss = 0.13000180\n",
      "Iteration 221, loss = 0.10516096\n",
      "Iteration 222, loss = 0.11468460\n",
      "Iteration 223, loss = 0.10636424\n",
      "Iteration 224, loss = 0.11994665\n",
      "Iteration 225, loss = 0.12087100\n",
      "Iteration 226, loss = 0.23680639\n",
      "Iteration 227, loss = 0.46889543\n",
      "Iteration 228, loss = 0.34803264\n",
      "Iteration 229, loss = 0.33511963\n",
      "Iteration 230, loss = 0.32487131\n",
      "Iteration 231, loss = 2.25859695\n",
      "Iteration 232, loss = 3.17259821\n",
      "Iteration 233, loss = 2.64061113\n",
      "Iteration 234, loss = 2.36690395\n",
      "Iteration 235, loss = 1.41210967\n",
      "Iteration 236, loss = 1.04258491\n",
      "Iteration 237, loss = 0.48367893\n",
      "Iteration 238, loss = 0.56605151\n",
      "Iteration 239, loss = 0.42564881\n",
      "Iteration 240, loss = 0.51808921\n",
      "Iteration 241, loss = 0.38197634\n",
      "Iteration 242, loss = 0.38804437\n",
      "Iteration 243, loss = 0.24955095\n",
      "Iteration 244, loss = 0.33914176\n",
      "Iteration 245, loss = 0.29088138\n",
      "Iteration 246, loss = 0.31984196\n",
      "Iteration 247, loss = 0.33723341\n",
      "Iteration 248, loss = 0.25605144\n",
      "Iteration 249, loss = 0.22330979\n",
      "Iteration 250, loss = 0.17309017\n",
      "Iteration 251, loss = 0.17151740\n",
      "Iteration 252, loss = 0.29329279\n",
      "Iteration 253, loss = 0.18270297\n",
      "Iteration 254, loss = 0.12624573\n",
      "Iteration 255, loss = 0.14051661\n",
      "Iteration 256, loss = 0.15080497\n",
      "Iteration 257, loss = 0.13416863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.14915725\n",
      "Iteration 259, loss = 0.12553364\n",
      "Iteration 260, loss = 0.18034731\n",
      "Iteration 261, loss = 0.19418473\n",
      "Iteration 262, loss = 0.25223095\n",
      "Iteration 263, loss = 0.21885358\n",
      "Iteration 264, loss = 0.27763090\n",
      "Iteration 265, loss = 0.27102275\n",
      "Iteration 266, loss = 0.14715250\n",
      "Iteration 267, loss = 0.13378699\n",
      "Iteration 268, loss = 0.11692776\n",
      "Iteration 269, loss = 0.12415367\n",
      "Iteration 270, loss = 0.11108865\n",
      "Iteration 271, loss = 0.10866639\n",
      "Iteration 272, loss = 0.11090023\n",
      "Iteration 273, loss = 0.11869341\n",
      "Iteration 274, loss = 0.12695883\n",
      "Iteration 275, loss = 0.13852234\n",
      "Iteration 276, loss = 0.13792888\n",
      "Iteration 277, loss = 0.11817480\n",
      "Iteration 278, loss = 0.11691668\n",
      "Iteration 279, loss = 0.12029841\n",
      "Iteration 280, loss = 0.11368909\n",
      "Iteration 281, loss = 0.11286520\n",
      "Iteration 282, loss = 0.12328465\n",
      "Iteration 283, loss = 0.11952907\n",
      "Iteration 284, loss = 0.14075221\n",
      "Iteration 285, loss = 0.14064490\n",
      "Iteration 286, loss = 0.10982190\n",
      "Iteration 287, loss = 0.13385434\n",
      "Iteration 288, loss = 0.13232123\n",
      "Iteration 289, loss = 0.11270886\n",
      "Iteration 290, loss = 0.11017559\n",
      "Iteration 291, loss = 0.11637825\n",
      "Iteration 292, loss = 0.14924557\n",
      "Iteration 293, loss = 0.17938401\n",
      "Iteration 294, loss = 0.12062743\n",
      "Iteration 295, loss = 0.12087265\n",
      "Iteration 296, loss = 0.10779854\n",
      "Iteration 297, loss = 0.13448943\n",
      "Iteration 298, loss = 0.11079610\n",
      "Iteration 299, loss = 0.12385178\n",
      "Iteration 300, loss = 0.12806636\n",
      "Iteration 301, loss = 0.11291726\n",
      "Iteration 302, loss = 0.11595796\n",
      "Iteration 303, loss = 0.10196487\n",
      "Iteration 304, loss = 0.11025830\n",
      "Iteration 305, loss = 0.10799919\n",
      "Iteration 306, loss = 0.10991970\n",
      "Iteration 307, loss = 0.12190347\n",
      "Iteration 308, loss = 0.11311855\n",
      "Iteration 309, loss = 0.10448576\n",
      "Iteration 310, loss = 0.11262184\n",
      "Iteration 311, loss = 0.10963942\n",
      "Iteration 312, loss = 0.11186310\n",
      "Iteration 313, loss = 0.11094678\n",
      "Iteration 314, loss = 0.10428576\n",
      "Iteration 315, loss = 0.11013495\n",
      "Iteration 316, loss = 0.11658934\n",
      "Iteration 317, loss = 0.10743394\n",
      "Iteration 318, loss = 0.10680189\n",
      "Iteration 319, loss = 0.10852312\n",
      "Iteration 320, loss = 0.11670170\n",
      "Iteration 321, loss = 0.10829494\n",
      "Iteration 322, loss = 0.11162926\n",
      "Iteration 323, loss = 0.12734454\n",
      "Iteration 324, loss = 0.11125246\n",
      "Iteration 325, loss = 0.11228414\n",
      "Iteration 326, loss = 0.11671167\n",
      "Iteration 327, loss = 0.11401341\n",
      "Iteration 328, loss = 0.14095212\n",
      "Iteration 329, loss = 0.11781641\n",
      "Iteration 330, loss = 0.13550996\n",
      "Iteration 331, loss = 0.12869616\n",
      "Iteration 332, loss = 0.10852906\n",
      "Iteration 333, loss = 0.10756571\n",
      "Iteration 334, loss = 0.10828567\n",
      "Iteration 335, loss = 0.10858860\n",
      "Iteration 336, loss = 0.12129225\n",
      "Iteration 337, loss = 0.11112176\n",
      "Iteration 338, loss = 0.12628393\n",
      "Iteration 339, loss = 0.12084385\n",
      "Iteration 340, loss = 0.10975653\n",
      "Iteration 341, loss = 0.10585611\n",
      "Iteration 342, loss = 0.10844456\n",
      "Iteration 343, loss = 0.10620831\n",
      "Iteration 344, loss = 0.11109802\n",
      "Iteration 345, loss = 0.11248913\n",
      "Iteration 346, loss = 0.11454007\n",
      "Iteration 347, loss = 0.11577419\n",
      "Iteration 348, loss = 0.11026888\n",
      "Iteration 349, loss = 0.10881693\n",
      "Iteration 350, loss = 0.12770943\n",
      "Iteration 351, loss = 0.10900414\n",
      "Iteration 352, loss = 0.10457796\n",
      "Iteration 353, loss = 0.11281208\n",
      "Iteration 354, loss = 0.10846705\n",
      "Iteration 355, loss = 0.11139371\n",
      "Iteration 356, loss = 0.11537711\n",
      "Iteration 357, loss = 0.10819148\n",
      "Iteration 358, loss = 0.10993285\n",
      "Iteration 359, loss = 0.10549351\n",
      "Iteration 360, loss = 0.11057869\n",
      "Iteration 361, loss = 0.11714184\n",
      "Iteration 362, loss = 0.13500569\n",
      "Iteration 363, loss = 0.11310555\n",
      "Iteration 364, loss = 0.11801091\n",
      "Iteration 365, loss = 0.12715710\n",
      "Iteration 366, loss = 0.11775746\n",
      "Iteration 367, loss = 0.10894230\n",
      "Iteration 368, loss = 0.11260094\n",
      "Iteration 369, loss = 0.11936043\n",
      "Iteration 370, loss = 0.11565307\n",
      "Iteration 371, loss = 0.11373752\n",
      "Iteration 372, loss = 0.11869798\n",
      "Iteration 373, loss = 0.11997935\n",
      "Iteration 374, loss = 0.11766734\n",
      "Iteration 375, loss = 0.12152347\n",
      "Iteration 376, loss = 0.10938308\n",
      "Iteration 377, loss = 0.11461466\n",
      "Iteration 378, loss = 0.10830143\n",
      "Iteration 379, loss = 0.10840344\n",
      "Iteration 380, loss = 0.12169299\n",
      "Iteration 381, loss = 0.11548085\n",
      "Iteration 382, loss = 0.11482112\n",
      "Iteration 383, loss = 0.11029546\n",
      "Iteration 384, loss = 0.12159312\n",
      "Iteration 385, loss = 0.12507284\n",
      "Iteration 386, loss = 0.12808737\n",
      "Iteration 387, loss = 0.10558437\n",
      "Iteration 388, loss = 0.11818419\n",
      "Iteration 389, loss = 0.11783942\n",
      "Iteration 390, loss = 0.12021976\n",
      "Iteration 391, loss = 0.11148282\n",
      "Iteration 392, loss = 0.11341896\n",
      "Iteration 393, loss = 0.12333506\n",
      "Iteration 394, loss = 0.11521917\n",
      "Iteration 395, loss = 0.11884448\n",
      "Iteration 396, loss = 0.12460485\n",
      "Iteration 397, loss = 0.10829404\n",
      "Iteration 398, loss = 0.11455837\n",
      "Iteration 399, loss = 0.10656060\n",
      "Iteration 400, loss = 0.13116248\n",
      "Iteration 401, loss = 0.13690054\n",
      "Iteration 402, loss = 0.13135370\n",
      "Iteration 403, loss = 0.16896746\n",
      "Iteration 404, loss = 0.16737448\n",
      "Iteration 405, loss = 0.17245422\n",
      "Iteration 406, loss = 0.14530608\n",
      "Iteration 407, loss = 0.24293484\n",
      "Iteration 408, loss = 0.26462041\n",
      "Iteration 409, loss = 0.14393853\n",
      "Iteration 410, loss = 0.14882817\n",
      "Iteration 411, loss = 0.12339782\n",
      "Iteration 412, loss = 0.12975457\n",
      "Iteration 413, loss = 0.11916715\n",
      "Iteration 414, loss = 0.11089075\n",
      "Iteration 415, loss = 0.12113996\n",
      "Iteration 416, loss = 0.11162343\n",
      "Iteration 417, loss = 0.10571509\n",
      "Iteration 418, loss = 0.12083379\n",
      "Iteration 419, loss = 0.11056779\n",
      "Iteration 420, loss = 0.12211087\n",
      "Iteration 421, loss = 0.11531810\n",
      "Iteration 422, loss = 0.11283500\n",
      "Iteration 423, loss = 0.10490607\n",
      "Iteration 424, loss = 0.12003794\n",
      "Iteration 425, loss = 0.11600771\n",
      "Iteration 426, loss = 0.11305402\n",
      "Iteration 427, loss = 0.10528008\n",
      "Iteration 428, loss = 0.11036881\n",
      "Iteration 429, loss = 0.11099223\n",
      "Iteration 430, loss = 0.11514226\n",
      "Iteration 431, loss = 0.10450900\n",
      "Iteration 432, loss = 0.11721042\n",
      "Iteration 433, loss = 0.11980342\n",
      "Iteration 434, loss = 0.10954224\n",
      "Iteration 435, loss = 0.10568160\n",
      "Iteration 436, loss = 0.10454535\n",
      "Iteration 437, loss = 0.10962960\n",
      "Iteration 438, loss = 0.11079924\n",
      "Iteration 439, loss = 0.10999504\n",
      "Iteration 440, loss = 0.10655396\n",
      "Iteration 441, loss = 0.12357705\n",
      "Iteration 442, loss = 0.11660853\n",
      "Iteration 443, loss = 0.11596197\n",
      "Iteration 444, loss = 0.11776897\n",
      "Iteration 445, loss = 0.13703806\n",
      "Iteration 446, loss = 0.12617462\n",
      "Iteration 447, loss = 0.11076957\n",
      "Iteration 448, loss = 0.10894295\n",
      "Iteration 449, loss = 0.10331129\n",
      "Iteration 450, loss = 0.10860621\n",
      "Iteration 451, loss = 0.10536043\n",
      "Iteration 452, loss = 0.10416924\n",
      "Iteration 453, loss = 0.10913306\n",
      "Iteration 454, loss = 0.10998535\n",
      "Iteration 455, loss = 0.12618907\n",
      "Iteration 456, loss = 0.11074891\n",
      "Iteration 457, loss = 0.11457503\n",
      "Iteration 458, loss = 0.10127804\n",
      "Iteration 459, loss = 0.11646690\n",
      "Iteration 460, loss = 0.11719509\n",
      "Iteration 461, loss = 0.10486703\n",
      "Iteration 462, loss = 0.11014379\n",
      "Iteration 463, loss = 0.10552370\n",
      "Iteration 464, loss = 0.11225045\n",
      "Iteration 465, loss = 0.10622451\n",
      "Iteration 466, loss = 0.11524059\n",
      "Iteration 467, loss = 0.11120909\n",
      "Iteration 468, loss = 0.11064377\n",
      "Iteration 469, loss = 0.11381940\n",
      "Iteration 470, loss = 0.11191585\n",
      "Iteration 471, loss = 0.11899216\n",
      "Iteration 472, loss = 0.11120813\n",
      "Iteration 473, loss = 0.12396409\n",
      "Iteration 474, loss = 0.11538305\n",
      "Iteration 475, loss = 0.11735991\n",
      "Iteration 476, loss = 0.11290795\n",
      "Iteration 477, loss = 0.11046406\n",
      "Iteration 478, loss = 0.10703378\n",
      "Iteration 479, loss = 0.10458779\n",
      "Iteration 480, loss = 0.11388793\n",
      "Iteration 481, loss = 0.10509980\n",
      "Iteration 482, loss = 0.11325476\n",
      "Iteration 483, loss = 0.10700537\n",
      "Iteration 484, loss = 0.10795054\n",
      "Iteration 485, loss = 0.10354420\n",
      "Iteration 486, loss = 0.10373641\n",
      "Iteration 487, loss = 0.11272144\n",
      "Iteration 488, loss = 0.10431814\n",
      "Iteration 489, loss = 0.14510536\n",
      "Iteration 490, loss = 0.11906665\n",
      "Iteration 491, loss = 0.10514626\n",
      "Iteration 492, loss = 0.10920760\n",
      "Iteration 493, loss = 0.11697135\n",
      "Iteration 494, loss = 0.10810671\n",
      "Iteration 495, loss = 0.10952116\n",
      "Iteration 496, loss = 0.10029618\n",
      "Iteration 497, loss = 0.13173714\n",
      "Iteration 498, loss = 0.21596133\n",
      "Iteration 499, loss = 0.55421892\n",
      "Iteration 500, loss = 0.45739726\n",
      "Iteration 501, loss = 0.34910159\n",
      "Iteration 502, loss = 0.24665113\n",
      "Iteration 503, loss = 0.18877175\n",
      "Iteration 504, loss = 0.18719200\n",
      "Iteration 505, loss = 0.13252399\n",
      "Iteration 506, loss = 0.12921847\n",
      "Iteration 507, loss = 0.10953947\n",
      "Iteration 508, loss = 0.11260590\n",
      "Iteration 509, loss = 0.11169447\n",
      "Iteration 510, loss = 0.11373949\n",
      "Iteration 511, loss = 0.11250543\n",
      "Iteration 512, loss = 0.11527049\n",
      "Iteration 513, loss = 0.12562877\n",
      "Iteration 514, loss = 0.11154582\n",
      "Iteration 515, loss = 0.11512816\n",
      "Iteration 516, loss = 0.12272006\n",
      "Iteration 517, loss = 0.12409520\n",
      "Iteration 518, loss = 0.12458905\n",
      "Iteration 519, loss = 0.10700691\n",
      "Iteration 520, loss = 0.10806166\n",
      "Iteration 521, loss = 0.11040296\n",
      "Iteration 522, loss = 0.12018846\n",
      "Iteration 523, loss = 0.10423640\n",
      "Iteration 524, loss = 0.10978275\n",
      "Iteration 525, loss = 0.11355804\n",
      "Iteration 526, loss = 0.10885104\n",
      "Iteration 527, loss = 0.11461463\n",
      "Iteration 528, loss = 0.11290932\n",
      "Iteration 529, loss = 0.11580185\n",
      "Iteration 530, loss = 0.12416642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 531, loss = 0.26425972\n",
      "Iteration 532, loss = 0.16891948\n",
      "Iteration 533, loss = 0.12710485\n",
      "Iteration 534, loss = 0.15005732\n",
      "Iteration 535, loss = 0.29318743\n",
      "Iteration 536, loss = 0.36299531\n",
      "Iteration 537, loss = 0.19169285\n",
      "Iteration 538, loss = 0.16630619\n",
      "Iteration 539, loss = 0.18419499\n",
      "Iteration 540, loss = 0.13915771\n",
      "Iteration 541, loss = 0.10970480\n",
      "Iteration 542, loss = 0.11912446\n",
      "Iteration 543, loss = 0.10974836\n",
      "Iteration 544, loss = 0.10961533\n",
      "Iteration 545, loss = 0.10854490\n",
      "Iteration 546, loss = 0.10426220\n",
      "Iteration 547, loss = 0.10819410\n",
      "Iteration 548, loss = 0.10787572\n",
      "Iteration 549, loss = 0.11493361\n",
      "Iteration 550, loss = 0.12051849\n",
      "Iteration 551, loss = 0.13489236\n",
      "Iteration 552, loss = 0.11775234\n",
      "Iteration 553, loss = 0.11470136\n",
      "Iteration 554, loss = 0.11901591\n",
      "Iteration 555, loss = 0.11240046\n",
      "Iteration 556, loss = 0.12129360\n",
      "Iteration 557, loss = 0.12070882\n",
      "Iteration 558, loss = 0.11113269\n",
      "Iteration 559, loss = 0.10811513\n",
      "Iteration 560, loss = 0.10563718\n",
      "Iteration 561, loss = 0.11139625\n",
      "Iteration 562, loss = 0.12097711\n",
      "Iteration 563, loss = 0.11561897\n",
      "Iteration 564, loss = 0.11352571\n",
      "Iteration 565, loss = 0.12259566\n",
      "Iteration 566, loss = 0.12898750\n",
      "Iteration 567, loss = 0.20498673\n",
      "Iteration 568, loss = 0.29763641\n",
      "Iteration 569, loss = 1.60517383\n",
      "Iteration 570, loss = 0.76912047\n",
      "Iteration 571, loss = 1.63376789\n",
      "Iteration 572, loss = 1.66134885\n",
      "Iteration 573, loss = 0.94075811\n",
      "Iteration 574, loss = 0.68766183\n",
      "Iteration 575, loss = 0.63200259\n",
      "Iteration 576, loss = 0.67631793\n",
      "Iteration 577, loss = 0.41248239\n",
      "Iteration 578, loss = 1.74400124\n",
      "Iteration 579, loss = 1.30288560\n",
      "Iteration 580, loss = 0.97156389\n",
      "Iteration 581, loss = 0.93988738\n",
      "Iteration 582, loss = 0.55500257\n",
      "Iteration 583, loss = 1.06066634\n",
      "Iteration 584, loss = 0.69612328\n",
      "Iteration 585, loss = 0.80189814\n",
      "Iteration 586, loss = 0.71402700\n",
      "Iteration 587, loss = 0.56164578\n",
      "Iteration 588, loss = 0.35166751\n",
      "Iteration 589, loss = 0.34955970\n",
      "Iteration 590, loss = 0.48808351\n",
      "Iteration 591, loss = 0.66503047\n",
      "Iteration 592, loss = 0.47236531\n",
      "Iteration 593, loss = 0.33782527\n",
      "Iteration 594, loss = 1.39479997\n",
      "Iteration 595, loss = 1.23963981\n",
      "Iteration 596, loss = 0.94701291\n",
      "Iteration 597, loss = 0.68692484\n",
      "Iteration 598, loss = 0.66678990\n",
      "Iteration 599, loss = 0.45141900\n",
      "Iteration 600, loss = 0.57311327\n",
      "Iteration 601, loss = 0.54984372\n",
      "Iteration 602, loss = 0.45630867\n",
      "Iteration 603, loss = 0.35704400\n",
      "Iteration 604, loss = 0.35235688\n",
      "Iteration 605, loss = 0.32639820\n",
      "Iteration 606, loss = 0.31839211\n",
      "Iteration 607, loss = 0.25120331\n",
      "Iteration 608, loss = 0.22789280\n",
      "Iteration 609, loss = 0.14495017\n",
      "Iteration 610, loss = 0.20947728\n",
      "Iteration 611, loss = 0.23513489\n",
      "Iteration 612, loss = 0.18642199\n",
      "Iteration 613, loss = 0.18835457\n",
      "Iteration 614, loss = 0.13377755\n",
      "Iteration 615, loss = 0.11977587\n",
      "Iteration 616, loss = 0.11754436\n",
      "Iteration 617, loss = 0.11735410\n",
      "Iteration 618, loss = 0.12315800\n",
      "Iteration 619, loss = 0.11442685\n",
      "Iteration 620, loss = 0.13470252\n",
      "Iteration 621, loss = 0.12058131\n",
      "Iteration 622, loss = 0.12721389\n",
      "Iteration 623, loss = 0.10666295\n",
      "Iteration 624, loss = 0.14316893\n",
      "Iteration 625, loss = 0.12388688\n",
      "Iteration 626, loss = 0.13349335\n",
      "Iteration 627, loss = 0.12950814\n",
      "Iteration 628, loss = 0.13184744\n",
      "Iteration 629, loss = 0.12330959\n",
      "Iteration 630, loss = 0.13896175\n",
      "Iteration 631, loss = 0.10478569\n",
      "Iteration 632, loss = 0.11256777\n",
      "Iteration 633, loss = 0.10904641\n",
      "Iteration 634, loss = 0.10547644\n",
      "Iteration 635, loss = 0.11013635\n",
      "Iteration 636, loss = 0.10755778\n",
      "Iteration 637, loss = 0.11187059\n",
      "Iteration 638, loss = 0.11935919\n",
      "Iteration 639, loss = 0.13256736\n",
      "Iteration 640, loss = 0.12190665\n",
      "Iteration 641, loss = 0.10843549\n",
      "Iteration 642, loss = 0.10876787\n",
      "Iteration 643, loss = 0.10496734\n",
      "Iteration 644, loss = 0.11266378\n",
      "Iteration 645, loss = 0.11615213\n",
      "Iteration 646, loss = 0.11477861\n",
      "Iteration 647, loss = 0.10412885\n",
      "Iteration 648, loss = 0.10744900\n",
      "Iteration 649, loss = 0.10831353\n",
      "Iteration 650, loss = 0.10935092\n",
      "Iteration 651, loss = 0.11558396\n",
      "Iteration 652, loss = 0.11584154\n",
      "Iteration 653, loss = 0.11097330\n",
      "Iteration 654, loss = 0.10764347\n",
      "Iteration 655, loss = 0.10741318\n",
      "Iteration 656, loss = 0.10638238\n",
      "Iteration 657, loss = 0.10455121\n",
      "Iteration 658, loss = 0.10598021\n",
      "Iteration 659, loss = 0.10978467\n",
      "Iteration 660, loss = 0.10693525\n",
      "Iteration 661, loss = 0.10927982\n",
      "Iteration 662, loss = 0.10972789\n",
      "Iteration 663, loss = 0.10648195\n",
      "Iteration 664, loss = 0.10925126\n",
      "Iteration 665, loss = 0.10843350\n",
      "Iteration 666, loss = 0.10353142\n",
      "Iteration 667, loss = 0.11620457\n",
      "Iteration 668, loss = 0.11288487\n",
      "Iteration 669, loss = 0.10902749\n",
      "Iteration 670, loss = 0.11245841\n",
      "Iteration 671, loss = 0.10958420\n",
      "Iteration 672, loss = 0.11175674\n",
      "Iteration 673, loss = 0.11208542\n",
      "Iteration 674, loss = 0.11430030\n",
      "Iteration 675, loss = 0.10684793\n",
      "Iteration 676, loss = 0.10885484\n",
      "Iteration 677, loss = 0.10534436\n",
      "Iteration 678, loss = 0.10427756\n",
      "Iteration 679, loss = 0.10531617\n",
      "Iteration 680, loss = 0.10719533\n",
      "Iteration 681, loss = 0.10809161\n",
      "Iteration 682, loss = 0.10742741\n",
      "Iteration 683, loss = 0.10875413\n",
      "Iteration 684, loss = 0.10496241\n",
      "Iteration 685, loss = 0.11554263\n",
      "Iteration 686, loss = 0.11350631\n",
      "Iteration 687, loss = 0.10719105\n",
      "Iteration 688, loss = 0.10923954\n",
      "Iteration 689, loss = 0.10834134\n",
      "Iteration 690, loss = 0.10978363\n",
      "Iteration 691, loss = 0.10696101\n",
      "Iteration 692, loss = 0.10738283\n",
      "Iteration 693, loss = 0.10960403\n",
      "Iteration 694, loss = 0.10805971\n",
      "Iteration 695, loss = 0.11512224\n",
      "Iteration 696, loss = 0.11051990\n",
      "Iteration 697, loss = 0.10977276\n",
      "Iteration 698, loss = 0.10362988\n",
      "Iteration 699, loss = 0.10405014\n",
      "Iteration 700, loss = 0.10682868\n",
      "Iteration 701, loss = 0.11099736\n",
      "Iteration 702, loss = 0.11063682\n",
      "Iteration 703, loss = 0.11001932\n",
      "Iteration 704, loss = 0.10614395\n",
      "Iteration 705, loss = 0.11729277\n",
      "Iteration 706, loss = 0.10489222\n",
      "Iteration 707, loss = 0.11911435\n",
      "Iteration 708, loss = 0.11089189\n",
      "Iteration 709, loss = 0.11125683\n",
      "Iteration 710, loss = 0.10618156\n",
      "Iteration 711, loss = 0.10653143\n",
      "Iteration 712, loss = 0.10748833\n",
      "Iteration 713, loss = 0.12959674\n",
      "Iteration 714, loss = 0.12245356\n",
      "Iteration 715, loss = 0.12375180\n",
      "Iteration 716, loss = 0.10502239\n",
      "Iteration 717, loss = 0.11102803\n",
      "Iteration 718, loss = 0.10727023\n",
      "Iteration 719, loss = 0.11818931\n",
      "Iteration 720, loss = 0.10493487\n",
      "Iteration 721, loss = 0.11965748\n",
      "Iteration 722, loss = 0.10517861\n",
      "Iteration 723, loss = 0.10423906\n",
      "Iteration 724, loss = 0.10704307\n",
      "Iteration 725, loss = 0.10715132\n",
      "Iteration 726, loss = 0.11102492\n",
      "Iteration 727, loss = 0.10928048\n",
      "Iteration 728, loss = 0.10467542\n",
      "Iteration 729, loss = 0.10460489\n",
      "Iteration 730, loss = 0.10988417\n",
      "Iteration 731, loss = 0.10818080\n",
      "Iteration 732, loss = 0.11020698\n",
      "Iteration 733, loss = 0.13360022\n",
      "Iteration 734, loss = 0.10812053\n",
      "Iteration 735, loss = 0.12703641\n",
      "Iteration 736, loss = 0.10656351\n",
      "Iteration 737, loss = 0.10887612\n",
      "Iteration 738, loss = 0.12080061\n",
      "Iteration 739, loss = 0.10948816\n",
      "Iteration 740, loss = 0.10559346\n",
      "Iteration 741, loss = 0.11092383\n",
      "Iteration 742, loss = 0.10756022\n",
      "Iteration 743, loss = 0.10554028\n",
      "Iteration 744, loss = 0.10816641\n",
      "Iteration 745, loss = 0.10951718\n",
      "Iteration 746, loss = 0.10709862\n",
      "Iteration 747, loss = 0.11099828\n",
      "Iteration 748, loss = 0.10473358\n",
      "Iteration 749, loss = 0.11181240\n",
      "Iteration 750, loss = 0.10498983\n",
      "Iteration 751, loss = 0.10462604\n",
      "Iteration 752, loss = 0.11676054\n",
      "Iteration 753, loss = 0.11996185\n",
      "Iteration 754, loss = 0.11568931\n",
      "Iteration 755, loss = 0.11607463\n",
      "Iteration 756, loss = 0.10463411\n",
      "Iteration 757, loss = 0.10929254\n",
      "Iteration 758, loss = 0.10538973\n",
      "Iteration 759, loss = 0.11043862\n",
      "Iteration 760, loss = 0.10863893\n",
      "Iteration 761, loss = 0.10771667\n",
      "Iteration 762, loss = 0.11691013\n",
      "Iteration 763, loss = 0.10465581\n",
      "Iteration 764, loss = 0.10387674\n",
      "Iteration 765, loss = 0.10800951\n",
      "Iteration 766, loss = 0.11123539\n",
      "Iteration 767, loss = 0.10858146\n",
      "Iteration 768, loss = 0.11298149\n",
      "Iteration 769, loss = 0.11566696\n",
      "Iteration 770, loss = 0.11610840\n",
      "Iteration 771, loss = 0.11993445\n",
      "Iteration 772, loss = 0.11299897\n",
      "Iteration 773, loss = 0.11697680\n",
      "Iteration 774, loss = 0.12790125\n",
      "Iteration 775, loss = 0.12626069\n",
      "Iteration 776, loss = 0.10278064\n",
      "Iteration 777, loss = 0.11671462\n",
      "Iteration 778, loss = 0.10615926\n",
      "Iteration 779, loss = 0.10617466\n",
      "Iteration 780, loss = 0.15333200\n",
      "Iteration 781, loss = 0.13470559\n",
      "Iteration 782, loss = 0.23131341\n",
      "Iteration 783, loss = 0.15363327\n",
      "Iteration 784, loss = 0.14013146\n",
      "Iteration 785, loss = 0.15636555\n",
      "Iteration 786, loss = 0.25166147\n",
      "Iteration 787, loss = 0.19017091\n",
      "Iteration 788, loss = 0.14897912\n",
      "Iteration 789, loss = 0.13333488\n",
      "Iteration 790, loss = 0.11620763\n",
      "Iteration 791, loss = 0.11263376\n",
      "Iteration 792, loss = 0.11137321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 793, loss = 0.10459320\n",
      "Iteration 794, loss = 0.11591551\n",
      "Iteration 795, loss = 0.10506430\n",
      "Iteration 796, loss = 0.10877228\n",
      "Iteration 797, loss = 0.10786259\n",
      "Iteration 798, loss = 0.10610113\n",
      "Iteration 799, loss = 0.11667192\n",
      "Iteration 800, loss = 0.11218726\n",
      "Iteration 801, loss = 0.11442524\n",
      "Iteration 802, loss = 0.11065043\n",
      "Iteration 803, loss = 0.11546787\n",
      "Iteration 804, loss = 0.11341207\n",
      "Iteration 805, loss = 0.11830519\n",
      "Iteration 806, loss = 0.11056868\n",
      "Iteration 807, loss = 0.12245770\n",
      "Iteration 808, loss = 0.10967351\n",
      "Iteration 809, loss = 0.10865383\n",
      "Iteration 810, loss = 0.11459746\n",
      "Iteration 811, loss = 0.10835163\n",
      "Iteration 812, loss = 0.10963144\n",
      "Iteration 813, loss = 0.11427038\n",
      "Iteration 814, loss = 0.13730402\n",
      "Iteration 815, loss = 0.10368721\n",
      "Iteration 816, loss = 0.10579547\n",
      "Iteration 817, loss = 0.11212556\n",
      "Iteration 818, loss = 0.11213256\n",
      "Iteration 819, loss = 0.11454814\n",
      "Iteration 820, loss = 0.12875214\n",
      "Iteration 821, loss = 0.10725420\n",
      "Iteration 822, loss = 0.11896453\n",
      "Iteration 823, loss = 0.11557464\n",
      "Iteration 824, loss = 0.11602891\n",
      "Iteration 825, loss = 0.10692745\n",
      "Iteration 826, loss = 0.10776292\n",
      "Iteration 827, loss = 0.11711831\n",
      "Iteration 828, loss = 0.10508275\n",
      "Iteration 829, loss = 0.10560901\n",
      "Iteration 830, loss = 0.10319488\n",
      "Iteration 831, loss = 0.10637375\n",
      "Iteration 832, loss = 0.10825039\n",
      "Iteration 833, loss = 0.10739626\n",
      "Iteration 834, loss = 0.11157579\n",
      "Iteration 835, loss = 0.11441306\n",
      "Iteration 836, loss = 0.10822437\n",
      "Iteration 837, loss = 0.10618406\n",
      "Iteration 838, loss = 0.11667622\n",
      "Iteration 839, loss = 0.11182727\n",
      "Iteration 840, loss = 0.10784244\n",
      "Iteration 841, loss = 0.11846899\n",
      "Iteration 842, loss = 0.11631699\n",
      "Iteration 843, loss = 0.13636613\n",
      "Iteration 844, loss = 0.10472637\n",
      "Iteration 845, loss = 0.12505404\n",
      "Iteration 846, loss = 0.13027803\n",
      "Iteration 847, loss = 0.12886392\n",
      "Iteration 848, loss = 0.12450268\n",
      "Iteration 849, loss = 0.13751867\n",
      "Iteration 850, loss = 0.11952570\n",
      "Iteration 851, loss = 0.11036896\n",
      "Iteration 852, loss = 0.11699032\n",
      "Iteration 853, loss = 0.10719885\n",
      "Iteration 854, loss = 0.11047315\n",
      "Iteration 855, loss = 0.10907752\n",
      "Iteration 856, loss = 0.13470065\n",
      "Iteration 857, loss = 0.14078440\n",
      "Iteration 858, loss = 0.14889105\n",
      "Iteration 859, loss = 0.15093321\n",
      "Iteration 860, loss = 0.13766303\n",
      "Iteration 861, loss = 0.12305914\n",
      "Iteration 862, loss = 0.12460642\n",
      "Iteration 863, loss = 0.11063980\n",
      "Iteration 864, loss = 0.11557526\n",
      "Iteration 865, loss = 0.11552563\n",
      "Iteration 866, loss = 0.11207668\n",
      "Iteration 867, loss = 0.12083947\n",
      "Iteration 868, loss = 0.11275806\n",
      "Iteration 869, loss = 0.11011473\n",
      "Iteration 870, loss = 0.11583375\n",
      "Iteration 871, loss = 0.09798239\n",
      "Iteration 872, loss = 0.12876533\n",
      "Iteration 873, loss = 0.12097644\n",
      "Iteration 874, loss = 0.14235262\n",
      "Iteration 875, loss = 0.14871321\n",
      "Iteration 876, loss = 0.13416951\n",
      "Iteration 877, loss = 0.12478749\n",
      "Iteration 878, loss = 0.17788210\n",
      "Iteration 879, loss = 0.17068334\n",
      "Iteration 880, loss = 0.16095576\n",
      "Iteration 881, loss = 0.12400316\n",
      "Iteration 882, loss = 0.12078295\n",
      "Iteration 883, loss = 0.11440756\n",
      "Iteration 884, loss = 0.12924695\n",
      "Iteration 885, loss = 0.11321875\n",
      "Iteration 886, loss = 0.11826298\n",
      "Iteration 887, loss = 0.15258603\n",
      "Iteration 888, loss = 0.16087832\n",
      "Iteration 889, loss = 0.15422947\n",
      "Iteration 890, loss = 0.13603101\n",
      "Iteration 891, loss = 0.14110643\n",
      "Iteration 892, loss = 0.14750953\n",
      "Iteration 893, loss = 0.14453020\n",
      "Iteration 894, loss = 0.16061570\n",
      "Iteration 895, loss = 0.14406029\n",
      "Iteration 896, loss = 0.15567969\n",
      "Iteration 897, loss = 0.13699987\n",
      "Iteration 898, loss = 0.12147643\n",
      "Iteration 899, loss = 0.12676586\n",
      "Iteration 900, loss = 0.12572467\n",
      "Iteration 901, loss = 0.11670446\n",
      "Iteration 902, loss = 0.11383307\n",
      "Iteration 903, loss = 0.10727558\n",
      "Iteration 904, loss = 0.11768497\n",
      "Iteration 905, loss = 0.12094387\n",
      "Iteration 906, loss = 0.13565302\n",
      "Iteration 907, loss = 0.13569973\n",
      "Iteration 908, loss = 0.14598581\n",
      "Iteration 909, loss = 0.13061304\n",
      "Iteration 910, loss = 0.12707947\n",
      "Iteration 911, loss = 0.11130673\n",
      "Iteration 912, loss = 0.11055552\n",
      "Iteration 913, loss = 0.11967200\n",
      "Iteration 914, loss = 0.10909228\n",
      "Iteration 915, loss = 0.14582834\n",
      "Iteration 916, loss = 0.10585071\n",
      "Iteration 917, loss = 0.12277352\n",
      "Iteration 918, loss = 0.10447960\n",
      "Iteration 919, loss = 0.11155893\n",
      "Iteration 920, loss = 0.11206612\n",
      "Iteration 921, loss = 0.10970965\n",
      "Iteration 922, loss = 0.10793531\n",
      "Iteration 923, loss = 0.10721924\n",
      "Iteration 924, loss = 0.11845072\n",
      "Iteration 925, loss = 0.11795807\n",
      "Iteration 926, loss = 0.13065187\n",
      "Iteration 927, loss = 0.11001592\n",
      "Iteration 928, loss = 0.11874219\n",
      "Iteration 929, loss = 0.10894045\n",
      "Iteration 930, loss = 0.10563250\n",
      "Iteration 931, loss = 0.11418593\n",
      "Iteration 932, loss = 0.10604233\n",
      "Iteration 933, loss = 0.12094989\n",
      "Iteration 934, loss = 0.11460799\n",
      "Iteration 935, loss = 0.11327561\n",
      "Iteration 936, loss = 0.10675687\n",
      "Iteration 937, loss = 0.11726704\n",
      "Iteration 938, loss = 0.11160826\n",
      "Iteration 939, loss = 0.10741422\n",
      "Iteration 940, loss = 0.10591445\n",
      "Iteration 941, loss = 0.12737832\n",
      "Iteration 942, loss = 0.11227654\n",
      "Iteration 943, loss = 0.14731850\n",
      "Iteration 944, loss = 0.12033994\n",
      "Iteration 945, loss = 0.18470972\n",
      "Iteration 946, loss = 0.23779836\n",
      "Iteration 947, loss = 0.43959605\n",
      "Iteration 948, loss = 3.32566783\n",
      "Iteration 949, loss = 4.93598188\n",
      "Iteration 950, loss = 2.98241692\n",
      "Iteration 951, loss = 1.68188687\n",
      "Iteration 952, loss = 1.48936805\n",
      "Iteration 953, loss = 1.41307042\n",
      "Iteration 954, loss = 1.71762612\n",
      "Iteration 955, loss = 1.08603903\n",
      "Iteration 956, loss = 1.10254627\n",
      "Iteration 957, loss = 1.15246997\n",
      "Iteration 958, loss = 0.60313726\n",
      "Iteration 959, loss = 0.60345989\n",
      "Iteration 960, loss = 0.64209022\n",
      "Iteration 961, loss = 0.57653636\n",
      "Iteration 962, loss = 1.13836246\n",
      "Iteration 963, loss = 0.69888172\n",
      "Iteration 964, loss = 0.62785495\n",
      "Iteration 965, loss = 0.56260447\n",
      "Iteration 966, loss = 0.40810679\n",
      "Iteration 967, loss = 0.42559456\n",
      "Iteration 968, loss = 0.51965499\n",
      "Iteration 969, loss = 0.40486535\n",
      "Iteration 970, loss = 0.23827154\n",
      "Iteration 971, loss = 0.25355199\n",
      "Iteration 972, loss = 0.18391883\n",
      "Iteration 973, loss = 0.17417335\n",
      "Iteration 974, loss = 0.17189830\n",
      "Iteration 975, loss = 0.31412789\n",
      "Iteration 976, loss = 0.27284960\n",
      "Iteration 977, loss = 0.29244200\n",
      "Iteration 978, loss = 0.16844046\n",
      "Iteration 979, loss = 0.21497817\n",
      "Iteration 980, loss = 0.23923973\n",
      "Iteration 981, loss = 0.15561342\n",
      "Iteration 982, loss = 0.12644875\n",
      "Iteration 983, loss = 0.13451108\n",
      "Iteration 984, loss = 0.12872707\n",
      "Iteration 985, loss = 0.16152044\n",
      "Iteration 986, loss = 0.13530880\n",
      "Iteration 987, loss = 0.17750970\n",
      "Iteration 988, loss = 0.15778090\n",
      "Iteration 989, loss = 0.15277234\n",
      "Iteration 990, loss = 0.19876619\n",
      "Iteration 991, loss = 0.18352520\n",
      "Iteration 992, loss = 0.24898374\n",
      "Iteration 993, loss = 0.35426087\n",
      "Iteration 994, loss = 0.27074012\n",
      "Iteration 995, loss = 0.26631175\n",
      "Iteration 996, loss = 0.21781867\n",
      "Iteration 997, loss = 0.20901175\n",
      "Iteration 998, loss = 0.14619041\n",
      "Iteration 999, loss = 0.17234226\n",
      "Iteration 1000, loss = 0.21018265\n",
      "Iteration 1001, loss = 0.18230142\n",
      "Iteration 1002, loss = 0.17570575\n",
      "Iteration 1003, loss = 0.17012614\n",
      "Iteration 1004, loss = 0.17312158\n",
      "Iteration 1005, loss = 0.16777785\n",
      "Iteration 1006, loss = 0.12890743\n",
      "Iteration 1007, loss = 0.19141057\n",
      "Iteration 1008, loss = 0.24510107\n",
      "Iteration 1009, loss = 0.19187000\n",
      "Iteration 1010, loss = 0.15252292\n",
      "Iteration 1011, loss = 0.12641962\n",
      "Iteration 1012, loss = 0.17896228\n",
      "Iteration 1013, loss = 0.15126663\n",
      "Iteration 1014, loss = 0.13890564\n",
      "Iteration 1015, loss = 0.11432233\n",
      "Iteration 1016, loss = 0.12549114\n",
      "Iteration 1017, loss = 0.30243803\n",
      "Iteration 1018, loss = 0.18723942\n",
      "Iteration 1019, loss = 0.13184494\n",
      "Iteration 1020, loss = 0.11593610\n",
      "Iteration 1021, loss = 0.12632168\n",
      "Iteration 1022, loss = 0.19452046\n",
      "Iteration 1023, loss = 0.18694206\n",
      "Iteration 1024, loss = 0.21334784\n",
      "Iteration 1025, loss = 0.20553262\n",
      "Iteration 1026, loss = 0.39680361\n",
      "Iteration 1027, loss = 0.39461947\n",
      "Iteration 1028, loss = 0.26721484\n",
      "Iteration 1029, loss = 0.21498387\n",
      "Iteration 1030, loss = 0.22339039\n",
      "Iteration 1031, loss = 0.56177630\n",
      "Iteration 1032, loss = 0.48204303\n",
      "Iteration 1033, loss = 0.48921299\n",
      "Iteration 1034, loss = 0.37878844\n",
      "Iteration 1035, loss = 0.20474605\n",
      "Iteration 1036, loss = 0.16085855\n",
      "Iteration 1037, loss = 0.11837140\n",
      "Iteration 1038, loss = 0.12171369\n",
      "Iteration 1039, loss = 0.10802176\n",
      "Iteration 1040, loss = 0.12611887\n",
      "Iteration 1041, loss = 0.11004882\n",
      "Iteration 1042, loss = 0.12817843\n",
      "Iteration 1043, loss = 0.13872350\n",
      "Iteration 1044, loss = 0.11037746\n",
      "Iteration 1045, loss = 0.14102023\n",
      "Iteration 1046, loss = 0.15026934\n",
      "Iteration 1047, loss = 0.16907349\n",
      "Iteration 1048, loss = 0.21519826\n",
      "Iteration 1049, loss = 0.16741837\n",
      "Iteration 1050, loss = 0.17558609\n",
      "Iteration 1051, loss = 0.11229004\n",
      "Iteration 1052, loss = 0.15707574\n",
      "Iteration 1053, loss = 0.23139424\n",
      "Iteration 1054, loss = 0.23402247\n",
      "Iteration 1055, loss = 0.18992245\n",
      "Iteration 1056, loss = 0.50185185\n",
      "Iteration 1057, loss = 0.43066868\n",
      "Iteration 1058, loss = 0.32818751\n",
      "Iteration 1059, loss = 0.49617623\n",
      "Iteration 1060, loss = 0.33533031\n",
      "Iteration 1061, loss = 0.24037271\n",
      "Iteration 1062, loss = 0.18724991\n",
      "Iteration 1063, loss = 0.14938120\n",
      "Iteration 1064, loss = 0.12369186\n",
      "Iteration 1065, loss = 0.14894237\n",
      "Iteration 1066, loss = 0.13174089\n",
      "Iteration 1067, loss = 0.12313919\n",
      "Iteration 1068, loss = 0.11540179\n",
      "Iteration 1069, loss = 0.12585323\n",
      "Iteration 1070, loss = 0.11016072\n",
      "Iteration 1071, loss = 0.10502121\n",
      "Iteration 1072, loss = 0.10820260\n",
      "Iteration 1073, loss = 0.11168061\n",
      "Iteration 1074, loss = 0.11702615\n",
      "Iteration 1075, loss = 0.11322231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1076, loss = 0.13778044\n",
      "Iteration 1077, loss = 0.12255247\n",
      "Iteration 1078, loss = 0.10861126\n",
      "Iteration 1079, loss = 0.11557789\n",
      "Iteration 1080, loss = 0.11324365\n",
      "Iteration 1081, loss = 0.11223911\n",
      "Iteration 1082, loss = 0.12431330\n",
      "Iteration 1083, loss = 0.12125708\n",
      "Iteration 1084, loss = 0.11329453\n",
      "Iteration 1085, loss = 0.10951222\n",
      "Iteration 1086, loss = 0.16558562\n",
      "Iteration 1087, loss = 0.21684706\n",
      "Iteration 1088, loss = 0.14668354\n",
      "Iteration 1089, loss = 0.12992544\n",
      "Iteration 1090, loss = 0.15205450\n",
      "Iteration 1091, loss = 0.17483819\n",
      "Iteration 1092, loss = 0.13698329\n",
      "Iteration 1093, loss = 0.14393499\n",
      "Iteration 1094, loss = 0.12893538\n",
      "Iteration 1095, loss = 0.15070263\n",
      "Iteration 1096, loss = 0.12214149\n",
      "Iteration 1097, loss = 0.10836443\n",
      "Iteration 1098, loss = 0.13281618\n",
      "Iteration 1099, loss = 0.13098823\n",
      "Iteration 1100, loss = 0.13235440\n",
      "Iteration 1101, loss = 0.11571666\n",
      "Iteration 1102, loss = 0.10288835\n",
      "Iteration 1103, loss = 0.11501626\n",
      "Iteration 1104, loss = 0.11246587\n",
      "Iteration 1105, loss = 0.11830204\n",
      "Iteration 1106, loss = 0.10763720\n",
      "Iteration 1107, loss = 0.10638742\n",
      "Iteration 1108, loss = 0.10732984\n",
      "Iteration 1109, loss = 0.10582007\n",
      "Iteration 1110, loss = 0.12186735\n",
      "Iteration 1111, loss = 0.10650321\n",
      "Iteration 1112, loss = 0.10712006\n",
      "Iteration 1113, loss = 0.13618725\n",
      "Iteration 1114, loss = 0.15047977\n",
      "Iteration 1115, loss = 0.20101829\n",
      "Iteration 1116, loss = 0.13459309\n",
      "Iteration 1117, loss = 0.11930914\n",
      "Iteration 1118, loss = 0.12681570\n",
      "Iteration 1119, loss = 0.11343847\n",
      "Iteration 1120, loss = 0.11790841\n",
      "Iteration 1121, loss = 0.10868387\n",
      "Iteration 1122, loss = 0.11021912\n",
      "Iteration 1123, loss = 0.12405327\n",
      "Iteration 1124, loss = 0.11992457\n",
      "Iteration 1125, loss = 0.10992911\n",
      "Iteration 1126, loss = 0.11725618\n",
      "Iteration 1127, loss = 0.10781296\n",
      "Iteration 1128, loss = 0.10743809\n",
      "Iteration 1129, loss = 0.15742188\n",
      "Iteration 1130, loss = 0.13976869\n",
      "Iteration 1131, loss = 0.17988372\n",
      "Iteration 1132, loss = 0.13147324\n",
      "Iteration 1133, loss = 0.11983251\n",
      "Iteration 1134, loss = 0.11220604\n",
      "Iteration 1135, loss = 0.11911165\n",
      "Iteration 1136, loss = 0.11412900\n",
      "Iteration 1137, loss = 0.11447574\n",
      "Iteration 1138, loss = 0.10651512\n",
      "Iteration 1139, loss = 0.10460326\n",
      "Iteration 1140, loss = 0.10593148\n",
      "Iteration 1141, loss = 0.11613810\n",
      "Iteration 1142, loss = 0.12438122\n",
      "Iteration 1143, loss = 0.11471234\n",
      "Iteration 1144, loss = 0.10994684\n",
      "Iteration 1145, loss = 0.11561588\n",
      "Iteration 1146, loss = 0.10334406\n",
      "Iteration 1147, loss = 0.12299394\n",
      "Iteration 1148, loss = 0.10816512\n",
      "Iteration 1149, loss = 0.10613592\n",
      "Iteration 1150, loss = 0.10845432\n",
      "Iteration 1151, loss = 0.11768751\n",
      "Iteration 1152, loss = 0.10201337\n",
      "Iteration 1153, loss = 0.11560723\n",
      "Iteration 1154, loss = 0.11363469\n",
      "Iteration 1155, loss = 0.11361211\n",
      "Iteration 1156, loss = 0.10673913\n",
      "Iteration 1157, loss = 0.11163253\n",
      "Iteration 1158, loss = 0.10918449\n",
      "Iteration 1159, loss = 0.10382389\n",
      "Iteration 1160, loss = 0.11287539\n",
      "Iteration 1161, loss = 0.11324439\n",
      "Iteration 1162, loss = 0.10611668\n",
      "Iteration 1163, loss = 0.10704899\n",
      "Iteration 1164, loss = 0.10924231\n",
      "Iteration 1165, loss = 0.11687742\n",
      "Iteration 1166, loss = 0.12102078\n",
      "Iteration 1167, loss = 0.12099846\n",
      "Iteration 1168, loss = 0.10567074\n",
      "Iteration 1169, loss = 0.11003946\n",
      "Iteration 1170, loss = 0.13645914\n",
      "Iteration 1171, loss = 0.12164474\n",
      "Iteration 1172, loss = 0.14267174\n",
      "Iteration 1173, loss = 0.13173871\n",
      "Iteration 1174, loss = 0.11587808\n",
      "Iteration 1175, loss = 0.14092949\n",
      "Iteration 1176, loss = 0.12993609\n",
      "Iteration 1177, loss = 0.13870191\n",
      "Iteration 1178, loss = 0.11245209\n",
      "Iteration 1179, loss = 0.12769988\n",
      "Iteration 1180, loss = 0.12374300\n",
      "Iteration 1181, loss = 0.13121913\n",
      "Iteration 1182, loss = 0.14202006\n",
      "Iteration 1183, loss = 0.14290053\n",
      "Iteration 1184, loss = 0.16098254\n",
      "Iteration 1185, loss = 0.12881893\n",
      "Iteration 1186, loss = 0.12292119\n",
      "Iteration 1187, loss = 0.10425652\n",
      "Iteration 1188, loss = 0.10657433\n",
      "Iteration 1189, loss = 0.11113619\n",
      "Iteration 1190, loss = 0.11557047\n",
      "Iteration 1191, loss = 0.13822365\n",
      "Iteration 1192, loss = 0.11672779\n",
      "Iteration 1193, loss = 0.10193103\n",
      "Iteration 1194, loss = 0.11110987\n",
      "Iteration 1195, loss = 0.11229093\n",
      "Iteration 1196, loss = 0.11662419\n",
      "Iteration 1197, loss = 0.11615228\n",
      "Iteration 1198, loss = 0.10818414\n",
      "Iteration 1199, loss = 0.10751216\n",
      "Iteration 1200, loss = 0.11196680\n",
      "Iteration 1201, loss = 0.10400110\n",
      "Iteration 1202, loss = 0.12363689\n",
      "Iteration 1203, loss = 0.11563574\n",
      "Iteration 1204, loss = 0.12289930\n",
      "Iteration 1205, loss = 0.12541333\n",
      "Iteration 1206, loss = 0.13335236\n",
      "Iteration 1207, loss = 0.14325107\n",
      "Iteration 1208, loss = 0.19506246\n",
      "Iteration 1209, loss = 0.17252797\n",
      "Iteration 1210, loss = 0.17978095\n",
      "Iteration 1211, loss = 0.14553528\n",
      "Iteration 1212, loss = 0.10628685\n",
      "Iteration 1213, loss = 0.12540457\n",
      "Iteration 1214, loss = 0.10977025\n",
      "Iteration 1215, loss = 0.10951831\n",
      "Iteration 1216, loss = 0.10977073\n",
      "Iteration 1217, loss = 0.11167426\n",
      "Iteration 1218, loss = 0.11159775\n",
      "Iteration 1219, loss = 0.10586765\n",
      "Iteration 1220, loss = 0.10546308\n",
      "Iteration 1221, loss = 0.10616185\n",
      "Iteration 1222, loss = 0.11102448\n",
      "Iteration 1223, loss = 0.10702411\n",
      "Iteration 1224, loss = 0.11075971\n",
      "Iteration 1225, loss = 0.10927508\n",
      "Iteration 1226, loss = 0.10767060\n",
      "Iteration 1227, loss = 0.10646133\n",
      "Iteration 1228, loss = 0.10547931\n",
      "Iteration 1229, loss = 0.10710379\n",
      "Iteration 1230, loss = 0.11164487\n",
      "Iteration 1231, loss = 0.11080803\n",
      "Iteration 1232, loss = 0.10933814\n",
      "Iteration 1233, loss = 0.11439461\n",
      "Iteration 1234, loss = 0.11096100\n",
      "Iteration 1235, loss = 0.11108242\n",
      "Iteration 1236, loss = 0.11410420\n",
      "Iteration 1237, loss = 0.11268988\n",
      "Iteration 1238, loss = 0.10980860\n",
      "Iteration 1239, loss = 0.11368074\n",
      "Iteration 1240, loss = 0.11085638\n",
      "Iteration 1241, loss = 0.11187463\n",
      "Iteration 1242, loss = 0.11734353\n",
      "Iteration 1243, loss = 0.11759479\n",
      "Iteration 1244, loss = 0.10891141\n",
      "Iteration 1245, loss = 0.10825284\n",
      "Iteration 1246, loss = 0.11300253\n",
      "Iteration 1247, loss = 0.10296961\n",
      "Iteration 1248, loss = 0.11446550\n",
      "Iteration 1249, loss = 0.11359434\n",
      "Iteration 1250, loss = 0.10596869\n",
      "Iteration 1251, loss = 0.13730091\n",
      "Iteration 1252, loss = 0.11187552\n",
      "Iteration 1253, loss = 0.10773983\n",
      "Iteration 1254, loss = 0.11501088\n",
      "Iteration 1255, loss = 0.11561616\n",
      "Iteration 1256, loss = 0.11318684\n",
      "Iteration 1257, loss = 0.11067894\n",
      "Iteration 1258, loss = 0.12296553\n",
      "Iteration 1259, loss = 0.11305985\n",
      "Iteration 1260, loss = 0.11481613\n",
      "Iteration 1261, loss = 0.11497386\n",
      "Iteration 1262, loss = 0.10783215\n",
      "Iteration 1263, loss = 0.11004841\n",
      "Iteration 1264, loss = 0.10670228\n",
      "Iteration 1265, loss = 0.10655531\n",
      "Iteration 1266, loss = 0.10710727\n",
      "Iteration 1267, loss = 0.10720330\n",
      "Iteration 1268, loss = 0.10965127\n",
      "Iteration 1269, loss = 0.10271453\n",
      "Iteration 1270, loss = 0.11045327\n",
      "Iteration 1271, loss = 0.11137315\n",
      "Iteration 1272, loss = 0.10774617\n",
      "Iteration 1273, loss = 0.10528797\n",
      "Iteration 1274, loss = 0.10778143\n",
      "Iteration 1275, loss = 0.10509679\n",
      "Iteration 1276, loss = 0.11970433\n",
      "Iteration 1277, loss = 0.13252412\n",
      "Iteration 1278, loss = 0.10626849\n",
      "Iteration 1279, loss = 0.10841044\n",
      "Iteration 1280, loss = 0.11500396\n",
      "Iteration 1281, loss = 0.10505414\n",
      "Iteration 1282, loss = 0.11294692\n",
      "Iteration 1283, loss = 0.12491521\n",
      "Iteration 1284, loss = 0.14027688\n",
      "Iteration 1285, loss = 0.12544793\n",
      "Iteration 1286, loss = 0.11580236\n",
      "Iteration 1287, loss = 0.12491386\n",
      "Iteration 1288, loss = 0.10651765\n",
      "Iteration 1289, loss = 0.13635771\n",
      "Iteration 1290, loss = 0.14434045\n",
      "Iteration 1291, loss = 0.13918759\n",
      "Iteration 1292, loss = 0.11426904\n",
      "Iteration 1293, loss = 0.10506282\n",
      "Iteration 1294, loss = 0.10815490\n",
      "Iteration 1295, loss = 0.11633805\n",
      "Iteration 1296, loss = 0.11684609\n",
      "Iteration 1297, loss = 0.10998481\n",
      "Iteration 1298, loss = 0.11652917\n",
      "Iteration 1299, loss = 0.11092524\n",
      "Iteration 1300, loss = 0.10938965\n",
      "Iteration 1301, loss = 0.11494375\n",
      "Iteration 1302, loss = 0.10882848\n",
      "Iteration 1303, loss = 0.10850242\n",
      "Iteration 1304, loss = 0.11497894\n",
      "Iteration 1305, loss = 0.11582988\n",
      "Iteration 1306, loss = 0.10535243\n",
      "Iteration 1307, loss = 0.12001071\n",
      "Iteration 1308, loss = 0.11805135\n",
      "Iteration 1309, loss = 0.11183588\n",
      "Iteration 1310, loss = 0.11046123\n",
      "Iteration 1311, loss = 0.12141517\n",
      "Iteration 1312, loss = 0.11656431\n",
      "Iteration 1313, loss = 0.11726685\n",
      "Iteration 1314, loss = 0.14842064\n",
      "Iteration 1315, loss = 0.10827763\n",
      "Iteration 1316, loss = 0.10320027\n",
      "Iteration 1317, loss = 0.11309933\n",
      "Iteration 1318, loss = 0.11439723\n",
      "Iteration 1319, loss = 0.11519344\n",
      "Iteration 1320, loss = 0.11428575\n",
      "Iteration 1321, loss = 0.11173018\n",
      "Iteration 1322, loss = 0.11089965\n",
      "Iteration 1323, loss = 0.10773309\n",
      "Iteration 1324, loss = 0.11771751\n",
      "Iteration 1325, loss = 0.11380181\n",
      "Iteration 1326, loss = 0.14275723\n",
      "Iteration 1327, loss = 0.11916926\n",
      "Iteration 1328, loss = 0.10890790\n",
      "Iteration 1329, loss = 0.10437449\n",
      "Iteration 1330, loss = 0.10701105\n",
      "Iteration 1331, loss = 0.10850337\n",
      "Iteration 1332, loss = 0.11367266\n",
      "Iteration 1333, loss = 0.11486470\n",
      "Iteration 1334, loss = 0.12956119\n",
      "Iteration 1335, loss = 0.15091533\n",
      "Iteration 1336, loss = 0.14006744\n",
      "Iteration 1337, loss = 0.17414542\n",
      "Iteration 1338, loss = 0.16325457\n",
      "Iteration 1339, loss = 0.12318991\n",
      "Iteration 1340, loss = 0.11694114\n",
      "Iteration 1341, loss = 0.10413715\n",
      "Iteration 1342, loss = 0.10578177\n",
      "Iteration 1343, loss = 0.10769644\n",
      "Iteration 1344, loss = 0.10995762\n",
      "Iteration 1345, loss = 0.11252524\n",
      "Iteration 1346, loss = 0.11632590\n",
      "Iteration 1347, loss = 0.12274535\n",
      "Iteration 1348, loss = 0.10558584\n",
      "Iteration 1349, loss = 0.10906750\n",
      "Iteration 1350, loss = 0.11384970\n",
      "Iteration 1351, loss = 0.10986385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1352, loss = 0.11105612\n",
      "Iteration 1353, loss = 0.10514177\n",
      "Iteration 1354, loss = 0.10894765\n",
      "Iteration 1355, loss = 0.10477235\n",
      "Iteration 1356, loss = 0.10260393\n",
      "Iteration 1357, loss = 0.11388433\n",
      "Iteration 1358, loss = 0.12848229\n",
      "Iteration 1359, loss = 0.10503517\n",
      "Iteration 1360, loss = 0.10677418\n",
      "Iteration 1361, loss = 0.11699731\n",
      "Iteration 1362, loss = 0.11015294\n",
      "Iteration 1363, loss = 0.11548736\n",
      "Iteration 1364, loss = 0.12191878\n",
      "Iteration 1365, loss = 0.12626231\n",
      "Iteration 1366, loss = 0.10933367\n",
      "Iteration 1367, loss = 0.10805739\n",
      "Iteration 1368, loss = 0.11531335\n",
      "Iteration 1369, loss = 0.11555492\n",
      "Iteration 1370, loss = 0.11011015\n",
      "Iteration 1371, loss = 0.10384930\n",
      "Iteration 1372, loss = 0.19469500\n",
      "Iteration 1373, loss = 0.17626672\n",
      "Iteration 1374, loss = 0.32279563\n",
      "Iteration 1375, loss = 0.25272003\n",
      "Iteration 1376, loss = 0.12704433\n",
      "Iteration 1377, loss = 0.16822470\n",
      "Iteration 1378, loss = 0.25268481\n",
      "Iteration 1379, loss = 0.20275600\n",
      "Iteration 1380, loss = 0.16385547\n",
      "Iteration 1381, loss = 0.16137818\n",
      "Iteration 1382, loss = 0.22483877\n",
      "Iteration 1383, loss = 0.15448221\n",
      "Iteration 1384, loss = 0.35788329\n",
      "Iteration 1385, loss = 0.24859182\n",
      "Iteration 1386, loss = 1.93659587\n",
      "Iteration 1387, loss = 5.18530160\n",
      "Iteration 1388, loss = 1.60471811\n",
      "Iteration 1389, loss = 1.24126769\n",
      "Iteration 1390, loss = 1.08668376\n",
      "Iteration 1391, loss = 1.01962907\n",
      "Iteration 1392, loss = 1.10691088\n",
      "Iteration 1393, loss = 0.91283527\n",
      "Iteration 1394, loss = 0.80253943\n",
      "Iteration 1395, loss = 0.78929874\n",
      "Iteration 1396, loss = 0.76749102\n",
      "Iteration 1397, loss = 0.73487933\n",
      "Iteration 1398, loss = 0.96483551\n",
      "Iteration 1399, loss = 0.64276629\n",
      "Iteration 1400, loss = 0.58775513\n",
      "Iteration 1401, loss = 0.66605970\n",
      "Iteration 1402, loss = 0.49772173\n",
      "Iteration 1403, loss = 0.65068854\n",
      "Iteration 1404, loss = 0.78482410\n",
      "Iteration 1405, loss = 0.58342107\n",
      "Iteration 1406, loss = 0.37894039\n",
      "Iteration 1407, loss = 0.32452605\n",
      "Iteration 1408, loss = 0.36790138\n",
      "Iteration 1409, loss = 0.23434327\n",
      "Iteration 1410, loss = 0.29153756\n",
      "Iteration 1411, loss = 0.34328100\n",
      "Iteration 1412, loss = 0.24018401\n",
      "Iteration 1413, loss = 0.19085955\n",
      "Iteration 1414, loss = 0.15014530\n",
      "Iteration 1415, loss = 0.15425292\n",
      "Iteration 1416, loss = 0.14555815\n",
      "Iteration 1417, loss = 0.18515682\n",
      "Iteration 1418, loss = 0.13712168\n",
      "Iteration 1419, loss = 0.12552317\n",
      "Iteration 1420, loss = 0.14787521\n",
      "Iteration 1421, loss = 0.12472755\n",
      "Iteration 1422, loss = 0.14184827\n",
      "Iteration 1423, loss = 0.16914845\n",
      "Iteration 1424, loss = 0.12898060\n",
      "Iteration 1425, loss = 0.19513022\n",
      "Iteration 1426, loss = 0.21972246\n",
      "Iteration 1427, loss = 0.25297917\n",
      "Iteration 1428, loss = 0.31155748\n",
      "Iteration 1429, loss = 0.28515722\n",
      "Iteration 1430, loss = 0.30496747\n",
      "Iteration 1431, loss = 0.31980643\n",
      "Iteration 1432, loss = 0.18872418\n",
      "Iteration 1433, loss = 0.17885413\n",
      "Iteration 1434, loss = 0.15891747\n",
      "Iteration 1435, loss = 0.36418303\n",
      "Iteration 1436, loss = 0.27399497\n",
      "Iteration 1437, loss = 0.21216639\n",
      "Iteration 1438, loss = 0.14609117\n",
      "Iteration 1439, loss = 0.18503620\n",
      "Iteration 1440, loss = 0.20086872\n",
      "Iteration 1441, loss = 0.34619496\n",
      "Iteration 1442, loss = 0.26305301\n",
      "Iteration 1443, loss = 0.43848657\n",
      "Iteration 1444, loss = 0.31426930\n",
      "Iteration 1445, loss = 0.48703328\n",
      "Iteration 1446, loss = 0.30414017\n",
      "Iteration 1447, loss = 0.34017289\n",
      "Iteration 1448, loss = 0.24100606\n",
      "Iteration 1449, loss = 0.23152555\n",
      "Iteration 1450, loss = 0.16476559\n",
      "Iteration 1451, loss = 0.22978429\n",
      "Iteration 1452, loss = 0.18397516\n",
      "Iteration 1453, loss = 0.12741478\n",
      "Iteration 1454, loss = 0.11308662\n",
      "Iteration 1455, loss = 0.12802884\n",
      "Iteration 1456, loss = 0.12942079\n",
      "Iteration 1457, loss = 0.13752056\n",
      "Iteration 1458, loss = 0.12041990\n",
      "Iteration 1459, loss = 0.10462579\n",
      "Iteration 1460, loss = 0.11563799\n",
      "Iteration 1461, loss = 0.11925337\n",
      "Iteration 1462, loss = 0.11066503\n",
      "Iteration 1463, loss = 0.12077930\n",
      "Iteration 1464, loss = 0.11736427\n",
      "Iteration 1465, loss = 0.15093218\n",
      "Iteration 1466, loss = 0.46040223\n",
      "Iteration 1467, loss = 0.31467676\n",
      "Iteration 1468, loss = 0.23320514\n",
      "Iteration 1469, loss = 0.15395122\n",
      "Iteration 1470, loss = 0.16320531\n",
      "Iteration 1471, loss = 0.14417395\n",
      "Iteration 1472, loss = 0.12143435\n",
      "Iteration 1473, loss = 0.12229152\n",
      "Iteration 1474, loss = 0.10642514\n",
      "Iteration 1475, loss = 0.12136136\n",
      "Iteration 1476, loss = 0.11131537\n",
      "Iteration 1477, loss = 0.10856816\n",
      "Iteration 1478, loss = 0.10544442\n",
      "Iteration 1479, loss = 0.10447400\n",
      "Iteration 1480, loss = 0.12304085\n",
      "Iteration 1481, loss = 0.13988496\n",
      "Iteration 1482, loss = 0.20262884\n",
      "Iteration 1483, loss = 0.17307340\n",
      "Iteration 1484, loss = 0.16174273\n",
      "Iteration 1485, loss = 0.12684962\n",
      "Iteration 1486, loss = 0.10932332\n",
      "Iteration 1487, loss = 0.13058926\n",
      "Iteration 1488, loss = 0.13894428\n",
      "Iteration 1489, loss = 0.12092220\n",
      "Iteration 1490, loss = 0.11238661\n",
      "Iteration 1491, loss = 0.11336244\n",
      "Iteration 1492, loss = 0.11340345\n",
      "Iteration 1493, loss = 0.10955037\n",
      "Iteration 1494, loss = 0.11458690\n",
      "Iteration 1495, loss = 0.13555397\n",
      "Iteration 1496, loss = 0.13760263\n",
      "Iteration 1497, loss = 0.12029025\n",
      "Iteration 1498, loss = 0.11556441\n",
      "Iteration 1499, loss = 0.11631329\n",
      "Iteration 1500, loss = 0.11665326\n",
      "Iteration 1501, loss = 0.11470840\n",
      "Iteration 1502, loss = 0.11033299\n",
      "Iteration 1503, loss = 0.10656839\n",
      "Iteration 1504, loss = 0.10927392\n",
      "Iteration 1505, loss = 0.10946835\n",
      "Iteration 1506, loss = 0.13795349\n",
      "Iteration 1507, loss = 0.11208460\n",
      "Iteration 1508, loss = 0.12339907\n",
      "Iteration 1509, loss = 0.17644940\n",
      "Iteration 1510, loss = 0.29089838\n",
      "Iteration 1511, loss = 0.25720011\n",
      "Iteration 1512, loss = 0.23221623\n",
      "Iteration 1513, loss = 0.36928963\n",
      "Iteration 1514, loss = 0.18316769\n",
      "Iteration 1515, loss = 0.18238701\n",
      "Iteration 1516, loss = 0.17173367\n",
      "Iteration 1517, loss = 0.16241581\n",
      "Iteration 1518, loss = 0.13864862\n",
      "Iteration 1519, loss = 0.14856867\n",
      "Iteration 1520, loss = 0.11951339\n",
      "Iteration 1521, loss = 0.12442635\n",
      "Iteration 1522, loss = 0.11594451\n",
      "Iteration 1523, loss = 0.10591149\n",
      "Iteration 1524, loss = 0.10664369\n",
      "Iteration 1525, loss = 0.10821029\n",
      "Iteration 1526, loss = 0.10368979\n",
      "Iteration 1527, loss = 0.10690701\n",
      "Iteration 1528, loss = 0.10598944\n",
      "Iteration 1529, loss = 0.10498231\n",
      "Iteration 1530, loss = 0.11116848\n",
      "Iteration 1531, loss = 0.10449056\n",
      "Iteration 1532, loss = 0.10645179\n",
      "Iteration 1533, loss = 0.11137379\n",
      "Iteration 1534, loss = 0.10725182\n",
      "Iteration 1535, loss = 0.12084285\n",
      "Iteration 1536, loss = 0.15185112\n",
      "Iteration 1537, loss = 0.12935993\n",
      "Iteration 1538, loss = 0.22526565\n",
      "Iteration 1539, loss = 0.12438880\n",
      "Iteration 1540, loss = 0.13119196\n",
      "Iteration 1541, loss = 0.12845346\n",
      "Iteration 1542, loss = 0.12818341\n",
      "Iteration 1543, loss = 0.12097193\n",
      "Iteration 1544, loss = 0.11245838\n",
      "Iteration 1545, loss = 0.12577074\n",
      "Iteration 1546, loss = 0.11827921\n",
      "Iteration 1547, loss = 0.11194418\n",
      "Iteration 1548, loss = 0.11938392\n",
      "Iteration 1549, loss = 0.10030091\n",
      "Iteration 1550, loss = 0.10736650\n",
      "Iteration 1551, loss = 0.10274244\n",
      "Iteration 1552, loss = 0.11209141\n",
      "Iteration 1553, loss = 0.10595514\n",
      "Iteration 1554, loss = 0.10348273\n",
      "Iteration 1555, loss = 0.10296507\n",
      "Iteration 1556, loss = 0.10657782\n",
      "Iteration 1557, loss = 0.10588274\n",
      "Iteration 1558, loss = 0.10880070\n",
      "Iteration 1559, loss = 0.10813043\n",
      "Iteration 1560, loss = 0.10604016\n",
      "Iteration 1561, loss = 0.11136644\n",
      "Iteration 1562, loss = 0.10201542\n",
      "Iteration 1563, loss = 0.10766859\n",
      "Iteration 1564, loss = 0.10355702\n",
      "Iteration 1565, loss = 0.11221520\n",
      "Iteration 1566, loss = 0.20100028\n",
      "Iteration 1567, loss = 0.14211208\n",
      "Iteration 1568, loss = 0.13987165\n",
      "Iteration 1569, loss = 0.13156206\n",
      "Iteration 1570, loss = 0.10845760\n",
      "Iteration 1571, loss = 0.10866991\n",
      "Iteration 1572, loss = 0.10622440\n",
      "Iteration 1573, loss = 0.11573573\n",
      "Iteration 1574, loss = 0.10666503\n",
      "Iteration 1575, loss = 0.10500741\n",
      "Iteration 1576, loss = 0.11721832\n",
      "Iteration 1577, loss = 0.10624320\n",
      "Iteration 1578, loss = 0.12655500\n",
      "Iteration 1579, loss = 0.10854408\n",
      "Iteration 1580, loss = 0.11979536\n",
      "Iteration 1581, loss = 0.12651055\n",
      "Iteration 1582, loss = 0.10947550\n",
      "Iteration 1583, loss = 0.10813728\n",
      "Iteration 1584, loss = 0.11302147\n",
      "Iteration 1585, loss = 0.10296788\n",
      "Iteration 1586, loss = 0.10520569\n",
      "Iteration 1587, loss = 0.10511822\n",
      "Iteration 1588, loss = 0.12028382\n",
      "Iteration 1589, loss = 0.10721322\n",
      "Iteration 1590, loss = 0.10877762\n",
      "Iteration 1591, loss = 0.14965749\n",
      "Iteration 1592, loss = 0.11194440\n",
      "Iteration 1593, loss = 0.11194764\n",
      "Iteration 1594, loss = 0.12243952\n",
      "Iteration 1595, loss = 0.13959613\n",
      "Iteration 1596, loss = 0.10902110\n",
      "Iteration 1597, loss = 0.10967555\n",
      "Iteration 1598, loss = 0.12255860\n",
      "Iteration 1599, loss = 0.11251648\n",
      "Iteration 1600, loss = 0.10476154\n",
      "Iteration 1601, loss = 0.10812900\n",
      "Iteration 1602, loss = 0.11184768\n",
      "Iteration 1603, loss = 0.11736754\n",
      "Iteration 1604, loss = 0.11186757\n",
      "Iteration 1605, loss = 0.12155632\n",
      "Iteration 1606, loss = 0.11679705\n",
      "Iteration 1607, loss = 0.13358317\n",
      "Iteration 1608, loss = 0.13857299\n",
      "Iteration 1609, loss = 0.23497237\n",
      "Iteration 1610, loss = 0.15004296\n",
      "Iteration 1611, loss = 0.12022803\n",
      "Iteration 1612, loss = 0.11045757\n",
      "Iteration 1613, loss = 0.10835653\n",
      "Iteration 1614, loss = 0.11256454\n",
      "Iteration 1615, loss = 0.12030214\n",
      "Iteration 1616, loss = 0.11146671\n",
      "Iteration 1617, loss = 0.11832014\n",
      "Iteration 1618, loss = 0.10206075\n",
      "Iteration 1619, loss = 0.11271727\n",
      "Iteration 1620, loss = 0.12381628\n",
      "Iteration 1621, loss = 0.12241739\n",
      "Iteration 1622, loss = 0.12451169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1623, loss = 0.12169186\n",
      "Iteration 1624, loss = 0.12074579\n",
      "Iteration 1625, loss = 0.11641639\n",
      "Iteration 1626, loss = 0.14577315\n",
      "Iteration 1627, loss = 0.12127961\n",
      "Iteration 1628, loss = 0.11327182\n",
      "Iteration 1629, loss = 0.11247788\n",
      "Iteration 1630, loss = 0.11585506\n",
      "Iteration 1631, loss = 0.10379704\n",
      "Iteration 1632, loss = 0.11748707\n",
      "Iteration 1633, loss = 0.12235756\n",
      "Iteration 1634, loss = 0.20604824\n",
      "Iteration 1635, loss = 0.15458382\n",
      "Iteration 1636, loss = 0.20489685\n",
      "Iteration 1637, loss = 0.12437682\n",
      "Iteration 1638, loss = 0.11707019\n",
      "Iteration 1639, loss = 0.10491063\n",
      "Iteration 1640, loss = 0.11871164\n",
      "Iteration 1641, loss = 0.14336807\n",
      "Iteration 1642, loss = 0.18166411\n",
      "Iteration 1643, loss = 0.11209017\n",
      "Iteration 1644, loss = 0.13829279\n",
      "Iteration 1645, loss = 0.10206589\n",
      "Iteration 1646, loss = 0.11375203\n",
      "Iteration 1647, loss = 0.11815173\n",
      "Iteration 1648, loss = 0.11091668\n",
      "Iteration 1649, loss = 0.10870427\n",
      "Iteration 1650, loss = 0.10877365\n",
      "Iteration 1651, loss = 0.10509019\n",
      "Iteration 1652, loss = 0.10524295\n",
      "Iteration 1653, loss = 0.10396461\n",
      "Iteration 1654, loss = 0.13115405\n",
      "Iteration 1655, loss = 0.12912701\n",
      "Iteration 1656, loss = 0.11784633\n",
      "Iteration 1657, loss = 0.11501528\n",
      "Iteration 1658, loss = 0.10916955\n",
      "Iteration 1659, loss = 0.10933228\n",
      "Iteration 1660, loss = 0.10585644\n",
      "Iteration 1661, loss = 0.10379613\n",
      "Iteration 1662, loss = 0.11821436\n",
      "Iteration 1663, loss = 0.13112299\n",
      "Iteration 1664, loss = 0.10827186\n",
      "Iteration 1665, loss = 0.10518084\n",
      "Iteration 1666, loss = 0.12014824\n",
      "Iteration 1667, loss = 0.10541084\n",
      "Iteration 1668, loss = 0.11115646\n",
      "Iteration 1669, loss = 0.11154285\n",
      "Iteration 1670, loss = 0.11233440\n",
      "Iteration 1671, loss = 0.11196935\n",
      "Iteration 1672, loss = 0.10667383\n",
      "Iteration 1673, loss = 0.10825056\n",
      "Iteration 1674, loss = 0.11081491\n",
      "Iteration 1675, loss = 0.10943184\n",
      "Iteration 1676, loss = 0.11647912\n",
      "Iteration 1677, loss = 0.12664679\n",
      "Iteration 1678, loss = 0.12793068\n",
      "Iteration 1679, loss = 0.13899346\n",
      "Iteration 1680, loss = 0.11547873\n",
      "Iteration 1681, loss = 0.10103593\n",
      "Iteration 1682, loss = 0.12145878\n",
      "Iteration 1683, loss = 0.11646284\n",
      "Iteration 1684, loss = 0.10497251\n",
      "Iteration 1685, loss = 0.10585923\n",
      "Iteration 1686, loss = 0.10694807\n",
      "Iteration 1687, loss = 0.12210350\n",
      "Iteration 1688, loss = 0.14051699\n",
      "Iteration 1689, loss = 0.18206900\n",
      "Iteration 1690, loss = 0.15582472\n",
      "Iteration 1691, loss = 0.12013245\n",
      "Iteration 1692, loss = 0.11062291\n",
      "Iteration 1693, loss = 0.10684501\n",
      "Iteration 1694, loss = 0.11334839\n",
      "Iteration 1695, loss = 0.11067459\n",
      "Iteration 1696, loss = 0.11130712\n",
      "Iteration 1697, loss = 0.11657235\n",
      "Iteration 1698, loss = 0.11076725\n",
      "Iteration 1699, loss = 0.11087291\n",
      "Iteration 1700, loss = 0.10224526\n",
      "Iteration 1701, loss = 0.10750673\n",
      "Iteration 1702, loss = 0.11004613\n",
      "Iteration 1703, loss = 0.10932954\n",
      "Iteration 1704, loss = 0.10839129\n",
      "Iteration 1705, loss = 0.10527677\n",
      "Iteration 1706, loss = 0.10543056\n",
      "Iteration 1707, loss = 0.10718744\n",
      "Iteration 1708, loss = 0.10814078\n",
      "Iteration 1709, loss = 0.11069847\n",
      "Iteration 1710, loss = 0.12583615\n",
      "Iteration 1711, loss = 0.10706704\n",
      "Iteration 1712, loss = 0.11255096\n",
      "Iteration 1713, loss = 0.13871453\n",
      "Iteration 1714, loss = 0.12109286\n",
      "Iteration 1715, loss = 0.13591598\n",
      "Iteration 1716, loss = 0.18204326\n",
      "Iteration 1717, loss = 0.17508349\n",
      "Iteration 1718, loss = 0.14703525\n",
      "Iteration 1719, loss = 0.10649808\n",
      "Iteration 1720, loss = 0.11897315\n",
      "Iteration 1721, loss = 0.11105666\n",
      "Iteration 1722, loss = 0.10622389\n",
      "Iteration 1723, loss = 0.10572869\n",
      "Iteration 1724, loss = 0.10471716\n",
      "Iteration 1725, loss = 0.10466203\n",
      "Iteration 1726, loss = 0.10597875\n",
      "Iteration 1727, loss = 0.10672678\n",
      "Iteration 1728, loss = 0.11108765\n",
      "Iteration 1729, loss = 0.11539150\n",
      "Iteration 1730, loss = 0.10804094\n",
      "Iteration 1731, loss = 0.11364803\n",
      "Iteration 1732, loss = 0.12112342\n",
      "Iteration 1733, loss = 0.13903613\n",
      "Iteration 1734, loss = 0.14345483\n",
      "Iteration 1735, loss = 0.11974608\n",
      "Iteration 1736, loss = 0.11083883\n",
      "Iteration 1737, loss = 0.11173739\n",
      "Iteration 1738, loss = 0.12719915\n",
      "Iteration 1739, loss = 0.11439894\n",
      "Iteration 1740, loss = 0.10663897\n",
      "Iteration 1741, loss = 0.12070416\n",
      "Iteration 1742, loss = 0.12027493\n",
      "Iteration 1743, loss = 0.10973376\n",
      "Iteration 1744, loss = 0.10831036\n",
      "Iteration 1745, loss = 0.11169854\n",
      "Iteration 1746, loss = 0.10484200\n",
      "Iteration 1747, loss = 0.10901724\n",
      "Iteration 1748, loss = 0.11268474\n",
      "Iteration 1749, loss = 0.10572849\n",
      "Iteration 1750, loss = 0.10574400\n",
      "Iteration 1751, loss = 0.10369299\n",
      "Iteration 1752, loss = 0.10561654\n",
      "Iteration 1753, loss = 0.10632581\n",
      "Iteration 1754, loss = 0.10417611\n",
      "Iteration 1755, loss = 0.11563241\n",
      "Iteration 1756, loss = 0.11359055\n",
      "Iteration 1757, loss = 0.11162737\n",
      "Iteration 1758, loss = 0.11135525\n",
      "Iteration 1759, loss = 0.12598534\n",
      "Iteration 1760, loss = 0.10497384\n",
      "Iteration 1761, loss = 0.11019174\n",
      "Iteration 1762, loss = 0.10795553\n",
      "Iteration 1763, loss = 0.10599195\n",
      "Iteration 1764, loss = 0.10694564\n",
      "Iteration 1765, loss = 0.11164741\n",
      "Iteration 1766, loss = 0.11175681\n",
      "Iteration 1767, loss = 0.12196705\n",
      "Iteration 1768, loss = 0.12416636\n",
      "Iteration 1769, loss = 0.12107109\n",
      "Iteration 1770, loss = 0.12478874\n",
      "Iteration 1771, loss = 0.11530741\n",
      "Iteration 1772, loss = 0.12658293\n",
      "Iteration 1773, loss = 0.11948676\n",
      "Iteration 1774, loss = 0.13753871\n",
      "Iteration 1775, loss = 0.10716304\n",
      "Iteration 1776, loss = 0.11139922\n",
      "Iteration 1777, loss = 0.11323918\n",
      "Iteration 1778, loss = 0.10573928\n",
      "Iteration 1779, loss = 0.11806228\n",
      "Iteration 1780, loss = 0.10756165\n",
      "Iteration 1781, loss = 0.11388377\n",
      "Iteration 1782, loss = 0.10345666\n",
      "Iteration 1783, loss = 0.11026321\n",
      "Iteration 1784, loss = 0.10836655\n",
      "Iteration 1785, loss = 0.12139053\n",
      "Iteration 1786, loss = 0.13011808\n",
      "Iteration 1787, loss = 0.21953039\n",
      "Iteration 1788, loss = 0.29224524\n",
      "Iteration 1789, loss = 0.28969577\n",
      "Iteration 1790, loss = 0.54035039\n",
      "Iteration 1791, loss = 0.82270507\n",
      "Iteration 1792, loss = 0.74130325\n",
      "Iteration 1793, loss = 1.64903227\n",
      "Iteration 1794, loss = 3.49343876\n",
      "Iteration 1795, loss = 6.49227394\n",
      "Iteration 1796, loss = 2.38428100\n",
      "Iteration 1797, loss = 1.54207796\n",
      "Iteration 1798, loss = 0.96984131\n",
      "Iteration 1799, loss = 1.21861557\n",
      "Iteration 1800, loss = 1.43694360\n",
      "Iteration 1801, loss = 1.09056555\n",
      "Iteration 1802, loss = 1.19151916\n",
      "Iteration 1803, loss = 0.75030209\n",
      "Iteration 1804, loss = 1.01693326\n",
      "Iteration 1805, loss = 0.64640503\n",
      "Iteration 1806, loss = 1.02626988\n",
      "Iteration 1807, loss = 0.76356652\n",
      "Iteration 1808, loss = 0.72563761\n",
      "Iteration 1809, loss = 0.81325119\n",
      "Iteration 1810, loss = 0.76396102\n",
      "Iteration 1811, loss = 0.81765858\n",
      "Iteration 1812, loss = 0.58722295\n",
      "Iteration 1813, loss = 1.01517354\n",
      "Iteration 1814, loss = 0.67781060\n",
      "Iteration 1815, loss = 0.93195997\n",
      "Iteration 1816, loss = 0.76795087\n",
      "Iteration 1817, loss = 0.57054828\n",
      "Iteration 1818, loss = 0.49127559\n",
      "Iteration 1819, loss = 0.70067818\n",
      "Iteration 1820, loss = 0.59592695\n",
      "Iteration 1821, loss = 0.43296073\n",
      "Iteration 1822, loss = 0.56635366\n",
      "Iteration 1823, loss = 0.57957167\n",
      "Iteration 1824, loss = 0.58098709\n",
      "Iteration 1825, loss = 0.72870418\n",
      "Iteration 1826, loss = 0.52560146\n",
      "Iteration 1827, loss = 0.49649044\n",
      "Iteration 1828, loss = 0.41912813\n",
      "Iteration 1829, loss = 0.41948193\n",
      "Iteration 1830, loss = 0.30480126\n",
      "Iteration 1831, loss = 0.21191057\n",
      "Iteration 1832, loss = 0.70666898\n",
      "Iteration 1833, loss = 0.91022061\n",
      "Iteration 1834, loss = 0.78402361\n",
      "Iteration 1835, loss = 0.62936266\n",
      "Iteration 1836, loss = 0.88964690\n",
      "Iteration 1837, loss = 0.50207025\n",
      "Iteration 1838, loss = 0.53016667\n",
      "Iteration 1839, loss = 0.68009750\n",
      "Iteration 1840, loss = 0.58336184\n",
      "Iteration 1841, loss = 0.55802527\n",
      "Iteration 1842, loss = 0.32888343\n",
      "Iteration 1843, loss = 0.34553135\n",
      "Iteration 1844, loss = 0.24667849\n",
      "Iteration 1845, loss = 0.22727144\n",
      "Iteration 1846, loss = 0.59423954\n",
      "Iteration 1847, loss = 0.97866937\n",
      "Iteration 1848, loss = 0.46742138\n",
      "Iteration 1849, loss = 0.42980654\n",
      "Iteration 1850, loss = 0.41979931\n",
      "Iteration 1851, loss = 0.31265553\n",
      "Iteration 1852, loss = 0.27577412\n",
      "Iteration 1853, loss = 0.19768180\n",
      "Iteration 1854, loss = 0.21164289\n",
      "Iteration 1855, loss = 0.42464614\n",
      "Iteration 1856, loss = 0.67077416\n",
      "Iteration 1857, loss = 0.57367525\n",
      "Iteration 1858, loss = 0.37043656\n",
      "Iteration 1859, loss = 0.46303083\n",
      "Iteration 1860, loss = 0.44280777\n",
      "Iteration 1861, loss = 0.26829301\n",
      "Iteration 1862, loss = 0.16199194\n",
      "Iteration 1863, loss = 0.15291353\n",
      "Iteration 1864, loss = 0.13263043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1865, loss = 0.12270496\n",
      "Iteration 1866, loss = 0.12175646\n",
      "Iteration 1867, loss = 0.12656305\n",
      "Iteration 1868, loss = 0.12285071\n",
      "Iteration 1869, loss = 0.12342202\n",
      "Iteration 1870, loss = 0.12147648\n",
      "Iteration 1871, loss = 0.13684466\n",
      "Iteration 1872, loss = 0.13566247\n",
      "Iteration 1873, loss = 0.14839598\n",
      "Iteration 1874, loss = 0.16938322\n",
      "Iteration 1875, loss = 0.13425963\n",
      "Iteration 1876, loss = 0.13856922\n",
      "Iteration 1877, loss = 0.14769463\n",
      "Iteration 1878, loss = 0.15236236\n",
      "Iteration 1879, loss = 0.12507679\n",
      "Iteration 1880, loss = 0.15839904\n",
      "Iteration 1881, loss = 0.11982891\n",
      "Iteration 1882, loss = 0.12425502\n",
      "Iteration 1883, loss = 0.12516788\n",
      "Iteration 1884, loss = 0.12406158\n",
      "Iteration 1885, loss = 0.13207051\n",
      "Iteration 1886, loss = 0.38595879\n",
      "Iteration 1887, loss = 0.19466075\n",
      "Iteration 1888, loss = 0.18867485\n",
      "Iteration 1889, loss = 0.17556583\n",
      "Iteration 1890, loss = 0.20645434\n",
      "Iteration 1891, loss = 0.15034987\n",
      "Iteration 1892, loss = 0.12248444\n",
      "Iteration 1893, loss = 0.12221611\n",
      "Iteration 1894, loss = 0.12346916\n",
      "Iteration 1895, loss = 0.16708554\n",
      "Iteration 1896, loss = 0.15651112\n",
      "Iteration 1897, loss = 0.14067839\n",
      "Iteration 1898, loss = 0.11118014\n",
      "Iteration 1899, loss = 0.11286079\n",
      "Iteration 1900, loss = 0.11121526\n",
      "Iteration 1901, loss = 0.11827067\n",
      "Iteration 1902, loss = 0.12780402\n",
      "Iteration 1903, loss = 0.10409535\n",
      "Iteration 1904, loss = 0.11234934\n",
      "Iteration 1905, loss = 0.11576625\n",
      "Iteration 1906, loss = 0.16167058\n",
      "Iteration 1907, loss = 0.15779505\n",
      "Iteration 1908, loss = 0.18428770\n",
      "Iteration 1909, loss = 0.12906729\n",
      "Iteration 1910, loss = 0.12695411\n",
      "Iteration 1911, loss = 0.13683999\n",
      "Iteration 1912, loss = 0.21571391\n",
      "Iteration 1913, loss = 0.16788622\n",
      "Iteration 1914, loss = 0.14558642\n",
      "Iteration 1915, loss = 0.20398955\n",
      "Iteration 1916, loss = 0.64301352\n",
      "Iteration 1917, loss = 0.57909220\n",
      "Iteration 1918, loss = 0.38780921\n",
      "Iteration 1919, loss = 0.27505166\n",
      "Iteration 1920, loss = 0.23710545\n",
      "Iteration 1921, loss = 0.19807962\n",
      "Iteration 1922, loss = 0.18613453\n",
      "Iteration 1923, loss = 0.12437368\n",
      "Iteration 1924, loss = 0.13327130\n",
      "Iteration 1925, loss = 0.15160941\n",
      "Iteration 1926, loss = 0.13261227\n",
      "Iteration 1927, loss = 0.11044326\n",
      "Iteration 1928, loss = 0.13360075\n",
      "Iteration 1929, loss = 0.13039873\n",
      "Iteration 1930, loss = 0.12265109\n",
      "Iteration 1931, loss = 0.12601047\n",
      "Iteration 1932, loss = 0.13381498\n",
      "Iteration 1933, loss = 0.18106599\n",
      "Iteration 1934, loss = 0.24328741\n",
      "Iteration 1935, loss = 0.20713655\n",
      "Iteration 1936, loss = 0.21945251\n",
      "Iteration 1937, loss = 0.17397144\n",
      "Iteration 1938, loss = 0.14720482\n",
      "Iteration 1939, loss = 0.12697072\n",
      "Iteration 1940, loss = 0.12170326\n",
      "Iteration 1941, loss = 0.11978574\n",
      "Iteration 1942, loss = 0.10826475\n",
      "Iteration 1943, loss = 0.11638491\n",
      "Iteration 1944, loss = 0.14273820\n",
      "Iteration 1945, loss = 0.14425593\n",
      "Iteration 1946, loss = 0.14248774\n",
      "Iteration 1947, loss = 0.11872789\n",
      "Iteration 1948, loss = 0.11194821\n",
      "Iteration 1949, loss = 0.10766318\n",
      "Iteration 1950, loss = 0.13829596\n",
      "Iteration 1951, loss = 0.11140972\n",
      "Iteration 1952, loss = 0.10963841\n",
      "Iteration 1953, loss = 0.10893400\n",
      "Iteration 1954, loss = 0.10701876\n",
      "Iteration 1955, loss = 0.11235010\n",
      "Iteration 1956, loss = 0.12800703\n",
      "Iteration 1957, loss = 0.10836441\n",
      "Iteration 1958, loss = 0.12811748\n",
      "Iteration 1959, loss = 0.12603068\n",
      "Iteration 1960, loss = 0.13759568\n",
      "Iteration 1961, loss = 0.24325008\n",
      "Iteration 1962, loss = 0.32652375\n",
      "Iteration 1963, loss = 0.35057334\n",
      "Iteration 1964, loss = 0.59756868\n",
      "Iteration 1965, loss = 0.48718369\n",
      "Iteration 1966, loss = 0.40042349\n",
      "Iteration 1967, loss = 0.32135188\n",
      "Iteration 1968, loss = 0.32942559\n",
      "Iteration 1969, loss = 0.20929789\n",
      "Iteration 1970, loss = 0.12616212\n",
      "Iteration 1971, loss = 0.14363085\n",
      "Iteration 1972, loss = 0.12610747\n",
      "Iteration 1973, loss = 0.11527713\n",
      "Iteration 1974, loss = 0.14193946\n",
      "Iteration 1975, loss = 0.10997951\n",
      "Iteration 1976, loss = 0.11749097\n",
      "Iteration 1977, loss = 0.10982642\n",
      "Iteration 1978, loss = 0.10556039\n",
      "Iteration 1979, loss = 0.10776580\n",
      "Iteration 1980, loss = 0.11165635\n",
      "Iteration 1981, loss = 0.11870521\n",
      "Iteration 1982, loss = 0.12945728\n",
      "Iteration 1983, loss = 0.10352278\n",
      "Iteration 1984, loss = 0.10426222\n",
      "Iteration 1985, loss = 0.10692848\n",
      "Iteration 1986, loss = 0.10509955\n",
      "Iteration 1987, loss = 0.11216605\n",
      "Iteration 1988, loss = 0.11176200\n",
      "Iteration 1989, loss = 0.10877890\n",
      "Iteration 1990, loss = 0.10573646\n",
      "Iteration 1991, loss = 0.10428431\n",
      "Iteration 1992, loss = 0.10973902\n",
      "Iteration 1993, loss = 0.10832066\n",
      "Iteration 1994, loss = 0.11196763\n",
      "Iteration 1995, loss = 0.11245090\n",
      "Iteration 1996, loss = 0.11359542\n",
      "Iteration 1997, loss = 0.11319618\n",
      "Iteration 1998, loss = 0.11579262\n",
      "Iteration 1999, loss = 0.12140248\n",
      "Iteration 2000, loss = 0.10987574\n",
      "Iteration 2001, loss = 0.10570271\n",
      "Iteration 2002, loss = 0.10981645\n",
      "Iteration 2003, loss = 0.10675050\n",
      "Iteration 2004, loss = 0.15741119\n",
      "Iteration 2005, loss = 0.17408344\n",
      "Iteration 2006, loss = 0.14090129\n",
      "Iteration 2007, loss = 0.11122214\n",
      "Iteration 2008, loss = 0.10960867\n",
      "Iteration 2009, loss = 0.11205654\n",
      "Iteration 2010, loss = 0.10980143\n",
      "Iteration 2011, loss = 0.12481186\n",
      "Iteration 2012, loss = 0.10343092\n",
      "Iteration 2013, loss = 0.10772750\n",
      "Iteration 2014, loss = 0.10954592\n",
      "Iteration 2015, loss = 0.11171771\n",
      "Iteration 2016, loss = 0.10875607\n",
      "Iteration 2017, loss = 0.10441734\n",
      "Iteration 2018, loss = 0.11064402\n",
      "Iteration 2019, loss = 0.10502350\n",
      "Iteration 2020, loss = 0.11830477\n",
      "Iteration 2021, loss = 0.12433922\n",
      "Iteration 2022, loss = 0.12037055\n",
      "Iteration 2023, loss = 0.13561431\n",
      "Iteration 2024, loss = 0.10880152\n",
      "Iteration 2025, loss = 0.11813731\n",
      "Iteration 2026, loss = 0.11325209\n",
      "Iteration 2027, loss = 0.10946106\n",
      "Iteration 2028, loss = 0.10230648\n",
      "Iteration 2029, loss = 0.11790502\n",
      "Iteration 2030, loss = 0.11268489\n",
      "Iteration 2031, loss = 0.10825899\n",
      "Iteration 2032, loss = 0.10731039\n",
      "Iteration 2033, loss = 0.11248683\n",
      "Iteration 2034, loss = 0.10273447\n",
      "Iteration 2035, loss = 0.10819336\n",
      "Iteration 2036, loss = 0.11195274\n",
      "Iteration 2037, loss = 0.10937363\n",
      "Iteration 2038, loss = 0.10807165\n",
      "Iteration 2039, loss = 0.11718434\n",
      "Iteration 2040, loss = 0.10606366\n",
      "Iteration 2041, loss = 0.10873030\n",
      "Iteration 2042, loss = 0.11596242\n",
      "Iteration 2043, loss = 0.10939608\n",
      "Iteration 2044, loss = 0.11869829\n",
      "Iteration 2045, loss = 0.19907049\n",
      "Iteration 2046, loss = 0.13001424\n",
      "Iteration 2047, loss = 0.13640090\n",
      "Iteration 2048, loss = 0.11968045\n",
      "Iteration 2049, loss = 0.12445201\n",
      "Iteration 2050, loss = 0.11671236\n",
      "Iteration 2051, loss = 0.10899567\n",
      "Iteration 2052, loss = 0.10920526\n",
      "Iteration 2053, loss = 0.11035756\n",
      "Iteration 2054, loss = 0.10706248\n",
      "Iteration 2055, loss = 0.10383775\n",
      "Iteration 2056, loss = 0.10846829\n",
      "Iteration 2057, loss = 0.11688757\n",
      "Iteration 2058, loss = 0.13367911\n",
      "Iteration 2059, loss = 0.13088280\n",
      "Iteration 2060, loss = 0.17939777\n",
      "Iteration 2061, loss = 0.32609431\n",
      "Iteration 2062, loss = 0.21006044\n",
      "Iteration 2063, loss = 0.21117661\n",
      "Iteration 2064, loss = 0.15362861\n",
      "Iteration 2065, loss = 0.26458453\n",
      "Iteration 2066, loss = 0.12558507\n",
      "Iteration 2067, loss = 0.11723976\n",
      "Iteration 2068, loss = 0.12011524\n",
      "Iteration 2069, loss = 0.11169928\n",
      "Iteration 2070, loss = 0.11930062\n",
      "Iteration 2071, loss = 0.12381471\n",
      "Iteration 2072, loss = 0.10503455\n",
      "Iteration 2073, loss = 0.10770046\n",
      "Iteration 2074, loss = 0.11627289\n",
      "Iteration 2075, loss = 0.12371834\n",
      "Iteration 2076, loss = 0.12506404\n",
      "Iteration 2077, loss = 0.10660742\n",
      "Iteration 2078, loss = 0.11476379\n",
      "Iteration 2079, loss = 0.12722217\n",
      "Iteration 2080, loss = 0.11505877\n",
      "Iteration 2081, loss = 0.10820789\n",
      "Iteration 2082, loss = 0.10938594\n",
      "Iteration 2083, loss = 0.11330133\n",
      "Iteration 2084, loss = 0.10490313\n",
      "Iteration 2085, loss = 0.10785909\n",
      "Iteration 2086, loss = 0.10453718\n",
      "Iteration 2087, loss = 0.11049331\n",
      "Iteration 2088, loss = 0.11352420\n",
      "Iteration 2089, loss = 0.10459409\n",
      "Iteration 2090, loss = 0.12032042\n",
      "Iteration 2091, loss = 0.10723937\n",
      "Iteration 2092, loss = 0.11915575\n",
      "Iteration 2093, loss = 0.10928761\n",
      "Iteration 2094, loss = 0.11769929\n",
      "Iteration 2095, loss = 0.10340387\n",
      "Iteration 2096, loss = 0.10535795\n",
      "Iteration 2097, loss = 0.10720476\n",
      "Iteration 2098, loss = 0.10468783\n",
      "Iteration 2099, loss = 0.11134811\n",
      "Iteration 2100, loss = 0.14565582\n",
      "Iteration 2101, loss = 0.10214960\n",
      "Iteration 2102, loss = 0.12902189\n",
      "Iteration 2103, loss = 0.11773262\n",
      "Iteration 2104, loss = 0.12513803\n",
      "Iteration 2105, loss = 0.11954566\n",
      "Iteration 2106, loss = 0.13202389\n",
      "Iteration 2107, loss = 0.10691205\n",
      "Iteration 2108, loss = 0.10949976\n",
      "Iteration 2109, loss = 0.10833911\n",
      "Iteration 2110, loss = 0.11039253\n",
      "Iteration 2111, loss = 0.13896309\n",
      "Iteration 2112, loss = 0.12308936\n",
      "Iteration 2113, loss = 0.11511639\n",
      "Iteration 2114, loss = 0.11871211\n",
      "Iteration 2115, loss = 0.10912600\n",
      "Iteration 2116, loss = 0.10820426\n",
      "Iteration 2117, loss = 0.10603962\n",
      "Iteration 2118, loss = 0.10402811\n",
      "Iteration 2119, loss = 0.11086228\n",
      "Iteration 2120, loss = 0.13748833\n",
      "Iteration 2121, loss = 0.10788368\n",
      "Iteration 2122, loss = 0.12188304\n",
      "Iteration 2123, loss = 0.14508688\n",
      "Iteration 2124, loss = 0.12871008\n",
      "Iteration 2125, loss = 0.12442386\n",
      "Iteration 2126, loss = 0.16176738\n",
      "Iteration 2127, loss = 0.21340620\n",
      "Iteration 2128, loss = 0.33798502\n",
      "Iteration 2129, loss = 0.16422306\n",
      "Iteration 2130, loss = 0.21701469\n",
      "Iteration 2131, loss = 0.17711286\n",
      "Iteration 2132, loss = 0.15792243\n",
      "Iteration 2133, loss = 0.12486898\n",
      "Iteration 2134, loss = 0.11199234\n",
      "Iteration 2135, loss = 0.12568127\n",
      "Iteration 2136, loss = 0.11704956\n",
      "Iteration 2137, loss = 0.12339115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2138, loss = 0.11002505\n",
      "Iteration 2139, loss = 0.13716387\n",
      "Iteration 2140, loss = 0.11617439\n",
      "Iteration 2141, loss = 0.10640393\n",
      "Iteration 2142, loss = 0.10855176\n",
      "Iteration 2143, loss = 0.10996471\n",
      "Iteration 2144, loss = 0.10542757\n",
      "Iteration 2145, loss = 0.10392229\n",
      "Iteration 2146, loss = 0.10181134\n",
      "Iteration 2147, loss = 0.11484037\n",
      "Iteration 2148, loss = 0.15469788\n",
      "Iteration 2149, loss = 0.11008336\n",
      "Iteration 2150, loss = 0.10876082\n",
      "Iteration 2151, loss = 0.11868224\n",
      "Iteration 2152, loss = 0.14517480\n",
      "Iteration 2153, loss = 0.13255473\n",
      "Iteration 2154, loss = 0.12452585\n",
      "Iteration 2155, loss = 0.10880164\n",
      "Iteration 2156, loss = 0.11968861\n",
      "Iteration 2157, loss = 0.10907078\n",
      "Iteration 2158, loss = 0.11748338\n",
      "Iteration 2159, loss = 0.10659122\n",
      "Iteration 2160, loss = 0.10621107\n",
      "Iteration 2161, loss = 0.11198881\n",
      "Iteration 2162, loss = 0.11700247\n",
      "Iteration 2163, loss = 0.12129502\n",
      "Iteration 2164, loss = 0.11950935\n",
      "Iteration 2165, loss = 0.11364585\n",
      "Iteration 2166, loss = 0.12433017\n",
      "Iteration 2167, loss = 0.12359285\n",
      "Iteration 2168, loss = 0.10660844\n",
      "Iteration 2169, loss = 0.10517544\n",
      "Iteration 2170, loss = 0.10887296\n",
      "Iteration 2171, loss = 0.10876124\n",
      "Iteration 2172, loss = 0.10844885\n",
      "Iteration 2173, loss = 0.11484188\n",
      "Iteration 2174, loss = 0.11231120\n",
      "Iteration 2175, loss = 0.10590026\n",
      "Iteration 2176, loss = 0.14114584\n",
      "Iteration 2177, loss = 0.10824289\n",
      "Iteration 2178, loss = 0.10850950\n",
      "Iteration 2179, loss = 0.13222807\n",
      "Iteration 2180, loss = 0.13150037\n",
      "Iteration 2181, loss = 0.15674761\n",
      "Iteration 2182, loss = 0.14800116\n",
      "Iteration 2183, loss = 0.20065240\n",
      "Iteration 2184, loss = 0.16099223\n",
      "Iteration 2185, loss = 0.13979443\n",
      "Iteration 2186, loss = 0.12801059\n",
      "Iteration 2187, loss = 0.11304284\n",
      "Iteration 2188, loss = 0.11390252\n",
      "Iteration 2189, loss = 0.13296101\n",
      "Iteration 2190, loss = 0.12961878\n",
      "Iteration 2191, loss = 0.12271590\n",
      "Iteration 2192, loss = 0.13633438\n",
      "Iteration 2193, loss = 0.10879419\n",
      "Iteration 2194, loss = 0.10534579\n",
      "Iteration 2195, loss = 0.11019621\n",
      "Iteration 2196, loss = 0.11296130\n",
      "Iteration 2197, loss = 0.12139127\n",
      "Iteration 2198, loss = 0.14262572\n",
      "Iteration 2199, loss = 0.13807573\n",
      "Iteration 2200, loss = 0.23348874\n",
      "Iteration 2201, loss = 0.19761563\n",
      "Iteration 2202, loss = 0.56279561\n",
      "Iteration 2203, loss = 0.65571418\n",
      "Iteration 2204, loss = 0.93339369\n",
      "Iteration 2205, loss = 0.63427947\n",
      "Iteration 2206, loss = 0.50807512\n",
      "Iteration 2207, loss = 0.23659793\n",
      "Iteration 2208, loss = 0.16324975\n",
      "Iteration 2209, loss = 0.15503672\n",
      "Iteration 2210, loss = 0.12971429\n",
      "Iteration 2211, loss = 0.11381226\n",
      "Iteration 2212, loss = 0.12190445\n",
      "Iteration 2213, loss = 0.12678259\n",
      "Iteration 2214, loss = 0.12458785\n",
      "Iteration 2215, loss = 0.11237654\n",
      "Iteration 2216, loss = 0.11059320\n",
      "Iteration 2217, loss = 0.11829064\n",
      "Iteration 2218, loss = 0.11038642\n",
      "Iteration 2219, loss = 0.11826295\n",
      "Iteration 2220, loss = 0.11607783\n",
      "Iteration 2221, loss = 0.10913454\n",
      "Iteration 2222, loss = 0.10512907\n",
      "Iteration 2223, loss = 0.10446074\n",
      "Iteration 2224, loss = 0.10705310\n",
      "Iteration 2225, loss = 0.10551755\n",
      "Iteration 2226, loss = 0.10331656\n",
      "Iteration 2227, loss = 0.11422746\n",
      "Iteration 2228, loss = 0.11103750\n",
      "Iteration 2229, loss = 0.11099805\n",
      "Iteration 2230, loss = 0.10518353\n",
      "Iteration 2231, loss = 0.10991183\n",
      "Iteration 2232, loss = 0.11174921\n",
      "Iteration 2233, loss = 0.11939416\n",
      "Iteration 2234, loss = 0.11750100\n",
      "Iteration 2235, loss = 0.11114175\n",
      "Iteration 2236, loss = 0.11812039\n",
      "Iteration 2237, loss = 0.13533651\n",
      "Iteration 2238, loss = 0.12198236\n",
      "Iteration 2239, loss = 0.12947493\n",
      "Iteration 2240, loss = 0.11920203\n",
      "Iteration 2241, loss = 0.12591374\n",
      "Iteration 2242, loss = 0.10558745\n",
      "Iteration 2243, loss = 0.11064128\n",
      "Iteration 2244, loss = 0.10318443\n",
      "Iteration 2245, loss = 0.11691811\n",
      "Iteration 2246, loss = 0.11198939\n",
      "Iteration 2247, loss = 0.12224139\n",
      "Iteration 2248, loss = 0.11445099\n",
      "Iteration 2249, loss = 0.12878449\n",
      "Iteration 2250, loss = 0.12599073\n",
      "Iteration 2251, loss = 0.13839207\n",
      "Iteration 2252, loss = 0.16005957\n",
      "Iteration 2253, loss = 0.15192393\n",
      "Iteration 2254, loss = 0.14344486\n",
      "Iteration 2255, loss = 0.11771966\n",
      "Iteration 2256, loss = 0.10653287\n",
      "Iteration 2257, loss = 0.11045741\n",
      "Iteration 2258, loss = 0.11704258\n",
      "Iteration 2259, loss = 0.14332267\n",
      "Iteration 2260, loss = 0.11890716\n",
      "Iteration 2261, loss = 0.11600798\n",
      "Iteration 2262, loss = 0.13538980\n",
      "Iteration 2263, loss = 0.11042783\n",
      "Iteration 2264, loss = 0.12617934\n",
      "Iteration 2265, loss = 0.10797153\n",
      "Iteration 2266, loss = 0.10728452\n",
      "Iteration 2267, loss = 0.11088397\n",
      "Iteration 2268, loss = 0.11357439\n",
      "Iteration 2269, loss = 0.12387595\n",
      "Iteration 2270, loss = 0.10884916\n",
      "Iteration 2271, loss = 0.10152590\n",
      "Iteration 2272, loss = 0.10948055\n",
      "Iteration 2273, loss = 0.10747268\n",
      "Iteration 2274, loss = 0.13678392\n",
      "Iteration 2275, loss = 0.12713289\n",
      "Iteration 2276, loss = 0.11733037\n",
      "Iteration 2277, loss = 0.12216582\n",
      "Iteration 2278, loss = 0.10879943\n",
      "Iteration 2279, loss = 0.10622386\n",
      "Iteration 2280, loss = 0.10354927\n",
      "Iteration 2281, loss = 0.11050791\n",
      "Iteration 2282, loss = 0.11877924\n",
      "Iteration 2283, loss = 0.10839480\n",
      "Iteration 2284, loss = 0.10282333\n",
      "Iteration 2285, loss = 0.10443713\n",
      "Iteration 2286, loss = 0.10717388\n",
      "Iteration 2287, loss = 0.10221910\n",
      "Iteration 2288, loss = 0.11035854\n",
      "Iteration 2289, loss = 0.11155471\n",
      "Iteration 2290, loss = 0.10397452\n",
      "Iteration 2291, loss = 0.10973277\n",
      "Iteration 2292, loss = 0.11876573\n",
      "Iteration 2293, loss = 0.10722457\n",
      "Iteration 2294, loss = 0.11554280\n",
      "Iteration 2295, loss = 0.11574351\n",
      "Iteration 2296, loss = 0.10969916\n",
      "Iteration 2297, loss = 0.10608489\n",
      "Iteration 2298, loss = 0.11451038\n",
      "Iteration 2299, loss = 0.10446858\n",
      "Iteration 2300, loss = 0.12921333\n",
      "Iteration 2301, loss = 0.10516857\n",
      "Iteration 2302, loss = 0.10704799\n",
      "Iteration 2303, loss = 0.11038376\n",
      "Iteration 2304, loss = 0.11456875\n",
      "Iteration 2305, loss = 0.11919786\n",
      "Iteration 2306, loss = 0.10967285\n",
      "Iteration 2307, loss = 0.10906401\n",
      "Iteration 2308, loss = 0.10143524\n",
      "Iteration 2309, loss = 0.10370069\n",
      "Iteration 2310, loss = 0.13435099\n",
      "Iteration 2311, loss = 0.12941719\n",
      "Iteration 2312, loss = 0.11891298\n",
      "Iteration 2313, loss = 0.18352400\n",
      "Iteration 2314, loss = 0.12527277\n",
      "Iteration 2315, loss = 0.10480422\n",
      "Iteration 2316, loss = 0.10903784\n",
      "Iteration 2317, loss = 0.11254507\n",
      "Iteration 2318, loss = 0.10641081\n",
      "Iteration 2319, loss = 0.10537027\n",
      "Iteration 2320, loss = 0.11407394\n",
      "Iteration 2321, loss = 0.10250905\n",
      "Iteration 2322, loss = 0.12872416\n",
      "Iteration 2323, loss = 0.12751764\n",
      "Iteration 2324, loss = 0.11494500\n",
      "Iteration 2325, loss = 0.11213904\n",
      "Iteration 2326, loss = 0.14629702\n",
      "Iteration 2327, loss = 0.12485662\n",
      "Iteration 2328, loss = 0.10061857\n",
      "Iteration 2329, loss = 0.12317897\n",
      "Iteration 2330, loss = 0.11075731\n",
      "Iteration 2331, loss = 0.10499350\n",
      "Iteration 2332, loss = 0.10663154\n",
      "Iteration 2333, loss = 0.10834213\n",
      "Iteration 2334, loss = 0.10928130\n",
      "Iteration 2335, loss = 0.11354926\n",
      "Iteration 2336, loss = 0.11950066\n",
      "Iteration 2337, loss = 0.12782893\n",
      "Iteration 2338, loss = 0.13609248\n",
      "Iteration 2339, loss = 0.14190447\n",
      "Iteration 2340, loss = 0.14074965\n",
      "Iteration 2341, loss = 0.11630051\n",
      "Iteration 2342, loss = 0.11307790\n",
      "Iteration 2343, loss = 0.11675817\n",
      "Iteration 2344, loss = 0.10744609\n",
      "Iteration 2345, loss = 0.10476122\n",
      "Iteration 2346, loss = 0.10934488\n",
      "Iteration 2347, loss = 0.12200880\n",
      "Iteration 2348, loss = 0.12319076\n",
      "Iteration 2349, loss = 0.11213124\n",
      "Iteration 2350, loss = 0.11464484\n",
      "Iteration 2351, loss = 0.10550568\n",
      "Iteration 2352, loss = 0.10878216\n",
      "Iteration 2353, loss = 0.11794507\n",
      "Iteration 2354, loss = 0.11791583\n",
      "Iteration 2355, loss = 0.10865298\n",
      "Iteration 2356, loss = 0.11600777\n",
      "Iteration 2357, loss = 0.11199784\n",
      "Iteration 2358, loss = 0.10972202\n",
      "Iteration 2359, loss = 0.12689710\n",
      "Iteration 2360, loss = 0.10925260\n",
      "Iteration 2361, loss = 0.10409274\n",
      "Iteration 2362, loss = 0.10817659\n",
      "Iteration 2363, loss = 0.11275872\n",
      "Iteration 2364, loss = 0.11723320\n",
      "Iteration 2365, loss = 0.10798083\n",
      "Iteration 2366, loss = 0.10602185\n",
      "Iteration 2367, loss = 0.10906060\n",
      "Iteration 2368, loss = 0.10859527\n",
      "Iteration 2369, loss = 0.10383571\n",
      "Iteration 2370, loss = 0.10262424\n",
      "Iteration 2371, loss = 0.11366428\n",
      "Iteration 2372, loss = 0.11856513\n",
      "Iteration 2373, loss = 0.13352064\n",
      "Iteration 2374, loss = 0.39050274\n",
      "Iteration 2375, loss = 0.37337537\n",
      "Iteration 2376, loss = 0.19730168\n",
      "Iteration 2377, loss = 0.13558358\n",
      "Iteration 2378, loss = 0.35758482\n",
      "Iteration 2379, loss = 0.29869930\n",
      "Iteration 2380, loss = 0.31164938\n",
      "Iteration 2381, loss = 0.25539728\n",
      "Iteration 2382, loss = 0.16456872\n",
      "Iteration 2383, loss = 0.17457652\n",
      "Iteration 2384, loss = 0.20232237\n",
      "Iteration 2385, loss = 0.18946165\n",
      "Iteration 2386, loss = 0.17766076\n",
      "Iteration 2387, loss = 0.21438467\n",
      "Iteration 2388, loss = 0.17886160\n",
      "Iteration 2389, loss = 0.26473185\n",
      "Iteration 2390, loss = 0.23683197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2391, loss = 0.14741909\n",
      "Iteration 2392, loss = 0.14419423\n",
      "Iteration 2393, loss = 0.17382782\n",
      "Iteration 2394, loss = 0.21249328\n",
      "Iteration 2395, loss = 0.17918856\n",
      "Iteration 2396, loss = 0.23565110\n",
      "Iteration 2397, loss = 0.15067892\n",
      "Iteration 2398, loss = 0.16835383\n",
      "Iteration 2399, loss = 0.12186128\n",
      "Iteration 2400, loss = 0.12806464\n",
      "Iteration 2401, loss = 0.20320796\n",
      "Iteration 2402, loss = 0.23478705\n",
      "Iteration 2403, loss = 0.23007879\n",
      "Iteration 2404, loss = 0.36756330\n",
      "Iteration 2405, loss = 0.47769154\n",
      "Iteration 2406, loss = 0.80124271\n",
      "Iteration 2407, loss = 3.56518717\n",
      "Iteration 2408, loss = 5.04673521\n",
      "Iteration 2409, loss = 3.65235224\n",
      "Iteration 2410, loss = 1.27640187\n",
      "Iteration 2411, loss = 1.29931618\n",
      "Iteration 2412, loss = 0.90790993\n",
      "Iteration 2413, loss = 0.68127211\n",
      "Iteration 2414, loss = 0.97484920\n",
      "Iteration 2415, loss = 1.12479369\n",
      "Iteration 2416, loss = 0.72283857\n",
      "Iteration 2417, loss = 0.82251744\n",
      "Iteration 2418, loss = 0.94617359\n",
      "Iteration 2419, loss = 0.77052225\n",
      "Iteration 2420, loss = 0.86796248\n",
      "Iteration 2421, loss = 0.77624544\n",
      "Iteration 2422, loss = 0.77041738\n",
      "Iteration 2423, loss = 0.70932340\n",
      "Iteration 2424, loss = 0.71549435\n",
      "Iteration 2425, loss = 0.82716277\n",
      "Iteration 2426, loss = 0.68703538\n",
      "Iteration 2427, loss = 0.87729244\n",
      "Iteration 2428, loss = 0.99691887\n",
      "Iteration 2429, loss = 0.77673745\n",
      "Iteration 2430, loss = 0.68378496\n",
      "Iteration 2431, loss = 0.55123737\n",
      "Iteration 2432, loss = 0.77510296\n",
      "Iteration 2433, loss = 0.98512825\n",
      "Iteration 2434, loss = 0.72729526\n",
      "Iteration 2435, loss = 0.79696416\n",
      "Iteration 2436, loss = 0.60594082\n",
      "Iteration 2437, loss = 0.67309551\n",
      "Iteration 2438, loss = 0.49235126\n",
      "Iteration 2439, loss = 0.57487573\n",
      "Iteration 2440, loss = 0.36454793\n",
      "Iteration 2441, loss = 0.27262819\n",
      "Iteration 2442, loss = 0.25951926\n",
      "Iteration 2443, loss = 0.25313268\n",
      "Iteration 2444, loss = 0.37767568\n",
      "Iteration 2445, loss = 0.35738112\n",
      "Iteration 2446, loss = 0.45942862\n",
      "Iteration 2447, loss = 0.31796298\n",
      "Iteration 2448, loss = 0.26592708\n",
      "Iteration 2449, loss = 0.31563022\n",
      "Iteration 2450, loss = 0.46643263\n",
      "Iteration 2451, loss = 0.33002831\n",
      "Iteration 2452, loss = 0.56211939\n",
      "Iteration 2453, loss = 0.41968971\n",
      "Iteration 2454, loss = 0.44830518\n",
      "Iteration 2455, loss = 0.25605796\n",
      "Iteration 2456, loss = 0.30856926\n",
      "Iteration 2457, loss = 0.25422901\n",
      "Iteration 2458, loss = 0.16511256\n",
      "Iteration 2459, loss = 0.36142916\n",
      "Iteration 2460, loss = 0.49341006\n",
      "Iteration 2461, loss = 0.30922211\n",
      "Iteration 2462, loss = 0.22153381\n",
      "Iteration 2463, loss = 0.20249562\n",
      "Iteration 2464, loss = 0.19022282\n",
      "Iteration 2465, loss = 0.14416471\n",
      "Iteration 2466, loss = 0.12210304\n",
      "Iteration 2467, loss = 0.11497054\n",
      "Iteration 2468, loss = 0.11236487\n",
      "Iteration 2469, loss = 0.11060165\n",
      "Iteration 2470, loss = 0.10922079\n",
      "Iteration 2471, loss = 0.11861174\n",
      "Iteration 2472, loss = 0.11389821\n",
      "Iteration 2473, loss = 0.12435715\n",
      "Iteration 2474, loss = 0.13844365\n",
      "Iteration 2475, loss = 0.12846256\n",
      "Iteration 2476, loss = 0.10567050\n",
      "Iteration 2477, loss = 0.11769676\n",
      "Iteration 2478, loss = 0.12686127\n",
      "Iteration 2479, loss = 0.10747118\n",
      "Iteration 2480, loss = 0.11695247\n",
      "Iteration 2481, loss = 0.12474130\n",
      "Iteration 2482, loss = 0.20943750\n",
      "Iteration 2483, loss = 0.47190451\n",
      "Iteration 2484, loss = 0.33885925\n",
      "Iteration 2485, loss = 0.27908439\n",
      "Iteration 2486, loss = 0.18656667\n",
      "Iteration 2487, loss = 0.15678658\n",
      "Iteration 2488, loss = 0.21297482\n",
      "Iteration 2489, loss = 0.22112703\n",
      "Iteration 2490, loss = 0.18652237\n",
      "Iteration 2491, loss = 0.17012298\n",
      "Iteration 2492, loss = 0.14514126\n",
      "Iteration 2493, loss = 0.17159365\n",
      "Iteration 2494, loss = 0.12997699\n",
      "Iteration 2495, loss = 0.13129135\n",
      "Iteration 2496, loss = 0.12465135\n",
      "Iteration 2497, loss = 0.12625836\n",
      "Iteration 2498, loss = 0.13860497\n",
      "Iteration 2499, loss = 0.12842047\n",
      "Iteration 2500, loss = 0.29984494\n",
      "Iteration 2501, loss = 0.19375593\n",
      "Iteration 2502, loss = 0.14064032\n",
      "Iteration 2503, loss = 0.12916091\n",
      "Iteration 2504, loss = 0.13018454\n",
      "Iteration 2505, loss = 0.11834178\n",
      "Iteration 2506, loss = 0.12572711\n",
      "Iteration 2507, loss = 0.10966806\n",
      "Iteration 2508, loss = 0.10990780\n",
      "Iteration 2509, loss = 0.11536068\n",
      "Iteration 2510, loss = 0.11748152\n",
      "Iteration 2511, loss = 0.11670825\n",
      "Iteration 2512, loss = 0.11529875\n",
      "Iteration 2513, loss = 0.15580934\n",
      "Iteration 2514, loss = 0.14002088\n",
      "Iteration 2515, loss = 0.14315963\n",
      "Iteration 2516, loss = 0.19707434\n",
      "Iteration 2517, loss = 0.18991071\n",
      "Iteration 2518, loss = 0.18087483\n",
      "Iteration 2519, loss = 0.22702701\n",
      "Iteration 2520, loss = 0.49093968\n",
      "Iteration 2521, loss = 0.22962592\n",
      "Iteration 2522, loss = 0.20396623\n",
      "Iteration 2523, loss = 0.30392513\n",
      "Iteration 2524, loss = 0.33805832\n",
      "Iteration 2525, loss = 0.25747669\n",
      "Iteration 2526, loss = 0.25912872\n",
      "Iteration 2527, loss = 0.12709465\n",
      "Iteration 2528, loss = 0.14702042\n",
      "Iteration 2529, loss = 0.22373114\n",
      "Iteration 2530, loss = 0.32800334\n",
      "Iteration 2531, loss = 0.52375803\n",
      "Iteration 2532, loss = 0.32430790\n",
      "Iteration 2533, loss = 0.26964313\n",
      "Iteration 2534, loss = 0.19115490\n",
      "Iteration 2535, loss = 0.21654278\n",
      "Iteration 2536, loss = 0.13578540\n",
      "Iteration 2537, loss = 0.11839156\n",
      "Iteration 2538, loss = 0.14090402\n",
      "Iteration 2539, loss = 0.17809708\n",
      "Iteration 2540, loss = 0.14086005\n",
      "Iteration 2541, loss = 0.11805093\n",
      "Iteration 2542, loss = 0.16619030\n",
      "Iteration 2543, loss = 0.14043232\n",
      "Iteration 2544, loss = 0.13274671\n",
      "Iteration 2545, loss = 0.10826136\n",
      "Iteration 2546, loss = 0.11229051\n",
      "Iteration 2547, loss = 0.11833981\n",
      "Iteration 2548, loss = 0.13299207\n",
      "Iteration 2549, loss = 0.29565756\n",
      "Iteration 2550, loss = 0.26037038\n",
      "Iteration 2551, loss = 0.53887280\n",
      "Iteration 2552, loss = 0.34905752\n",
      "Iteration 2553, loss = 0.21020381\n",
      "Iteration 2554, loss = 0.25032955\n",
      "Iteration 2555, loss = 0.12055446\n",
      "Iteration 2556, loss = 0.10498797\n",
      "Iteration 2557, loss = 0.13019784\n",
      "Iteration 2558, loss = 0.15062595\n",
      "Iteration 2559, loss = 0.12107096\n",
      "Iteration 2560, loss = 0.11018119\n",
      "Iteration 2561, loss = 0.10239308\n",
      "Iteration 2562, loss = 0.10988874\n",
      "Iteration 2563, loss = 0.12772010\n",
      "Iteration 2564, loss = 0.13265034\n",
      "Iteration 2565, loss = 0.10257076\n",
      "Iteration 2566, loss = 0.12174737\n",
      "Iteration 2567, loss = 0.10905328\n",
      "Iteration 2568, loss = 0.10175241\n",
      "Iteration 2569, loss = 0.11575880\n",
      "Iteration 2570, loss = 0.10619291\n",
      "Iteration 2571, loss = 0.11046133\n",
      "Iteration 2572, loss = 0.10348801\n",
      "Iteration 2573, loss = 0.11376662\n",
      "Iteration 2574, loss = 0.11159792\n",
      "Iteration 2575, loss = 0.10790401\n",
      "Iteration 2576, loss = 0.10908798\n",
      "Iteration 2577, loss = 0.10506565\n",
      "Iteration 2578, loss = 0.10785458\n",
      "Iteration 2579, loss = 0.10745682\n",
      "Iteration 2580, loss = 0.11749745\n",
      "Iteration 2581, loss = 0.11277899\n",
      "Iteration 2582, loss = 0.10722670\n",
      "Iteration 2583, loss = 0.13618282\n",
      "Iteration 2584, loss = 0.23299994\n",
      "Iteration 2585, loss = 0.17853535\n",
      "Iteration 2586, loss = 0.22702601\n",
      "Iteration 2587, loss = 0.15701915\n",
      "Iteration 2588, loss = 0.12403796\n",
      "Iteration 2589, loss = 0.11530140\n",
      "Iteration 2590, loss = 0.11919789\n",
      "Iteration 2591, loss = 0.11630256\n",
      "Iteration 2592, loss = 0.10478690\n",
      "Iteration 2593, loss = 0.11962267\n",
      "Iteration 2594, loss = 0.11023368\n",
      "Iteration 2595, loss = 0.11279873\n",
      "Iteration 2596, loss = 0.10291068\n",
      "Iteration 2597, loss = 0.10581930\n",
      "Iteration 2598, loss = 0.11740576\n",
      "Iteration 2599, loss = 0.11328099\n",
      "Iteration 2600, loss = 0.11806472\n",
      "Iteration 2601, loss = 0.11445223\n",
      "Iteration 2602, loss = 0.11256497\n",
      "Iteration 2603, loss = 0.10848316\n",
      "Iteration 2604, loss = 0.11276453\n",
      "Iteration 2605, loss = 0.11313591\n",
      "Iteration 2606, loss = 0.11345676\n",
      "Iteration 2607, loss = 0.11924857\n",
      "Iteration 2608, loss = 0.11329237\n",
      "Iteration 2609, loss = 0.10643848\n",
      "Iteration 2610, loss = 0.10603533\n",
      "Iteration 2611, loss = 0.10964652\n",
      "Iteration 2612, loss = 0.13195519\n",
      "Iteration 2613, loss = 0.14600965\n",
      "Iteration 2614, loss = 0.21223267\n",
      "Iteration 2615, loss = 0.13471552\n",
      "Iteration 2616, loss = 0.17189275\n",
      "Iteration 2617, loss = 0.13972605\n",
      "Iteration 2618, loss = 0.10890175\n",
      "Iteration 2619, loss = 0.11703402\n",
      "Iteration 2620, loss = 0.12470110\n",
      "Iteration 2621, loss = 0.12194791\n",
      "Iteration 2622, loss = 0.10799799\n",
      "Iteration 2623, loss = 0.10332323\n",
      "Iteration 2624, loss = 0.10638636\n",
      "Iteration 2625, loss = 0.11830741\n",
      "Iteration 2626, loss = 0.11166902\n",
      "Iteration 2627, loss = 0.10636505\n",
      "Iteration 2628, loss = 0.12273389\n",
      "Iteration 2629, loss = 0.15324587\n",
      "Iteration 2630, loss = 0.11496114\n",
      "Iteration 2631, loss = 0.14895096\n",
      "Iteration 2632, loss = 0.11735020\n",
      "Iteration 2633, loss = 0.11228193\n",
      "Iteration 2634, loss = 0.11090957\n",
      "Iteration 2635, loss = 0.13839050\n",
      "Iteration 2636, loss = 0.12231755\n",
      "Iteration 2637, loss = 0.11099250\n",
      "Iteration 2638, loss = 0.11358921\n",
      "Iteration 2639, loss = 0.10607481\n",
      "Iteration 2640, loss = 0.13038984\n",
      "Iteration 2641, loss = 0.11255951\n",
      "Iteration 2642, loss = 0.11672481\n",
      "Iteration 2643, loss = 0.11610152\n",
      "Iteration 2644, loss = 0.15603030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2645, loss = 0.21827133\n",
      "Iteration 2646, loss = 0.20062254\n",
      "Iteration 2647, loss = 0.19088753\n",
      "Iteration 2648, loss = 0.18542717\n",
      "Iteration 2649, loss = 0.25088103\n",
      "Iteration 2650, loss = 0.11305369\n",
      "Iteration 2651, loss = 0.11859219\n",
      "Iteration 2652, loss = 0.12600123\n",
      "Iteration 2653, loss = 0.10733659\n",
      "Iteration 2654, loss = 0.11073141\n",
      "Iteration 2655, loss = 0.10781223\n",
      "Iteration 2656, loss = 0.12148318\n",
      "Iteration 2657, loss = 0.11747422\n",
      "Iteration 2658, loss = 0.13321761\n",
      "Iteration 2659, loss = 0.11825851\n",
      "Iteration 2660, loss = 0.10434321\n",
      "Iteration 2661, loss = 0.11371449\n",
      "Iteration 2662, loss = 0.12102616\n",
      "Iteration 2663, loss = 0.12598940\n",
      "Iteration 2664, loss = 0.12633246\n",
      "Iteration 2665, loss = 0.16296622\n",
      "Iteration 2666, loss = 0.15219574\n",
      "Iteration 2667, loss = 0.13910349\n",
      "Iteration 2668, loss = 0.18539486\n",
      "Iteration 2669, loss = 0.14538181\n",
      "Iteration 2670, loss = 0.11688500\n",
      "Iteration 2671, loss = 0.14915691\n",
      "Iteration 2672, loss = 0.11863498\n",
      "Iteration 2673, loss = 0.12977781\n",
      "Iteration 2674, loss = 0.11943253\n",
      "Iteration 2675, loss = 0.11492252\n",
      "Iteration 2676, loss = 0.17840736\n",
      "Iteration 2677, loss = 0.25280268\n",
      "Iteration 2678, loss = 0.15859449\n",
      "Iteration 2679, loss = 0.12034000\n",
      "Iteration 2680, loss = 0.13145129\n",
      "Iteration 2681, loss = 0.16583671\n",
      "Iteration 2682, loss = 0.23777594\n",
      "Iteration 2683, loss = 0.18386902\n",
      "Iteration 2684, loss = 0.16519193\n",
      "Iteration 2685, loss = 0.14636719\n",
      "Iteration 2686, loss = 0.10927211\n",
      "Iteration 2687, loss = 0.11425222\n",
      "Iteration 2688, loss = 0.12611280\n",
      "Iteration 2689, loss = 0.12991481\n",
      "Iteration 2690, loss = 0.12737605\n",
      "Iteration 2691, loss = 0.14583186\n",
      "Iteration 2692, loss = 0.11610490\n",
      "Iteration 2693, loss = 0.13331014\n",
      "Iteration 2694, loss = 0.10851382\n",
      "Iteration 2695, loss = 0.15812003\n",
      "Iteration 2696, loss = 0.12188075\n",
      "Iteration 2697, loss = 0.13081826\n",
      "Iteration 2698, loss = 0.15083606\n",
      "Iteration 2699, loss = 0.11542782\n",
      "Iteration 2700, loss = 0.12877477\n",
      "Iteration 2701, loss = 0.11047435\n",
      "Iteration 2702, loss = 0.11599022\n",
      "Iteration 2703, loss = 0.10454188\n",
      "Iteration 2704, loss = 0.11391462\n",
      "Iteration 2705, loss = 0.10955978\n",
      "Iteration 2706, loss = 0.10559395\n",
      "Iteration 2707, loss = 0.11764810\n",
      "Iteration 2708, loss = 0.14832489\n",
      "Iteration 2709, loss = 0.16188686\n",
      "Iteration 2710, loss = 0.19773460\n",
      "Iteration 2711, loss = 0.20308031\n",
      "Iteration 2712, loss = 0.22620213\n",
      "Iteration 2713, loss = 0.17467872\n",
      "Iteration 2714, loss = 0.25288788\n",
      "Iteration 2715, loss = 0.26581590\n",
      "Iteration 2716, loss = 0.44454600\n",
      "Iteration 2717, loss = 0.32871723\n",
      "Iteration 2718, loss = 0.15663114\n",
      "Iteration 2719, loss = 0.20808852\n",
      "Iteration 2720, loss = 0.15361661\n",
      "Iteration 2721, loss = 0.18683707\n",
      "Iteration 2722, loss = 0.11487963\n",
      "Iteration 2723, loss = 0.12798322\n",
      "Iteration 2724, loss = 0.12036119\n",
      "Iteration 2725, loss = 0.11447432\n",
      "Iteration 2726, loss = 0.12310247\n",
      "Iteration 2727, loss = 0.11328535\n",
      "Iteration 2728, loss = 0.12051361\n",
      "Iteration 2729, loss = 0.11757080\n",
      "Iteration 2730, loss = 0.11465391\n",
      "Iteration 2731, loss = 0.10394246\n",
      "Iteration 2732, loss = 0.11204831\n",
      "Iteration 2733, loss = 0.11114298\n",
      "Iteration 2734, loss = 0.10640225\n",
      "Iteration 2735, loss = 0.14392105\n",
      "Iteration 2736, loss = 0.12109560\n",
      "Iteration 2737, loss = 0.11281682\n",
      "Iteration 2738, loss = 0.10886432\n",
      "Iteration 2739, loss = 0.12573920\n",
      "Iteration 2740, loss = 0.10474141\n",
      "Iteration 2741, loss = 0.11653021\n",
      "Iteration 2742, loss = 0.11273785\n",
      "Iteration 2743, loss = 0.10784488\n",
      "Iteration 2744, loss = 0.12413168\n",
      "Iteration 2745, loss = 0.14482129\n",
      "Iteration 2746, loss = 0.11124618\n",
      "Iteration 2747, loss = 0.11368200\n",
      "Iteration 2748, loss = 0.11649735\n",
      "Iteration 2749, loss = 0.10313776\n",
      "Iteration 2750, loss = 0.10775597\n",
      "Iteration 2751, loss = 0.12595199\n",
      "Iteration 2752, loss = 0.11817948\n",
      "Iteration 2753, loss = 0.11779721\n",
      "Iteration 2754, loss = 0.11359796\n",
      "Iteration 2755, loss = 0.12134677\n",
      "Iteration 2756, loss = 0.12566138\n",
      "Iteration 2757, loss = 0.10682683\n",
      "Iteration 2758, loss = 0.10718164\n",
      "Iteration 2759, loss = 0.11577706\n",
      "Iteration 2760, loss = 0.12076885\n",
      "Iteration 2761, loss = 0.11299794\n",
      "Iteration 2762, loss = 0.10991249\n",
      "Iteration 2763, loss = 0.10761246\n",
      "Iteration 2764, loss = 0.10468804\n",
      "Iteration 2765, loss = 0.11109917\n",
      "Iteration 2766, loss = 0.11826712\n",
      "Iteration 2767, loss = 0.11838457\n",
      "Iteration 2768, loss = 0.12500787\n",
      "Iteration 2769, loss = 0.11713630\n",
      "Iteration 2770, loss = 0.14407984\n",
      "Iteration 2771, loss = 0.19272450\n",
      "Iteration 2772, loss = 0.21580947\n",
      "Iteration 2773, loss = 0.14640857\n",
      "Iteration 2774, loss = 0.14343199\n",
      "Iteration 2775, loss = 0.12316061\n",
      "Iteration 2776, loss = 0.11373021\n",
      "Iteration 2777, loss = 0.10293790\n",
      "Iteration 2778, loss = 0.13609082\n",
      "Iteration 2779, loss = 0.12260223\n",
      "Iteration 2780, loss = 0.12295898\n",
      "Iteration 2781, loss = 0.12115793\n",
      "Iteration 2782, loss = 0.11093143\n",
      "Iteration 2783, loss = 0.10769000\n",
      "Iteration 2784, loss = 0.10689832\n",
      "Iteration 2785, loss = 0.10232902\n",
      "Iteration 2786, loss = 0.10335868\n",
      "Iteration 2787, loss = 0.11453108\n",
      "Iteration 2788, loss = 0.10858660\n",
      "Iteration 2789, loss = 0.10383652\n",
      "Iteration 2790, loss = 0.11044980\n",
      "Iteration 2791, loss = 0.10717845\n",
      "Iteration 2792, loss = 0.10746232\n",
      "Iteration 2793, loss = 0.10924641\n",
      "Iteration 2794, loss = 0.12794331\n",
      "Iteration 2795, loss = 0.14283575\n",
      "Iteration 2796, loss = 0.10870254\n",
      "Iteration 2797, loss = 0.10695208\n",
      "Iteration 2798, loss = 0.10717797\n",
      "Iteration 2799, loss = 0.12018005\n",
      "Iteration 2800, loss = 0.10847763\n",
      "Iteration 2801, loss = 0.11027329\n",
      "Iteration 2802, loss = 0.11514243\n",
      "Iteration 2803, loss = 0.11368872\n",
      "Iteration 2804, loss = 0.11846059\n",
      "Iteration 2805, loss = 0.14675328\n",
      "Iteration 2806, loss = 0.13202838\n",
      "Iteration 2807, loss = 0.14273405\n",
      "Iteration 2808, loss = 0.12184846\n",
      "Iteration 2809, loss = 0.10611991\n",
      "Iteration 2810, loss = 0.10342461\n",
      "Iteration 2811, loss = 0.11164297\n",
      "Iteration 2812, loss = 0.12660346\n",
      "Iteration 2813, loss = 0.11318196\n",
      "Iteration 2814, loss = 0.10648373\n",
      "Iteration 2815, loss = 0.10406601\n",
      "Iteration 2816, loss = 0.10506049\n",
      "Iteration 2817, loss = 0.10339980\n",
      "Iteration 2818, loss = 0.10714749\n",
      "Iteration 2819, loss = 0.10393667\n",
      "Iteration 2820, loss = 0.10586826\n",
      "Iteration 2821, loss = 0.10938011\n",
      "Iteration 2822, loss = 0.10638214\n",
      "Iteration 2823, loss = 0.11656466\n",
      "Iteration 2824, loss = 0.10458917\n",
      "Iteration 2825, loss = 0.10831062\n",
      "Iteration 2826, loss = 0.10860878\n",
      "Iteration 2827, loss = 0.10651623\n",
      "Iteration 2828, loss = 0.10895142\n",
      "Iteration 2829, loss = 0.11689019\n",
      "Iteration 2830, loss = 0.13723593\n",
      "Iteration 2831, loss = 0.17249806\n",
      "Iteration 2832, loss = 0.33931501\n",
      "Iteration 2833, loss = 0.50006467\n",
      "Iteration 2834, loss = 0.78251898\n",
      "Iteration 2835, loss = 0.80629454\n",
      "Iteration 2836, loss = 0.89273403\n",
      "Iteration 2837, loss = 0.82897789\n",
      "Iteration 2838, loss = 0.80544299\n",
      "Iteration 2839, loss = 0.81534289\n",
      "Iteration 2840, loss = 0.66490361\n",
      "Iteration 2841, loss = 0.69534030\n",
      "Iteration 2842, loss = 0.49288239\n",
      "Iteration 2843, loss = 1.14914255\n",
      "Iteration 2844, loss = 2.80019483\n",
      "Iteration 2845, loss = 1.12706069\n",
      "Iteration 2846, loss = 0.83124341\n",
      "Iteration 2847, loss = 1.61781708\n",
      "Iteration 2848, loss = 1.19093856\n",
      "Iteration 2849, loss = 1.12327466\n",
      "Iteration 2850, loss = 0.81104686\n",
      "Iteration 2851, loss = 0.98484848\n",
      "Iteration 2852, loss = 0.59112463\n",
      "Iteration 2853, loss = 0.64512012\n",
      "Iteration 2854, loss = 0.38888509\n",
      "Iteration 2855, loss = 0.54497672\n",
      "Iteration 2856, loss = 0.85786206\n",
      "Iteration 2857, loss = 0.68458114\n",
      "Iteration 2858, loss = 0.71291123\n",
      "Iteration 2859, loss = 0.61246662\n",
      "Iteration 2860, loss = 0.48551041\n",
      "Iteration 2861, loss = 0.36276788\n",
      "Iteration 2862, loss = 0.34794130\n",
      "Iteration 2863, loss = 0.27052741\n",
      "Iteration 2864, loss = 0.29594931\n",
      "Iteration 2865, loss = 0.23871589\n",
      "Iteration 2866, loss = 0.19962358\n",
      "Iteration 2867, loss = 0.15738404\n",
      "Iteration 2868, loss = 0.13831876\n",
      "Iteration 2869, loss = 0.17410708\n",
      "Iteration 2870, loss = 0.14275061\n",
      "Iteration 2871, loss = 0.12821489\n",
      "Iteration 2872, loss = 0.12759475\n",
      "Iteration 2873, loss = 0.11687584\n",
      "Iteration 2874, loss = 0.12719554\n",
      "Iteration 2875, loss = 0.13350334\n",
      "Iteration 2876, loss = 0.15281917\n",
      "Iteration 2877, loss = 0.11545577\n",
      "Iteration 2878, loss = 0.11484062\n",
      "Iteration 2879, loss = 0.11098442\n",
      "Iteration 2880, loss = 0.11024859\n",
      "Iteration 2881, loss = 0.12906195\n",
      "Iteration 2882, loss = 0.11700123\n",
      "Iteration 2883, loss = 0.12160166\n",
      "Iteration 2884, loss = 0.11927194\n",
      "Iteration 2885, loss = 0.11369577\n",
      "Iteration 2886, loss = 0.12238962\n",
      "Iteration 2887, loss = 0.11199068\n",
      "Iteration 2888, loss = 0.10595034\n",
      "Iteration 2889, loss = 0.11512909\n",
      "Iteration 2890, loss = 0.11001991\n",
      "Iteration 2891, loss = 0.10656839\n",
      "Iteration 2892, loss = 0.10695500\n",
      "Iteration 2893, loss = 0.13088522\n",
      "Iteration 2894, loss = 0.11491780\n",
      "Iteration 2895, loss = 0.11444939\n",
      "Iteration 2896, loss = 0.13595704\n",
      "Iteration 2897, loss = 0.12989007\n",
      "Iteration 2898, loss = 0.12063847\n",
      "Iteration 2899, loss = 0.11706015\n",
      "Iteration 2900, loss = 0.12240973\n",
      "Iteration 2901, loss = 0.10795069\n",
      "Iteration 2902, loss = 0.13155446\n",
      "Iteration 2903, loss = 0.12389288\n",
      "Iteration 2904, loss = 0.11880429\n",
      "Iteration 2905, loss = 0.11845464\n",
      "Iteration 2906, loss = 0.11072166\n",
      "Iteration 2907, loss = 0.12096841\n",
      "Iteration 2908, loss = 0.12240254\n",
      "Iteration 2909, loss = 0.16127295\n",
      "Iteration 2910, loss = 0.14054240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2911, loss = 0.12528652\n",
      "Iteration 2912, loss = 0.11484491\n",
      "Iteration 2913, loss = 0.11719786\n",
      "Iteration 2914, loss = 0.10840499\n",
      "Iteration 2915, loss = 0.11209982\n",
      "Iteration 2916, loss = 0.10647113\n",
      "Iteration 2917, loss = 0.11842591\n",
      "Iteration 2918, loss = 0.13346335\n",
      "Iteration 2919, loss = 0.12770849\n",
      "Iteration 2920, loss = 0.12416381\n",
      "Iteration 2921, loss = 0.13731613\n",
      "Iteration 2922, loss = 0.11025839\n",
      "Iteration 2923, loss = 0.11975345\n",
      "Iteration 2924, loss = 0.11142359\n",
      "Iteration 2925, loss = 0.12008689\n",
      "Iteration 2926, loss = 0.13018440\n",
      "Iteration 2927, loss = 0.12417424\n",
      "Iteration 2928, loss = 0.12885417\n",
      "Iteration 2929, loss = 0.11033712\n",
      "Iteration 2930, loss = 0.10974055\n",
      "Iteration 2931, loss = 0.11162769\n",
      "Iteration 2932, loss = 0.11040678\n",
      "Iteration 2933, loss = 0.10719779\n",
      "Iteration 2934, loss = 0.13158184\n",
      "Iteration 2935, loss = 0.13375014\n",
      "Iteration 2936, loss = 0.11197919\n",
      "Iteration 2937, loss = 0.12242521\n",
      "Iteration 2938, loss = 0.13835705\n",
      "Iteration 2939, loss = 0.12670326\n",
      "Iteration 2940, loss = 0.12120962\n",
      "Iteration 2941, loss = 0.12893059\n",
      "Iteration 2942, loss = 0.11008681\n",
      "Iteration 2943, loss = 0.11445273\n",
      "Iteration 2944, loss = 0.11353382\n",
      "Iteration 2945, loss = 0.11300996\n",
      "Iteration 2946, loss = 0.10732866\n",
      "Iteration 2947, loss = 0.10923829\n",
      "Iteration 2948, loss = 0.11144317\n",
      "Iteration 2949, loss = 0.11487631\n",
      "Iteration 2950, loss = 0.10733955\n",
      "Iteration 2951, loss = 0.10851058\n",
      "Iteration 2952, loss = 0.10713043\n",
      "Iteration 2953, loss = 0.10873259\n",
      "Iteration 2954, loss = 0.10412471\n",
      "Iteration 2955, loss = 0.10596374\n",
      "Iteration 2956, loss = 0.10630682\n",
      "Iteration 2957, loss = 0.10690318\n",
      "Iteration 2958, loss = 0.10959913\n",
      "Iteration 2959, loss = 0.10559981\n",
      "Iteration 2960, loss = 0.11561120\n",
      "Iteration 2961, loss = 0.10716333\n",
      "Iteration 2962, loss = 0.12531517\n",
      "Iteration 2963, loss = 0.13258804\n",
      "Iteration 2964, loss = 0.12719446\n",
      "Iteration 2965, loss = 0.11517273\n",
      "Iteration 2966, loss = 0.10752277\n",
      "Iteration 2967, loss = 0.11504610\n",
      "Iteration 2968, loss = 0.10594287\n",
      "Iteration 2969, loss = 0.11506986\n",
      "Iteration 2970, loss = 0.11517459\n",
      "Iteration 2971, loss = 0.10389641\n",
      "Iteration 2972, loss = 0.10291714\n",
      "Iteration 2973, loss = 0.11044150\n",
      "Iteration 2974, loss = 0.10558211\n",
      "Iteration 2975, loss = 0.10031221\n",
      "Iteration 2976, loss = 0.11664781\n",
      "Iteration 2977, loss = 0.10361245\n",
      "Iteration 2978, loss = 0.11682619\n",
      "Iteration 2979, loss = 0.12572181\n",
      "Iteration 2980, loss = 0.11369181\n",
      "Iteration 2981, loss = 0.11480011\n",
      "Iteration 2982, loss = 0.11033175\n",
      "Iteration 2983, loss = 0.12412937\n",
      "Iteration 2984, loss = 0.11730605\n",
      "Iteration 2985, loss = 0.10576506\n",
      "Iteration 2986, loss = 0.10880575\n",
      "Iteration 2987, loss = 0.10785387\n",
      "Iteration 2988, loss = 0.10742906\n",
      "Iteration 2989, loss = 0.11174546\n",
      "Iteration 2990, loss = 0.10801738\n",
      "Iteration 2991, loss = 0.11906675\n",
      "Iteration 2992, loss = 0.13271447\n",
      "Iteration 2993, loss = 0.12434075\n",
      "Iteration 2994, loss = 0.11058264\n",
      "Iteration 2995, loss = 0.11617492\n",
      "Iteration 2996, loss = 0.12477851\n",
      "Iteration 2997, loss = 0.12326394\n",
      "Iteration 2998, loss = 0.13804827\n",
      "Iteration 2999, loss = 0.13603997\n",
      "Iteration 3000, loss = 0.11849749\n",
      "Iteration 3001, loss = 0.10810386\n",
      "Iteration 3002, loss = 0.11394484\n",
      "Iteration 3003, loss = 0.10754389\n",
      "Iteration 3004, loss = 0.11508382\n",
      "Iteration 3005, loss = 0.12241550\n",
      "Iteration 3006, loss = 0.12853483\n",
      "Iteration 3007, loss = 0.11673583\n",
      "Iteration 3008, loss = 0.10435004\n",
      "Iteration 3009, loss = 0.10202883\n",
      "Iteration 3010, loss = 0.10409064\n",
      "Iteration 3011, loss = 0.10841770\n",
      "Iteration 3012, loss = 0.10837752\n",
      "Iteration 3013, loss = 0.10762962\n",
      "Iteration 3014, loss = 0.10349348\n",
      "Iteration 3015, loss = 0.11174527\n",
      "Iteration 3016, loss = 0.11656240\n",
      "Iteration 3017, loss = 0.10780855\n",
      "Iteration 3018, loss = 0.10956044\n",
      "Iteration 3019, loss = 0.10484736\n",
      "Iteration 3020, loss = 0.11640389\n",
      "Iteration 3021, loss = 0.11065406\n",
      "Iteration 3022, loss = 0.10889855\n",
      "Iteration 3023, loss = 0.10794727\n",
      "Iteration 3024, loss = 0.11672676\n",
      "Iteration 3025, loss = 0.10382680\n",
      "Iteration 3026, loss = 0.10878565\n",
      "Iteration 3027, loss = 0.11169344\n",
      "Iteration 3028, loss = 0.10964243\n",
      "Iteration 3029, loss = 0.11199126\n",
      "Iteration 3030, loss = 0.11938338\n",
      "Iteration 3031, loss = 0.12747855\n",
      "Iteration 3032, loss = 0.10416218\n",
      "Iteration 3033, loss = 0.10906369\n",
      "Iteration 3034, loss = 0.10648850\n",
      "Iteration 3035, loss = 0.10720832\n",
      "Iteration 3036, loss = 0.13085898\n",
      "Iteration 3037, loss = 0.12292454\n",
      "Iteration 3038, loss = 0.11105816\n",
      "Iteration 3039, loss = 0.13579260\n",
      "Iteration 3040, loss = 0.19458150\n",
      "Iteration 3041, loss = 0.40934833\n",
      "Iteration 3042, loss = 0.31793391\n",
      "Iteration 3043, loss = 0.16913864\n",
      "Iteration 3044, loss = 0.20293062\n",
      "Iteration 3045, loss = 0.11933507\n",
      "Iteration 3046, loss = 0.16235182\n",
      "Iteration 3047, loss = 0.11640852\n",
      "Iteration 3048, loss = 0.13600231\n",
      "Iteration 3049, loss = 0.18232184\n",
      "Iteration 3050, loss = 0.19883389\n",
      "Iteration 3051, loss = 0.13295528\n",
      "Iteration 3052, loss = 0.11463189\n",
      "Iteration 3053, loss = 0.11332342\n",
      "Iteration 3054, loss = 0.10815223\n",
      "Iteration 3055, loss = 0.12262779\n",
      "Iteration 3056, loss = 0.10867461\n",
      "Iteration 3057, loss = 0.10652213\n",
      "Iteration 3058, loss = 0.11049437\n",
      "Iteration 3059, loss = 0.11427798\n",
      "Iteration 3060, loss = 0.11696497\n",
      "Iteration 3061, loss = 0.12240872\n",
      "Iteration 3062, loss = 0.12430506\n",
      "Iteration 3063, loss = 0.12047670\n",
      "Iteration 3064, loss = 0.11815471\n",
      "Iteration 3065, loss = 0.11087213\n",
      "Iteration 3066, loss = 0.10628186\n",
      "Iteration 3067, loss = 0.11232357\n",
      "Iteration 3068, loss = 0.11318957\n",
      "Iteration 3069, loss = 0.09953701\n",
      "Iteration 3070, loss = 0.12656491\n",
      "Iteration 3071, loss = 0.12194737\n",
      "Iteration 3072, loss = 0.10494727\n",
      "Iteration 3073, loss = 0.10905598\n",
      "Iteration 3074, loss = 0.10541340\n",
      "Iteration 3075, loss = 0.11158891\n",
      "Iteration 3076, loss = 0.11774346\n",
      "Iteration 3077, loss = 0.13207257\n",
      "Iteration 3078, loss = 0.11593970\n",
      "Iteration 3079, loss = 0.10685558\n",
      "Iteration 3080, loss = 0.10681657\n",
      "Iteration 3081, loss = 0.10654971\n",
      "Iteration 3082, loss = 0.10272336\n",
      "Iteration 3083, loss = 0.12798380\n",
      "Iteration 3084, loss = 0.10966720\n",
      "Iteration 3085, loss = 0.11061781\n",
      "Iteration 3086, loss = 0.11313252\n",
      "Iteration 3087, loss = 0.10267617\n",
      "Iteration 3088, loss = 0.10815034\n",
      "Iteration 3089, loss = 0.09954091\n",
      "Iteration 3090, loss = 0.11247741\n",
      "Iteration 3091, loss = 0.10557380\n",
      "Iteration 3092, loss = 0.10361129\n",
      "Iteration 3093, loss = 0.10510611\n",
      "Iteration 3094, loss = 0.10580783\n",
      "Iteration 3095, loss = 0.10605528\n",
      "Iteration 3096, loss = 0.10805558\n",
      "Iteration 3097, loss = 0.11374165\n",
      "Iteration 3098, loss = 0.13496837\n",
      "Iteration 3099, loss = 0.12444844\n",
      "Iteration 3100, loss = 0.12478479\n",
      "Iteration 3101, loss = 0.15072693\n",
      "Iteration 3102, loss = 0.12501606\n",
      "Iteration 3103, loss = 0.12011471\n",
      "Iteration 3104, loss = 0.15986608\n",
      "Iteration 3105, loss = 0.17602163\n",
      "Iteration 3106, loss = 0.12090650\n",
      "Iteration 3107, loss = 0.11164097\n",
      "Iteration 3108, loss = 0.10680969\n",
      "Iteration 3109, loss = 0.10406314\n",
      "Iteration 3110, loss = 0.10552445\n",
      "Iteration 3111, loss = 0.10657754\n",
      "Iteration 3112, loss = 0.11754620\n",
      "Iteration 3113, loss = 0.10712047\n",
      "Iteration 3114, loss = 0.10821738\n",
      "Iteration 3115, loss = 0.11498644\n",
      "Iteration 3116, loss = 0.10885714\n",
      "Iteration 3117, loss = 0.10703570\n",
      "Iteration 3118, loss = 0.11059878\n",
      "Iteration 3119, loss = 0.10883350\n",
      "Iteration 3120, loss = 0.10972766\n",
      "Iteration 3121, loss = 0.10726259\n",
      "Iteration 3122, loss = 0.11979783\n",
      "Iteration 3123, loss = 0.11346642\n",
      "Iteration 3124, loss = 0.11027745\n",
      "Iteration 3125, loss = 0.10566827\n",
      "Iteration 3126, loss = 0.10771555\n",
      "Iteration 3127, loss = 0.11196358\n",
      "Iteration 3128, loss = 0.12034198\n",
      "Iteration 3129, loss = 0.11166191\n",
      "Iteration 3130, loss = 0.12707038\n",
      "Iteration 3131, loss = 0.11277178\n",
      "Iteration 3132, loss = 0.13013893\n",
      "Iteration 3133, loss = 0.21565898\n",
      "Iteration 3134, loss = 0.13272182\n",
      "Iteration 3135, loss = 0.16248526\n",
      "Iteration 3136, loss = 0.12085762\n",
      "Iteration 3137, loss = 0.11757625\n",
      "Iteration 3138, loss = 0.12129519\n",
      "Iteration 3139, loss = 0.12624062\n",
      "Iteration 3140, loss = 0.14421447\n",
      "Iteration 3141, loss = 0.12148358\n",
      "Iteration 3142, loss = 0.11802477\n",
      "Iteration 3143, loss = 0.11238409\n",
      "Iteration 3144, loss = 0.10555855\n",
      "Iteration 3145, loss = 0.10783867\n",
      "Iteration 3146, loss = 0.10874239\n",
      "Iteration 3147, loss = 0.10920162\n",
      "Iteration 3148, loss = 0.10227516\n",
      "Iteration 3149, loss = 0.11387078\n",
      "Iteration 3150, loss = 0.11478540\n",
      "Iteration 3151, loss = 0.12688221\n",
      "Iteration 3152, loss = 0.21968694\n",
      "Iteration 3153, loss = 0.24353469\n",
      "Iteration 3154, loss = 0.11843027\n",
      "Iteration 3155, loss = 0.13468236\n",
      "Iteration 3156, loss = 0.12008906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3157, loss = 0.11549156\n",
      "Iteration 3158, loss = 0.10714821\n",
      "Iteration 3159, loss = 0.10792383\n",
      "Iteration 3160, loss = 0.12021620\n",
      "Iteration 3161, loss = 0.11361017\n",
      "Iteration 3162, loss = 0.11348429\n",
      "Iteration 3163, loss = 0.11285784\n",
      "Iteration 3164, loss = 0.10735906\n",
      "Iteration 3165, loss = 0.10691481\n",
      "Iteration 3166, loss = 0.10515932\n",
      "Iteration 3167, loss = 0.11453033\n",
      "Iteration 3168, loss = 0.11254111\n",
      "Iteration 3169, loss = 0.11927778\n",
      "Iteration 3170, loss = 0.11524709\n",
      "Iteration 3171, loss = 0.10682945\n",
      "Iteration 3172, loss = 0.10714906\n",
      "Iteration 3173, loss = 0.09891819\n",
      "Iteration 3174, loss = 0.12756247\n",
      "Iteration 3175, loss = 0.11570787\n",
      "Iteration 3176, loss = 0.11652389\n",
      "Iteration 3177, loss = 0.12231881\n",
      "Iteration 3178, loss = 0.13713175\n",
      "Iteration 3179, loss = 0.12863243\n",
      "Iteration 3180, loss = 0.12729576\n",
      "Iteration 3181, loss = 0.10349240\n",
      "Iteration 3182, loss = 0.11314136\n",
      "Iteration 3183, loss = 0.11285375\n",
      "Iteration 3184, loss = 0.11199535\n",
      "Iteration 3185, loss = 0.13757198\n",
      "Iteration 3186, loss = 0.12028169\n",
      "Iteration 3187, loss = 0.11337052\n",
      "Iteration 3188, loss = 0.11563457\n",
      "Iteration 3189, loss = 0.10777213\n",
      "Iteration 3190, loss = 0.11406754\n",
      "Iteration 3191, loss = 0.10886554\n",
      "Iteration 3192, loss = 0.11055493\n",
      "Iteration 3193, loss = 0.11112502\n",
      "Iteration 3194, loss = 0.10674185\n",
      "Iteration 3195, loss = 0.10540419\n",
      "Iteration 3196, loss = 0.10488420\n",
      "Iteration 3197, loss = 0.10500369\n",
      "Iteration 3198, loss = 0.11773882\n",
      "Iteration 3199, loss = 0.10755380\n",
      "Iteration 3200, loss = 0.10947749\n",
      "Iteration 3201, loss = 0.13108675\n",
      "Iteration 3202, loss = 0.11724294\n",
      "Iteration 3203, loss = 0.11183943\n",
      "Iteration 3204, loss = 0.10691906\n",
      "Iteration 3205, loss = 0.11059376\n",
      "Iteration 3206, loss = 0.10717496\n",
      "Iteration 3207, loss = 0.10600563\n",
      "Iteration 3208, loss = 0.12156273\n",
      "Iteration 3209, loss = 0.11901293\n",
      "Iteration 3210, loss = 0.11221206\n",
      "Iteration 3211, loss = 0.12012713\n",
      "Iteration 3212, loss = 0.12175368\n",
      "Iteration 3213, loss = 0.10458928\n",
      "Iteration 3214, loss = 0.11116578\n",
      "Iteration 3215, loss = 0.12333468\n",
      "Iteration 3216, loss = 0.12018588\n",
      "Iteration 3217, loss = 0.14490119\n",
      "Iteration 3218, loss = 0.10895286\n",
      "Iteration 3219, loss = 0.12281821\n",
      "Iteration 3220, loss = 0.12243271\n",
      "Iteration 3221, loss = 0.11317691\n",
      "Iteration 3222, loss = 0.11944382\n",
      "Iteration 3223, loss = 0.10502967\n",
      "Iteration 3224, loss = 0.10488811\n",
      "Iteration 3225, loss = 0.10849889\n",
      "Iteration 3226, loss = 0.11352152\n",
      "Iteration 3227, loss = 0.16601342\n",
      "Iteration 3228, loss = 0.20572216\n",
      "Iteration 3229, loss = 0.17760643\n",
      "Iteration 3230, loss = 0.16453157\n",
      "Iteration 3231, loss = 0.58794254\n",
      "Iteration 3232, loss = 2.33389385\n",
      "Iteration 3233, loss = 1.66585429\n",
      "Iteration 3234, loss = 2.04292419\n",
      "Iteration 3235, loss = 1.01001083\n",
      "Iteration 3236, loss = 0.85008829\n",
      "Iteration 3237, loss = 1.30901345\n",
      "Iteration 3238, loss = 1.16040292\n",
      "Iteration 3239, loss = 0.85525124\n",
      "Iteration 3240, loss = 1.04578921\n",
      "Iteration 3241, loss = 0.73829354\n",
      "Iteration 3242, loss = 0.59729505\n",
      "Iteration 3243, loss = 0.60852517\n",
      "Iteration 3244, loss = 0.51767211\n",
      "Iteration 3245, loss = 0.62740153\n",
      "Iteration 3246, loss = 0.46854726\n",
      "Iteration 3247, loss = 0.24118006\n",
      "Iteration 3248, loss = 0.24446004\n",
      "Iteration 3249, loss = 0.29118863\n",
      "Iteration 3250, loss = 0.13684895\n",
      "Iteration 3251, loss = 0.14329050\n",
      "Iteration 3252, loss = 0.26054543\n",
      "Iteration 3253, loss = 0.27529861\n",
      "Iteration 3254, loss = 0.16066315\n",
      "Iteration 3255, loss = 0.13359363\n",
      "Iteration 3256, loss = 0.17843756\n",
      "Iteration 3257, loss = 0.13143332\n",
      "Iteration 3258, loss = 0.16655640\n",
      "Iteration 3259, loss = 0.14073121\n",
      "Iteration 3260, loss = 0.13801689\n",
      "Iteration 3261, loss = 0.10962668\n",
      "Iteration 3262, loss = 0.13954681\n",
      "Iteration 3263, loss = 0.15410160\n",
      "Iteration 3264, loss = 0.15824934\n",
      "Iteration 3265, loss = 0.14538228\n",
      "Iteration 3266, loss = 0.12467424\n",
      "Iteration 3267, loss = 0.10680624\n",
      "Iteration 3268, loss = 0.11016943\n",
      "Iteration 3269, loss = 0.14795700\n",
      "Iteration 3270, loss = 0.14263726\n",
      "Iteration 3271, loss = 0.18033721\n",
      "Iteration 3272, loss = 0.12292839\n",
      "Iteration 3273, loss = 0.11394124\n",
      "Iteration 3274, loss = 0.11404947\n",
      "Iteration 3275, loss = 0.11394626\n",
      "Iteration 3276, loss = 0.11219133\n",
      "Iteration 3277, loss = 0.11145452\n",
      "Iteration 3278, loss = 0.10806966\n",
      "Iteration 3279, loss = 0.10779781\n",
      "Iteration 3280, loss = 0.10458831\n",
      "Iteration 3281, loss = 0.10913474\n",
      "Iteration 3282, loss = 0.10776235\n",
      "Iteration 3283, loss = 0.10940506\n",
      "Iteration 3284, loss = 0.12017044\n",
      "Iteration 3285, loss = 0.11314959\n",
      "Iteration 3286, loss = 0.12126197\n",
      "Iteration 3287, loss = 0.11409198\n",
      "Iteration 3288, loss = 0.11300362\n",
      "Iteration 3289, loss = 0.11891296\n",
      "Iteration 3290, loss = 0.11967903\n",
      "Iteration 3291, loss = 0.11113211\n",
      "Iteration 3292, loss = 0.12414068\n",
      "Iteration 3293, loss = 0.11104414\n",
      "Iteration 3294, loss = 0.12219032\n",
      "Iteration 3295, loss = 0.14478799\n",
      "Iteration 3296, loss = 0.11817146\n",
      "Iteration 3297, loss = 0.13016026\n",
      "Iteration 3298, loss = 0.17426217\n",
      "Iteration 3299, loss = 0.15468514\n",
      "Iteration 3300, loss = 0.13622788\n",
      "Iteration 3301, loss = 0.10805863\n",
      "Iteration 3302, loss = 0.11352872\n",
      "Iteration 3303, loss = 0.11968695\n",
      "Iteration 3304, loss = 0.10543714\n",
      "Iteration 3305, loss = 0.12829692\n",
      "Iteration 3306, loss = 0.16625863\n",
      "Iteration 3307, loss = 0.14297004\n",
      "Iteration 3308, loss = 0.11081337\n",
      "Iteration 3309, loss = 0.10856487\n",
      "Iteration 3310, loss = 0.13533439\n",
      "Iteration 3311, loss = 0.12880633\n",
      "Iteration 3312, loss = 0.11191170\n",
      "Iteration 3313, loss = 0.11600772\n",
      "Iteration 3314, loss = 0.14735897\n",
      "Iteration 3315, loss = 0.10906482\n",
      "Iteration 3316, loss = 0.11348936\n",
      "Iteration 3317, loss = 0.10616328\n",
      "Iteration 3318, loss = 0.12206762\n",
      "Iteration 3319, loss = 0.14122905\n",
      "Iteration 3320, loss = 0.10621548\n",
      "Iteration 3321, loss = 0.12353801\n",
      "Iteration 3322, loss = 0.12029040\n",
      "Iteration 3323, loss = 0.11678912\n",
      "Iteration 3324, loss = 0.12051053\n",
      "Iteration 3325, loss = 0.13051775\n",
      "Iteration 3326, loss = 0.10390757\n",
      "Iteration 3327, loss = 0.10535493\n",
      "Iteration 3328, loss = 0.10168709\n",
      "Iteration 3329, loss = 0.10881938\n",
      "Iteration 3330, loss = 0.10807405\n",
      "Iteration 3331, loss = 0.11757405\n",
      "Iteration 3332, loss = 0.13946155\n",
      "Iteration 3333, loss = 0.13058710\n",
      "Iteration 3334, loss = 0.12127687\n",
      "Iteration 3335, loss = 0.11326558\n",
      "Iteration 3336, loss = 0.10664791\n",
      "Iteration 3337, loss = 0.11702398\n",
      "Iteration 3338, loss = 0.11804804\n",
      "Iteration 3339, loss = 0.11341209\n",
      "Iteration 3340, loss = 0.10529816\n",
      "Iteration 3341, loss = 0.10740874\n",
      "Iteration 3342, loss = 0.12115777\n",
      "Iteration 3343, loss = 0.16902665\n",
      "Iteration 3344, loss = 0.15298608\n",
      "Iteration 3345, loss = 0.42255093\n",
      "Iteration 3346, loss = 0.50205866\n",
      "Iteration 3347, loss = 0.23659545\n",
      "Iteration 3348, loss = 0.21239266\n",
      "Iteration 3349, loss = 0.21298687\n",
      "Iteration 3350, loss = 0.31041313\n",
      "Iteration 3351, loss = 0.47694735\n",
      "Iteration 3352, loss = 0.49939803\n",
      "Iteration 3353, loss = 0.27126362\n",
      "Iteration 3354, loss = 0.46699637\n",
      "Iteration 3355, loss = 0.71724979\n",
      "Iteration 3356, loss = 0.86973684\n",
      "Iteration 3357, loss = 0.88075363\n",
      "Iteration 3358, loss = 0.82439804\n",
      "Iteration 3359, loss = 0.79865039\n",
      "Iteration 3360, loss = 0.58414838\n",
      "Iteration 3361, loss = 0.48838031\n",
      "Iteration 3362, loss = 0.42569647\n",
      "Iteration 3363, loss = 0.31979352\n",
      "Iteration 3364, loss = 0.17735285\n",
      "Iteration 3365, loss = 0.12218595\n",
      "Iteration 3366, loss = 0.14439855\n",
      "Iteration 3367, loss = 0.13907487\n",
      "Iteration 3368, loss = 0.11059653\n",
      "Iteration 3369, loss = 0.13075964\n",
      "Iteration 3370, loss = 0.11174628\n",
      "Iteration 3371, loss = 0.12598393\n",
      "Iteration 3372, loss = 0.11236965\n",
      "Iteration 3373, loss = 0.12509221\n",
      "Iteration 3374, loss = 0.14156974\n",
      "Iteration 3375, loss = 0.15124787\n",
      "Iteration 3376, loss = 0.12378898\n",
      "Iteration 3377, loss = 0.10847798\n",
      "Iteration 3378, loss = 0.11264897\n",
      "Iteration 3379, loss = 0.12287745\n",
      "Iteration 3380, loss = 0.11235711\n",
      "Iteration 3381, loss = 0.11143531\n",
      "Iteration 3382, loss = 0.10562108\n",
      "Iteration 3383, loss = 0.10320937\n",
      "Iteration 3384, loss = 0.10591501\n",
      "Iteration 3385, loss = 0.10455294\n",
      "Iteration 3386, loss = 0.10358852\n",
      "Iteration 3387, loss = 0.10315872\n",
      "Iteration 3388, loss = 0.10502503\n",
      "Iteration 3389, loss = 0.10556124\n",
      "Iteration 3390, loss = 0.10232924\n",
      "Iteration 3391, loss = 0.10663717\n",
      "Iteration 3392, loss = 0.11232503\n",
      "Iteration 3393, loss = 0.10647748\n",
      "Iteration 3394, loss = 0.10947461\n",
      "Iteration 3395, loss = 0.10205962\n",
      "Iteration 3396, loss = 0.10230776\n",
      "Iteration 3397, loss = 0.10692948\n",
      "Iteration 3398, loss = 0.10348931\n",
      "Iteration 3399, loss = 0.10842447\n",
      "Iteration 3400, loss = 0.10889249\n",
      "Iteration 3401, loss = 0.10753270\n",
      "Iteration 3402, loss = 0.11160473\n",
      "Iteration 3403, loss = 0.11906430\n",
      "Iteration 3404, loss = 0.10745728\n",
      "Iteration 3405, loss = 0.11698524\n",
      "Iteration 3406, loss = 0.11479205\n",
      "Iteration 3407, loss = 0.10102496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3408, loss = 0.11647477\n",
      "Iteration 3409, loss = 0.10838355\n",
      "Iteration 3410, loss = 0.10404348\n",
      "Iteration 3411, loss = 0.10460588\n",
      "Iteration 3412, loss = 0.10634456\n",
      "Iteration 3413, loss = 0.10255669\n",
      "Iteration 3414, loss = 0.10317883\n",
      "Iteration 3415, loss = 0.10461555\n",
      "Iteration 3416, loss = 0.10681386\n",
      "Iteration 3417, loss = 0.10509806\n",
      "Iteration 3418, loss = 0.11206714\n",
      "Iteration 3419, loss = 0.10608276\n",
      "Iteration 3420, loss = 0.11152822\n",
      "Iteration 3421, loss = 0.10721464\n",
      "Iteration 3422, loss = 0.10629893\n",
      "Iteration 3423, loss = 0.11752641\n",
      "Iteration 3424, loss = 0.10941927\n",
      "Iteration 3425, loss = 0.12028733\n",
      "Iteration 3426, loss = 0.09965505\n",
      "Iteration 3427, loss = 0.11124621\n",
      "Iteration 3428, loss = 0.10693233\n",
      "Iteration 3429, loss = 0.10380466\n",
      "Iteration 3430, loss = 0.10342150\n",
      "Iteration 3431, loss = 0.10486541\n",
      "Iteration 3432, loss = 0.10476840\n",
      "Iteration 3433, loss = 0.11154127\n",
      "Iteration 3434, loss = 0.10817070\n",
      "Iteration 3435, loss = 0.10552000\n",
      "Iteration 3436, loss = 0.10396374\n",
      "Iteration 3437, loss = 0.10501152\n",
      "Iteration 3438, loss = 0.10323970\n",
      "Iteration 3439, loss = 0.10838765\n",
      "Iteration 3440, loss = 0.11461804\n",
      "Iteration 3441, loss = 0.11185698\n",
      "Iteration 3442, loss = 0.12470323\n",
      "Iteration 3443, loss = 0.10585578\n",
      "Iteration 3444, loss = 0.10682695\n",
      "Iteration 3445, loss = 0.11236735\n",
      "Iteration 3446, loss = 0.12569550\n",
      "Iteration 3447, loss = 0.12874372\n",
      "Iteration 3448, loss = 0.12258484\n",
      "Iteration 3449, loss = 0.11919715\n",
      "Iteration 3450, loss = 0.10874576\n",
      "Iteration 3451, loss = 0.11600027\n",
      "Iteration 3452, loss = 0.10765317\n",
      "Iteration 3453, loss = 0.12079757\n",
      "Iteration 3454, loss = 0.11197329\n",
      "Iteration 3455, loss = 0.10543207\n",
      "Iteration 3456, loss = 0.10346540\n",
      "Iteration 3457, loss = 0.10295386\n",
      "Iteration 3458, loss = 0.10227433\n",
      "Iteration 3459, loss = 0.10244077\n",
      "Iteration 3460, loss = 0.10292453\n",
      "Iteration 3461, loss = 0.10200369\n",
      "Iteration 3462, loss = 0.10263192\n",
      "Iteration 3463, loss = 0.10634770\n",
      "Iteration 3464, loss = 0.10993645\n",
      "Iteration 3465, loss = 0.11135183\n",
      "Iteration 3466, loss = 0.11057437\n",
      "Iteration 3467, loss = 0.11503372\n",
      "Iteration 3468, loss = 0.10372831\n",
      "Iteration 3469, loss = 0.10033170\n",
      "Iteration 3470, loss = 0.10846521\n",
      "Iteration 3471, loss = 0.11789487\n",
      "Iteration 3472, loss = 0.11735612\n",
      "Iteration 3473, loss = 0.13939890\n",
      "Iteration 3474, loss = 0.10784012\n",
      "Iteration 3475, loss = 0.11383938\n",
      "Iteration 3476, loss = 0.11638429\n",
      "Iteration 3477, loss = 0.11124369\n",
      "Iteration 3478, loss = 0.12059752\n",
      "Iteration 3479, loss = 0.13112441\n",
      "Iteration 3480, loss = 0.13830075\n",
      "Iteration 3481, loss = 0.11538452\n",
      "Iteration 3482, loss = 0.11346092\n",
      "Iteration 3483, loss = 0.11716654\n",
      "Iteration 3484, loss = 0.11238739\n",
      "Iteration 3485, loss = 0.12352521\n",
      "Iteration 3486, loss = 0.11557279\n",
      "Iteration 3487, loss = 0.10976120\n",
      "Iteration 3488, loss = 0.11375122\n",
      "Iteration 3489, loss = 0.11332679\n",
      "Iteration 3490, loss = 0.11635163\n",
      "Iteration 3491, loss = 0.11278057\n",
      "Iteration 3492, loss = 0.11205543\n",
      "Iteration 3493, loss = 0.10193771\n",
      "Iteration 3494, loss = 0.10662702\n",
      "Iteration 3495, loss = 0.10323945\n",
      "Iteration 3496, loss = 0.10603625\n",
      "Iteration 3497, loss = 0.10875768\n",
      "Iteration 3498, loss = 0.11281733\n",
      "Iteration 3499, loss = 0.10935173\n",
      "Iteration 3500, loss = 0.10883273\n",
      "Iteration 3501, loss = 0.11048121\n",
      "Iteration 3502, loss = 0.10687001\n",
      "Iteration 3503, loss = 0.16288103\n",
      "Iteration 3504, loss = 0.10631081\n",
      "Iteration 3505, loss = 0.13716082\n",
      "Iteration 3506, loss = 0.11731578\n",
      "Iteration 3507, loss = 0.14678799\n",
      "Iteration 3508, loss = 0.12340680\n",
      "Iteration 3509, loss = 0.13364773\n",
      "Iteration 3510, loss = 0.13271380\n",
      "Iteration 3511, loss = 0.10326207\n",
      "Iteration 3512, loss = 0.10830582\n",
      "Iteration 3513, loss = 0.11009154\n",
      "Iteration 3514, loss = 0.10881061\n",
      "Iteration 3515, loss = 0.10245529\n",
      "Iteration 3516, loss = 0.12065775\n",
      "Iteration 3517, loss = 0.09754484\n",
      "Iteration 3518, loss = 0.11087096\n",
      "Iteration 3519, loss = 0.11724919\n",
      "Iteration 3520, loss = 0.10489713\n",
      "Iteration 3521, loss = 0.10827655\n",
      "Iteration 3522, loss = 0.10965663\n",
      "Iteration 3523, loss = 0.10386311\n",
      "Iteration 3524, loss = 0.11879682\n",
      "Iteration 3525, loss = 0.11918134\n",
      "Iteration 3526, loss = 0.10917610\n",
      "Iteration 3527, loss = 0.11720904\n",
      "Iteration 3528, loss = 0.10984115\n",
      "Iteration 3529, loss = 0.10521288\n",
      "Iteration 3530, loss = 0.11685197\n",
      "Iteration 3531, loss = 0.11404684\n",
      "Iteration 3532, loss = 0.10267974\n",
      "Iteration 3533, loss = 0.10242094\n",
      "Iteration 3534, loss = 0.10712248\n",
      "Iteration 3535, loss = 0.11802026\n",
      "Iteration 3536, loss = 0.10328921\n",
      "Iteration 3537, loss = 0.10850399\n",
      "Iteration 3538, loss = 0.11792840\n",
      "Iteration 3539, loss = 0.11393745\n",
      "Iteration 3540, loss = 0.10725149\n",
      "Iteration 3541, loss = 0.10616820\n",
      "Iteration 3542, loss = 0.11168930\n",
      "Iteration 3543, loss = 0.10378237\n",
      "Iteration 3544, loss = 0.10415426\n",
      "Iteration 3545, loss = 0.10651969\n",
      "Iteration 3546, loss = 0.10432486\n",
      "Iteration 3547, loss = 0.10266590\n",
      "Iteration 3548, loss = 0.13178775\n",
      "Iteration 3549, loss = 0.13486518\n",
      "Iteration 3550, loss = 0.12274745\n",
      "Iteration 3551, loss = 0.11350515\n",
      "Iteration 3552, loss = 0.11247670\n",
      "Iteration 3553, loss = 0.12357126\n",
      "Iteration 3554, loss = 0.14681168\n",
      "Iteration 3555, loss = 0.11154111\n",
      "Iteration 3556, loss = 0.14966281\n",
      "Iteration 3557, loss = 0.12602981\n",
      "Iteration 3558, loss = 0.15093282\n",
      "Iteration 3559, loss = 0.13602517\n",
      "Iteration 3560, loss = 0.72419739\n",
      "Iteration 3561, loss = 0.74942547\n",
      "Iteration 3562, loss = 2.93294134\n",
      "Iteration 3563, loss = 3.17280369\n",
      "Iteration 3564, loss = 1.89446902\n",
      "Iteration 3565, loss = 1.21027540\n",
      "Iteration 3566, loss = 1.04299666\n",
      "Iteration 3567, loss = 1.15896736\n",
      "Iteration 3568, loss = 0.69081335\n",
      "Iteration 3569, loss = 0.87623273\n",
      "Iteration 3570, loss = 1.00347780\n",
      "Iteration 3571, loss = 0.71953044\n",
      "Iteration 3572, loss = 0.57686760\n",
      "Iteration 3573, loss = 0.62884411\n",
      "Iteration 3574, loss = 0.73278942\n",
      "Iteration 3575, loss = 0.65614325\n",
      "Iteration 3576, loss = 0.34014568\n",
      "Iteration 3577, loss = 0.50135569\n",
      "Iteration 3578, loss = 0.29358783\n",
      "Iteration 3579, loss = 0.22023913\n",
      "Iteration 3580, loss = 0.45948185\n",
      "Iteration 3581, loss = 0.32302767\n",
      "Iteration 3582, loss = 0.31739885\n",
      "Iteration 3583, loss = 0.23848090\n",
      "Iteration 3584, loss = 0.27016909\n",
      "Iteration 3585, loss = 0.52780629\n",
      "Iteration 3586, loss = 0.51863246\n",
      "Iteration 3587, loss = 0.44667726\n",
      "Iteration 3588, loss = 0.34846467\n",
      "Iteration 3589, loss = 0.49084873\n",
      "Iteration 3590, loss = 0.44524338\n",
      "Iteration 3591, loss = 0.31223495\n",
      "Iteration 3592, loss = 0.25451996\n",
      "Iteration 3593, loss = 0.19360117\n",
      "Iteration 3594, loss = 0.14449516\n",
      "Iteration 3595, loss = 0.14952389\n",
      "Iteration 3596, loss = 0.15995547\n",
      "Iteration 3597, loss = 0.21034155\n",
      "Iteration 3598, loss = 0.19366462\n",
      "Iteration 3599, loss = 0.70062147\n",
      "Iteration 3600, loss = 0.66516855\n",
      "Iteration 3601, loss = 0.54443120\n",
      "Iteration 3602, loss = 0.54181987\n",
      "Iteration 3603, loss = 0.35517364\n",
      "Iteration 3604, loss = 0.20798137\n",
      "Iteration 3605, loss = 0.25984284\n",
      "Iteration 3606, loss = 0.20471226\n",
      "Iteration 3607, loss = 0.14760204\n",
      "Iteration 3608, loss = 0.15191187\n",
      "Iteration 3609, loss = 0.13560450\n",
      "Iteration 3610, loss = 0.13322104\n",
      "Iteration 3611, loss = 0.12049883\n",
      "Iteration 3612, loss = 0.12216264\n",
      "Iteration 3613, loss = 0.12745642\n",
      "Iteration 3614, loss = 0.11954070\n",
      "Iteration 3615, loss = 0.12694364\n",
      "Iteration 3616, loss = 0.13741017\n",
      "Iteration 3617, loss = 0.16307470\n",
      "Iteration 3618, loss = 0.25026104\n",
      "Iteration 3619, loss = 0.19409363\n",
      "Iteration 3620, loss = 0.31638016\n",
      "Iteration 3621, loss = 0.17596081\n",
      "Iteration 3622, loss = 0.27616320\n",
      "Iteration 3623, loss = 0.47536656\n",
      "Iteration 3624, loss = 0.78963174\n",
      "Iteration 3625, loss = 0.67893031\n",
      "Iteration 3626, loss = 0.71727460\n",
      "Iteration 3627, loss = 0.46002498\n",
      "Iteration 3628, loss = 0.28526307\n",
      "Iteration 3629, loss = 0.17044839\n",
      "Iteration 3630, loss = 0.11966958\n",
      "Iteration 3631, loss = 0.12396156\n",
      "Iteration 3632, loss = 0.11092464\n",
      "Iteration 3633, loss = 0.10787735\n",
      "Iteration 3634, loss = 0.11719391\n",
      "Iteration 3635, loss = 0.12036689\n",
      "Iteration 3636, loss = 0.12720432\n",
      "Iteration 3637, loss = 0.14288698\n",
      "Iteration 3638, loss = 0.13791620\n",
      "Iteration 3639, loss = 0.11041224\n",
      "Iteration 3640, loss = 0.10654256\n",
      "Iteration 3641, loss = 0.10664230\n",
      "Iteration 3642, loss = 0.10958796\n",
      "Iteration 3643, loss = 0.11394391\n",
      "Iteration 3644, loss = 0.10903502\n",
      "Iteration 3645, loss = 0.10968927\n",
      "Iteration 3646, loss = 0.10325113\n",
      "Iteration 3647, loss = 0.11090557\n",
      "Iteration 3648, loss = 0.11840333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3649, loss = 0.10897450\n",
      "Iteration 3650, loss = 0.11137769\n",
      "Iteration 3651, loss = 0.11031788\n",
      "Iteration 3652, loss = 0.11176647\n",
      "Iteration 3653, loss = 0.11131410\n",
      "Iteration 3654, loss = 0.10826738\n",
      "Iteration 3655, loss = 0.10629737\n",
      "Iteration 3656, loss = 0.10631702\n",
      "Iteration 3657, loss = 0.11269620\n",
      "Iteration 3658, loss = 0.10685077\n",
      "Iteration 3659, loss = 0.11156981\n",
      "Iteration 3660, loss = 0.10870809\n",
      "Iteration 3661, loss = 0.12603030\n",
      "Iteration 3662, loss = 0.11422491\n",
      "Iteration 3663, loss = 0.10569813\n",
      "Iteration 3664, loss = 0.10792203\n",
      "Iteration 3665, loss = 0.10785305\n",
      "Iteration 3666, loss = 0.10484902\n",
      "Iteration 3667, loss = 0.11414617\n",
      "Iteration 3668, loss = 0.12740411\n",
      "Iteration 3669, loss = 0.13017788\n",
      "Iteration 3670, loss = 0.18532433\n",
      "Iteration 3671, loss = 0.13991823\n",
      "Iteration 3672, loss = 0.12302331\n",
      "Iteration 3673, loss = 0.11683361\n",
      "Iteration 3674, loss = 0.10462605\n",
      "Iteration 3675, loss = 0.10615806\n",
      "Iteration 3676, loss = 0.11842589\n",
      "Iteration 3677, loss = 0.10991548\n",
      "Iteration 3678, loss = 0.11121757\n",
      "Iteration 3679, loss = 0.10559792\n",
      "Iteration 3680, loss = 0.10663828\n",
      "Iteration 3681, loss = 0.10313883\n",
      "Iteration 3682, loss = 0.10923680\n",
      "Iteration 3683, loss = 0.13330313\n",
      "Iteration 3684, loss = 0.12489855\n",
      "Iteration 3685, loss = 0.11062832\n",
      "Iteration 3686, loss = 0.10409003\n",
      "Iteration 3687, loss = 0.11308765\n",
      "Iteration 3688, loss = 0.12155099\n",
      "Iteration 3689, loss = 0.11568164\n",
      "Iteration 3690, loss = 0.10954192\n",
      "Iteration 3691, loss = 0.11484465\n",
      "Iteration 3692, loss = 0.11236284\n",
      "Iteration 3693, loss = 0.10282229\n",
      "Iteration 3694, loss = 0.13124390\n",
      "Iteration 3695, loss = 0.12782258\n",
      "Iteration 3696, loss = 0.11199856\n",
      "Iteration 3697, loss = 0.12665315\n",
      "Iteration 3698, loss = 0.16376675\n",
      "Iteration 3699, loss = 0.12618345\n",
      "Iteration 3700, loss = 0.13126791\n",
      "Iteration 3701, loss = 0.11942417\n",
      "Iteration 3702, loss = 0.11763007\n",
      "Iteration 3703, loss = 0.10655283\n",
      "Iteration 3704, loss = 0.11721845\n",
      "Iteration 3705, loss = 0.10804068\n",
      "Iteration 3706, loss = 0.11999694\n",
      "Iteration 3707, loss = 0.11848079\n",
      "Iteration 3708, loss = 0.11011079\n",
      "Iteration 3709, loss = 0.11769887\n",
      "Iteration 3710, loss = 0.11096471\n",
      "Iteration 3711, loss = 0.10590310\n",
      "Iteration 3712, loss = 0.11603499\n",
      "Iteration 3713, loss = 0.10590247\n",
      "Iteration 3714, loss = 0.11061341\n",
      "Iteration 3715, loss = 0.12762240\n",
      "Iteration 3716, loss = 0.12446457\n",
      "Iteration 3717, loss = 0.11688853\n",
      "Iteration 3718, loss = 0.11688706\n",
      "Iteration 3719, loss = 0.10773020\n",
      "Iteration 3720, loss = 0.10433794\n",
      "Iteration 3721, loss = 0.10474949\n",
      "Iteration 3722, loss = 0.12887613\n",
      "Iteration 3723, loss = 0.14621470\n",
      "Iteration 3724, loss = 0.24381180\n",
      "Iteration 3725, loss = 0.11363776\n",
      "Iteration 3726, loss = 0.12124873\n",
      "Iteration 3727, loss = 0.10838401\n",
      "Iteration 3728, loss = 0.11271438\n",
      "Iteration 3729, loss = 0.10595244\n",
      "Iteration 3730, loss = 0.11021453\n",
      "Iteration 3731, loss = 0.12090119\n",
      "Iteration 3732, loss = 0.11341118\n",
      "Iteration 3733, loss = 0.13815938\n",
      "Iteration 3734, loss = 0.34190025\n",
      "Iteration 3735, loss = 0.28675814\n",
      "Iteration 3736, loss = 0.29451867\n",
      "Iteration 3737, loss = 0.41409092\n",
      "Iteration 3738, loss = 0.29763865\n",
      "Iteration 3739, loss = 0.15839512\n",
      "Iteration 3740, loss = 0.13427697\n",
      "Iteration 3741, loss = 0.11943833\n",
      "Iteration 3742, loss = 0.13422541\n",
      "Iteration 3743, loss = 0.12101592\n",
      "Iteration 3744, loss = 0.14560395\n",
      "Iteration 3745, loss = 0.12269169\n",
      "Iteration 3746, loss = 0.11991471\n",
      "Iteration 3747, loss = 0.11801490\n",
      "Iteration 3748, loss = 0.11283053\n",
      "Iteration 3749, loss = 0.10179884\n",
      "Iteration 3750, loss = 0.10643344\n",
      "Iteration 3751, loss = 0.11788272\n",
      "Iteration 3752, loss = 0.12391874\n",
      "Iteration 3753, loss = 0.12265158\n",
      "Iteration 3754, loss = 0.11297078\n",
      "Iteration 3755, loss = 0.12231256\n",
      "Iteration 3756, loss = 0.11261458\n",
      "Iteration 3757, loss = 0.10556577\n",
      "Iteration 3758, loss = 0.10366439\n",
      "Iteration 3759, loss = 0.10499053\n",
      "Iteration 3760, loss = 0.10812526\n",
      "Iteration 3761, loss = 0.18065255\n",
      "Iteration 3762, loss = 0.12126881\n",
      "Iteration 3763, loss = 0.12515712\n",
      "Iteration 3764, loss = 0.16119766\n",
      "Iteration 3765, loss = 0.26396474\n",
      "Iteration 3766, loss = 0.12330995\n",
      "Iteration 3767, loss = 0.10808169\n",
      "Iteration 3768, loss = 0.10946463\n",
      "Iteration 3769, loss = 0.10630165\n",
      "Iteration 3770, loss = 0.10394250\n",
      "Iteration 3771, loss = 0.10343275\n",
      "Iteration 3772, loss = 0.10868147\n",
      "Iteration 3773, loss = 0.10513934\n",
      "Iteration 3774, loss = 0.10599224\n",
      "Iteration 3775, loss = 0.11917249\n",
      "Iteration 3776, loss = 0.11242898\n",
      "Iteration 3777, loss = 0.11734646\n",
      "Iteration 3778, loss = 0.11219739\n",
      "Iteration 3779, loss = 0.10371647\n",
      "Iteration 3780, loss = 0.10553271\n",
      "Iteration 3781, loss = 0.11873246\n",
      "Iteration 3782, loss = 0.11859443\n",
      "Iteration 3783, loss = 0.10293477\n",
      "Iteration 3784, loss = 0.10914322\n",
      "Iteration 3785, loss = 0.11690521\n",
      "Iteration 3786, loss = 0.11377445\n",
      "Iteration 3787, loss = 0.11973184\n",
      "Iteration 3788, loss = 0.11444939\n",
      "Iteration 3789, loss = 0.10124069\n",
      "Iteration 3790, loss = 0.13784389\n",
      "Iteration 3791, loss = 0.10559930\n",
      "Iteration 3792, loss = 0.11036881\n",
      "Iteration 3793, loss = 0.11477293\n",
      "Iteration 3794, loss = 0.14329674\n",
      "Iteration 3795, loss = 0.11935380\n",
      "Iteration 3796, loss = 0.11510815\n",
      "Iteration 3797, loss = 0.11955372\n",
      "Iteration 3798, loss = 0.10748816\n",
      "Iteration 3799, loss = 0.10402267\n",
      "Iteration 3800, loss = 0.10626469\n",
      "Iteration 3801, loss = 0.10551361\n",
      "Iteration 3802, loss = 0.11934831\n",
      "Iteration 3803, loss = 0.12354291\n",
      "Iteration 3804, loss = 0.11846289\n",
      "Iteration 3805, loss = 0.11474136\n",
      "Iteration 3806, loss = 0.10651750\n",
      "Iteration 3807, loss = 0.10039517\n",
      "Iteration 3808, loss = 0.12316054\n",
      "Iteration 3809, loss = 0.10630265\n",
      "Iteration 3810, loss = 0.12011384\n",
      "Iteration 3811, loss = 0.11764397\n",
      "Iteration 3812, loss = 0.13152403\n",
      "Iteration 3813, loss = 0.11280292\n",
      "Iteration 3814, loss = 0.10801366\n",
      "Iteration 3815, loss = 0.10601976\n",
      "Iteration 3816, loss = 0.10783958\n",
      "Iteration 3817, loss = 0.10485289\n",
      "Iteration 3818, loss = 0.11437211\n",
      "Iteration 3819, loss = 0.10367040\n",
      "Iteration 3820, loss = 0.10499069\n",
      "Iteration 3821, loss = 0.10716779\n",
      "Iteration 3822, loss = 0.11287959\n",
      "Iteration 3823, loss = 0.10531869\n",
      "Iteration 3824, loss = 0.10927518\n",
      "Iteration 3825, loss = 0.10886982\n",
      "Iteration 3826, loss = 0.10704935\n",
      "Iteration 3827, loss = 0.10795741\n",
      "Iteration 3828, loss = 0.10179173\n",
      "Iteration 3829, loss = 0.10762821\n",
      "Iteration 3830, loss = 0.11111761\n",
      "Iteration 3831, loss = 0.10160149\n",
      "Iteration 3832, loss = 0.10566549\n",
      "Iteration 3833, loss = 0.12969003\n",
      "Iteration 3834, loss = 0.10916774\n",
      "Iteration 3835, loss = 0.10260950\n",
      "Iteration 3836, loss = 0.10500512\n",
      "Iteration 3837, loss = 0.10326493\n",
      "Iteration 3838, loss = 0.10817658\n",
      "Iteration 3839, loss = 0.10344979\n",
      "Iteration 3840, loss = 0.10212156\n",
      "Iteration 3841, loss = 0.10543673\n",
      "Iteration 3842, loss = 0.10317180\n",
      "Iteration 3843, loss = 0.10674379\n",
      "Iteration 3844, loss = 0.10772177\n",
      "Iteration 3845, loss = 0.10199754\n",
      "Iteration 3846, loss = 0.10798967\n",
      "Iteration 3847, loss = 0.11127476\n",
      "Iteration 3848, loss = 0.10280545\n",
      "Iteration 3849, loss = 0.10401688\n",
      "Iteration 3850, loss = 0.10877212\n",
      "Iteration 3851, loss = 0.10330332\n",
      "Iteration 3852, loss = 0.11527826\n",
      "Iteration 3853, loss = 0.10415028\n",
      "Iteration 3854, loss = 0.10285331\n",
      "Iteration 3855, loss = 0.10545706\n",
      "Iteration 3856, loss = 0.10288434\n",
      "Iteration 3857, loss = 0.11954308\n",
      "Iteration 3858, loss = 0.11942715\n",
      "Iteration 3859, loss = 0.12219549\n",
      "Iteration 3860, loss = 0.12787072\n",
      "Iteration 3861, loss = 0.14313583\n",
      "Iteration 3862, loss = 0.28776343\n",
      "Iteration 3863, loss = 0.26216732\n",
      "Iteration 3864, loss = 0.64026652\n",
      "Iteration 3865, loss = 0.80153833\n",
      "Iteration 3866, loss = 0.81258928\n",
      "Iteration 3867, loss = 0.82297026\n",
      "Iteration 3868, loss = 0.64902353\n",
      "Iteration 3869, loss = 0.65796805\n",
      "Iteration 3870, loss = 0.40015724\n",
      "Iteration 3871, loss = 0.28205198\n",
      "Iteration 3872, loss = 0.28437940\n",
      "Iteration 3873, loss = 0.26710723\n",
      "Iteration 3874, loss = 0.30707260\n",
      "Iteration 3875, loss = 0.46595035\n",
      "Iteration 3876, loss = 0.46770712\n",
      "Iteration 3877, loss = 0.50526627\n",
      "Iteration 3878, loss = 0.35392292\n",
      "Iteration 3879, loss = 0.25569928\n",
      "Iteration 3880, loss = 0.16684798\n",
      "Iteration 3881, loss = 0.12121978\n",
      "Iteration 3882, loss = 0.13954462\n",
      "Iteration 3883, loss = 0.11794337\n",
      "Iteration 3884, loss = 0.11379502\n",
      "Iteration 3885, loss = 0.11362870\n",
      "Iteration 3886, loss = 0.11331203\n",
      "Iteration 3887, loss = 0.10566512\n",
      "Iteration 3888, loss = 0.10433544\n",
      "Iteration 3889, loss = 0.11113958\n",
      "Iteration 3890, loss = 0.12499591\n",
      "Iteration 3891, loss = 0.11207403\n",
      "Iteration 3892, loss = 0.10763623\n",
      "Iteration 3893, loss = 0.11002974\n",
      "Iteration 3894, loss = 0.10586865\n",
      "Iteration 3895, loss = 0.11313249\n",
      "Iteration 3896, loss = 0.12183265\n",
      "Iteration 3897, loss = 0.11590675\n",
      "Iteration 3898, loss = 0.11957320\n",
      "Iteration 3899, loss = 0.12681568\n",
      "Iteration 3900, loss = 0.11877739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data\n",
    "#from sklearn import preprocessing\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "#X_test[X_test.columns] = scaler.fit_transform(X_test[X_test.columns])\n",
    "#X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "#X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy of svm model is: 97.0\n"
     ]
    }
   ],
   "source": [
    "#MODEL BUILDING(TRAINING)\n",
    "#CHOOSE THE BEST COMIBINATION OF PRAMETERS FROM ABOVE ie  C = 8,  and gamma = 0.0078125,   kernal = rbf  \n",
    "from sklearn import metrics\n",
    "#Create a svm Classifier\n",
    "#classifier = SVC(C=8, gamma=0.00078125, kernel='rbf')\n",
    "#train the model using training data\n",
    "#classifier.fit(X_train, y_train)\n",
    "#predict accuracy of model\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Overall Accuracy of svm model is:\",round(metrics.accuracy_score(y_test, y_pred) * 100,0))     #Compare predicted value with test values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-Class Accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.98      0.98      0.98        90\n",
      "           4       0.96      0.96      0.96        50\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       140\n",
      "   macro avg       0.97      0.97      0.97       140\n",
      "weighted avg       0.97      0.97      0.97       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(\"\\nPer-Class Accuracy\\n\"+str(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix\n",
      "[[88  2]\n",
      " [ 2 48]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConfusion matrix\\n\"+str(confusion_matrix(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error = 2.857142857142861\n"
     ]
    }
   ],
   "source": [
    "accuracy=metrics.accuracy_score(y_test, y_pred) * 100\n",
    "print(\"classification error = \"+str(100-accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operations Characteristic (ROC) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'False Positive Rate')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc,roc_curve\n",
    "false_positive_rate2,true_positive_rate2, thresholds2 =roc_curve(y_test,y_pred,pos_label=4)\n",
    "roc_auc2=auc(false_positive_rate2,true_positive_rate2)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.title('Receiver Operations Characteristic')\n",
    "plt.plot(false_positive_rate2,true_positive_rate2, color='red',label ='AUC=%0.2f' % roc_auc2)\n",
    "plt.legend(loc= 'lower_right')\n",
    "plt.plot([0,1],[0,1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_curve():\n",
    "    # instantiate\n",
    "    lg = classifier\n",
    "\n",
    "    # fit\n",
    "    lg.fit(X, y)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(lg, X, y, n_jobs=-1, cv=5, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # box-like grid\n",
    "    plt.grid()\n",
    "    \n",
    "    # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # plot the average training and test score lines at each training set size\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # sizes the window for readability and displays the plot\n",
    "    # shows error from 0 to 1.1\n",
    "    plt.ylim(0.8,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy=95.71428571428572 \n"
     ]
    }
   ],
   "source": [
    "#logistic_model.fit(X_train,y_train)\n",
    "#overallAccuracy=round(accuracy_score(y_test,y_pred1)*100,0)\n",
    "overallAccuracy=accuracy_score(y_test,y_pred1)*100\n",
    "print(\"Overall Accuracy=%s \"%overallAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Per-class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89  1]\n",
      " [ 5 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.95      0.99      0.97        90\n",
      "           4       0.98      0.90      0.94        50\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       140\n",
      "   macro avg       0.96      0.94      0.95       140\n",
      "weighted avg       0.96      0.96      0.96       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "y_pred1 = logistic_model.predict(X_test)\n",
    "y_score1 = logistic_model.score(X_test,y_test)\n",
    "print(confusion_matrix(y_test,y_pred1))  \n",
    "print(classification_report(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.285714285714278\n"
     ]
    }
   ],
   "source": [
    "error=100-overallAccuracy\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test=np.array(y_test)\n",
    "#y_test\n",
    "#predicted_values=np.array(predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operations Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'False Positive Rate')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc,roc_curve\n",
    "false_positive_rate,true_positive_rate, thresholds =roc_curve(y_test,y_pred1,pos_label=4)\n",
    "roc_auc=auc(false_positive_rate,true_positive_rate)\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title('Receiver Operations Characteristic')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='red',label ='AUC=%0.2f' % roc_auc)\n",
    "plt.legend(loc= 'lower_right')\n",
    "plt.plot([0,1],[0,1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_curve():\n",
    "    # instantiate\n",
    "    lg = logistic_model\n",
    "\n",
    "    # fit\n",
    "    lg.fit(X, y)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(lg, X, y, n_jobs=-1, cv=5, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # box-like grid\n",
    "    plt.grid()\n",
    "    \n",
    "    # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # plot the average training and test score lines at each training set size\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # sizes the window for readability and displays the plot\n",
    "    # shows error from 0 to 1.1\n",
    "    plt.ylim(0.8,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'False Positive Rate')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc,roc_curve\n",
    "false_positive_rate,true_positive_rate, thresholds =roc_curve(y_test,y_pred1,pos_label=4)\n",
    "roc_auc=auc(false_positive_rate,true_positive_rate)\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.title('Receiver Operations Characteristic')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='purple',label ='AUC=%0.2f' % roc_auc)\n",
    "plt.plot(false_positive_rate2,true_positive_rate2, color='red',label ='AUC=%0.2f' % roc_auc2)\n",
    "plt.plot(false_positive_rate1,true_positive_rate1, color='green',label ='AUC=%0.2f' % roc_auc1)\n",
    "plt.legend(loc= 'lower_right')\n",
    "plt.plot([0,1],[0,1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
